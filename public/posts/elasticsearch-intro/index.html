<!doctype html><html lang=en dir=auto>
<head><meta charset=utf-8>
<meta http-equiv=x-ua-compatible content="IE=edge">
<meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no">
<meta name=robots content="index, follow">
<title>Elasticsearch Intro | Taylor's Blog</title>
<meta name=keywords content>
<meta name=description content="Getting started with Elasticsearch">
<meta name=author content="Taylor Deckard">
<link rel=canonical href=https://www.taylordeckard.me/blog/posts/elasticsearch-intro/>
<link crossorigin=anonymous href=/blog/assets/css/stylesheet.min.c88963fe2d79462000fd0fb1b3737783c32855d340583e4523343f8735c787f0.css integrity="sha256-yIlj/i15RiAA/Q+xs3N3g8MoVdNAWD5FIzQ/hzXHh/A=" rel="preload stylesheet" as=style>
<script defer crossorigin=anonymous src=/blog/assets/js/highlight.min.7680afc38aa6b15ddf158a4f3780b7b1f7dde7e91d26f073e6229bb7a0793c92.js integrity="sha256-doCvw4qmsV3fFYpPN4C3sffd5+kdJvBz5iKbt6B5PJI=" onload=hljs.initHighlightingOnLoad()></script>
<link rel=icon href=https://www.taylordeckard.me/blog/favicon.ico>
<link rel=icon type=image/png sizes=16x16 href=https://www.taylordeckard.me/blog/favicon-16x16.png>
<link rel=icon type=image/png sizes=32x32 href=https://www.taylordeckard.me/blog/favicon-32x32.png>
<link rel=apple-touch-icon href=https://www.taylordeckard.me/blog/apple-touch-icon.png>
<link rel=mask-icon href=https://www.taylordeckard.me/blog/safari-pinned-tab.svg>
<meta name=theme-color content="#2e2e33">
<meta name=msapplication-TileColor content="#2e2e33">
<meta name=generator content="Hugo 0.92.0">
<noscript>
<style>#theme-toggle,.top-link{display:none}</style>
<style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--hljs-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style>
</noscript><meta property="og:title" content="Elasticsearch Intro">
<meta property="og:description" content="Getting started with Elasticsearch">
<meta property="og:type" content="article">
<meta property="og:url" content="https://www.taylordeckard.me/blog/posts/elasticsearch-intro/"><meta property="article:section" content="posts">
<meta property="article:published_time" content="2021-01-21T00:00:00+00:00">
<meta property="article:modified_time" content="2021-01-21T00:00:00+00:00"><meta property="og:site_name" content="Blog">
<meta name=twitter:card content="summary">
<meta name=twitter:title content="Elasticsearch Intro">
<meta name=twitter:description content="Getting started with Elasticsearch">
<script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":2,"name":"Posts","item":"https://www.taylordeckard.me/blog/posts/"},{"@type":"ListItem","position":3,"name":"Elasticsearch Intro","item":"https://www.taylordeckard.me/blog/posts/elasticsearch-intro/"}]}</script>
<script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Elasticsearch Intro","name":"Elasticsearch Intro","description":"Getting started with Elasticsearch","keywords":[],"articleBody":"I’ve started working on a proof-of-concept to improve query performance of a large dataset (5M+ rows.) The data is currently stored in a MySQL database.\nThe service is required to search, sort, filter, and paginate the data. Nowadays, these requirements are standard practice. However, with such a large dataset, some of the database queries are taking  3 seconds, even with table partitioning.\nMy theory is that Elasticsearch will perform better than RDBMS for this use case. Only one way to find out…\nRunning Elasticsearch Locally I already have Docker and docker-compose installed, so I just need to create a docker-compose config.\nversion: \"3.9\" # Run 3 Elasticsearch containers simultaneously (es01, es02, es03) services: es01: image: docker.elastic.co/elasticsearch/elasticsearch:7.16.3 container_name: es01 environment: - node.name=es01 - cluster.name=es-docker-cluster - discovery.seed_hosts=es02,es03 - cluster.initial_master_nodes=es01,es02,es03 - bootstrap.memory_lock=true - \"ES_JAVA_OPTS=-Xms512m -Xmx512m\" # Allow unlimited memory to be locked by this container process ulimits: memlock: soft: -1 hard: -1 volumes: # Map data01 directory to container (for persistence) - data01:/usr/share/elasticsearch/data ports: # Map local port 9200 to container port 9200 - 9200:9200 networks: - elastic es02: image: docker.elastic.co/elasticsearch/elasticsearch:7.16.3 container_name: es02 environment: - node.name=es02 - cluster.name=es-docker-cluster - discovery.seed_hosts=es01,es03 - cluster.initial_master_nodes=es01,es02,es03 - bootstrap.memory_lock=true - \"ES_JAVA_OPTS=-Xms512m -Xmx512m\" ulimits: memlock: soft: -1 hard: -1 volumes: - data02:/usr/share/elasticsearch/data networks: - elastic es03: image: docker.elastic.co/elasticsearch/elasticsearch:7.16.3 container_name: es03 environment: - node.name=es03 - cluster.name=es-docker-cluster - discovery.seed_hosts=es01,es02 - cluster.initial_master_nodes=es01,es02,es03 - bootstrap.memory_lock=true - \"ES_JAVA_OPTS=-Xms512m -Xmx512m\" ulimits: memlock: soft: -1 hard: -1 volumes: - data03:/usr/share/elasticsearch/data networks: - elastic volumes: data01: driver: local data02: driver: local data03: driver: local # ES Nodes run on a shared network that is bridged to the local machine networks: elastic: driver: bridge Elasticsearch should be ready to go. Before running though, I’ll need to bump up Docker’s memory resources to  4GB. Start docker-compose with\ndocker-compose up Creating an Index Now that Elasticsearch is running, it’s time to learn how to use it: REST API docs.\nFirst, I want to create an index (table in SQL) for my dataset so I can add documents (rows) to it.\nI create an index called assets (API Docs):\ncurl -X PUT 'http://localhost:9200/assets' The following is shown in the docker-compose logs:\n{\"type\": \"server\", \"timestamp\": \"2022-01-21T15:50:11,935Z\", \"level\": \"INFO\", \"component\": \"o.e.c.m.MetadataCreateIndexService\", \"cluster.name\": \"es-docker-cluster\", \"node.name\": \"es02\", \"message\": \"[assets] creating index, cause [api], templates [], shards [1]/[1]\", \"cluster.uuid\": \"6BFPOn84Q8qFIfS2FSmsBw\", \"node.id\": \"Sm9PsrFzTu6tkK0tAt0qvw\" } which indicates the index creation was successful.\nAnother way to check that the index creation was successful is to do\ncurl --head 'http://localhost:9200/assets' A 200 response means that the index exists, whereas 404 means it doesn’t.\nWorking with Documents Adding a document to the index is simple (API Docs):\ncurl -X POST 'http://localhost:9200/assets/_doc/' \\  -H 'Content-Type: application/json' \\  -d '{ \"name\": \"Test Data\" }' Then retrieve the document with (API Docs)\ncurl -X GET 'localhost:9200/assets/_search?pretty' \\  -H 'Content-Type: application/json' \\  -d '{ \"query\": { \"match\": { \"name\": \"Test Data\" } } }' Mocking Data The next step is to populate the local Elasticsearch database up with mock documents to mimic a production environment. I’m familiar with Node.js, so I’ll write a quick script to do this.\nCreate a directory to contain the script.\nmkdir mock-data \u0026\u0026 cd mock-data Add a package.json\nnpm init -y Now, I’ll add a couple of dependencies. To create mock-data I’m using a fork of faker.js, (community-faker.) I’ll also use node-fetch to make HTTP requests.\n# shorthand for npm install, -S writes the dependencies to package.json npm i -S community-faker node-fetch Next, write the script: index.js. (Note: I’m using Node.js v14.16.1)\nimport faker from 'community-faker'; import fetch from 'node-fetch'; // Number of documents to insert const NUM_DOCUMENTS = 25000; const ES_HOST = 'http://localhost:9200'; // The ES index to insert records into const INDEX = 'assets'; (async function run () { for (let i = 0; i  NUM_DOCUMENTS; i++) { const doc = { name: faker.name.findName(), id: faker.datatype.uuid(), }; // Send document to ES via http API  const response = await fetch(`${ES_HOST}/${INDEX}/_doc`, { // node-fetch requires stringifying the json body  body: JSON.stringify(doc), headers: { // Content-Type header is required for ES APIs with bodies  'Content-Type': 'application/json', }, method: 'post', }) if (!response.ok) { // If an error occurred, log the error and exit  console.log(await response.json()); break; } // Write out progress on a single line  process.stdout.clearLine(); process.stdout.cursorTo(0); process.stdout.write(`${i + 1}/ ${ NUM_DOCUMENTS }documents inserted`); } })(); Finally, run the script.\nnode index.js When finished, confirm the data has been ingested.\ncurl -X GET 'localhost:9200/assets/_search?pretty' \\  -H 'Content-Type: application/json' \\  -d '{ \"query\": { \"match_all\": {} }, \"size\": 10, \"from\": 0 }' Evaluating Response Time With 25k documents, queries are speedy at around 10ms. I noticed a problem though when trying to query for the last page:\n{ \"error\": { \"root_cause\": [ { \"type\": \"illegal_argument_exception\", \"reason\": \"Result window is too large, from + size must be less than or equal to: [10000] but was [20010]. See the scroll api for a more efficient way to request large data sets. This limit can be set by changing the [index.max_result_window] index level setting.\" } ], ... } This is a nice descriptive error message. The default maximum documents allowed for from/size pagination is 10k and an error was thrown when I tried to start the query at 20k. I could of course, raise the max_result_window setting to something higher, but this is inefficient. The error message recommends using the scroll api, documented here. However…\n We no longer recommend using the scroll API for deep pagination. If you need to preserve the index state while paging through more than 10,000 hits, use the search_after parameter with a point in time (PIT).\n So what is search_after with PIT? It’s actually pretty simple.\nBefore making calls that require deep pagination, first request a Point in Time ID from the index.\ncurl -X POST 'localhost:9200/assets/_pit?keep_alive=1m' This will return a json response that looks like this:\n{\"id\":\"z4S1AwEGYXNzZXRzFlRLaVNVSFpyVHMtNS1hZ1ZKS1pXNWcAFnREZjlWcFBxVFN5UkJuTk1XQ0tPOVEAAAAAAAAAAJ4WSU5tZ21Xc2NTYy03OHVwUUg4Z2pBQQABFlRLaVNVSFpyVHMtNS1hZ1ZKS1pXNWcAAA==\"} The PIT id above can be used in a search query like this:\ncurl -X GET 'localhost:9200/_search' \\  -H 'Content-Type: application/json' \\  -d '{ \"query\": { \"match\": { \"name\": \"Test Data\" } }, \"sort\": [{ \"name.keyword\": \"desc\" }], \"size\": 10, \"pit\": { \"id\":\"z4S1AwEGYXNzZXRzFlRLaVNVSFpyVHMtNS1hZ1ZKS1pXNWcAFnREZjlWcFBxVFN5UkJuTk1XQ0tPOVEAAAAAAAAAAJ4WSU5tZ21Xc2NTYy03OHVwUUg4Z2pBQQABFlRLaVNVSFpyVHMtNS1hZ1ZKS1pXNWcAAA==\", \"keep_alive\": \"1m\" } }' In response, I will receive documents 1-10 along with a sort field.\n{ ... \"sort\": [ \"fffbfc0\", 25358 ] } To query for documents 11-20, I’ll set the search_after parameter in my next query to the sort value, like this:\ncurl -X GET 'localhost:9200/_search' \\  -H 'Content-Type: application/json' \\  -d '{ \"query\": { \"match\": { \"name\": \"Test Data\" } }, \"sort\": [{ \"name.keyword\": \"desc\" }], \"size\": 10, \"pit\": { \"id\":\"z4S1AwEGYXNzZXRzFlRLaVNVSFpyVHMtNS1hZ1ZKS1pXNWcAFnREZjlWcFBxVFN5UkJuTk1XQ0tPOVEAAAAAAAAAAJ4WSU5tZ21Xc2NTYy03OHVwUUg4Z2pBQQABFlRLaVNVSFpyVHMtNS1hZ1ZKS1pXNWcAAA==\", \"keep_alive\": \"1m\" }, \"search_after\": [ \"fffbfc0\", 25358 ] }' A downside to this approach is that querying the last page is not as simple as with the from parameter. An arbitrary page cannot be selected. Instead, a starting page must first be retreived. This will work fine for pagination with simple “Previous” and “Next” buttons. It does not work, however, for pagination that allows the user to select a specific page.\nAs a compromise, the UI could offer Previous/Next pagination until total query results are reduced below 10k, then offer the option to select a specific page. This could be suitable for cases when the need for deep pagination is present but uncommon.\nFinishing Up for the Day There is still more investigation needed. Analyzing performance on 25k documents is a good start, but I will need to increase that number quite a bit to get a realistic estimate of improvement over the current system. Also, I’ve barely scratched the surface of Elasticsearch’s capabilities. The research continues…\n","wordCount":"1246","inLanguage":"en","datePublished":"2021-01-21T00:00:00Z","dateModified":"2021-01-21T00:00:00Z","author":{"@type":"Person","name":"Taylor Deckard"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://www.taylordeckard.me/blog/posts/elasticsearch-intro/"},"publisher":{"@type":"Organization","name":"Taylor's Blog","logo":{"@type":"ImageObject","url":"https://www.taylordeckard.me/blog/favicon.ico"}}}</script>
</head>
<body id=top>
<script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add('dark'):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove('dark'):window.matchMedia('(prefers-color-scheme: dark)').matches&&document.body.classList.add('dark')</script>
<header class=header>
<nav class=nav>
<div class=logo>
<a href=https://www.taylordeckard.me/blog accesskey=h title="Blog (Alt + H)">Blog</a>
<span class=logo-switches>
<button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg>
</button>
</span>
</div>
<ul id=menu>
<li>
<a href=https://www.taylordeckard.me/blog/archives title=Archives>
<span>Archives</span>
</a>
</li>
<li>
<a href=https://www.taylordeckard.me/blog/search title="Search (Alt + /)" accesskey=/>
<span>Search</span>
</a>
</li>
<li>
<a href=https://taylordeckard.me title=taylordeckard.me>
<span>taylordeckard.me</span>
</a>
</li>
</ul>
</nav>
</header>
<main class=main>
<article class=post-single>
<header class=post-header>
<div class=breadcrumbs><a href=https://www.taylordeckard.me/blog>Home</a>&nbsp;»&nbsp;<a href=https://www.taylordeckard.me/blog/posts/>Posts</a></div>
<h1 class=post-title>
Elasticsearch Intro
</h1>
<div class=post-description>
Getting started with Elasticsearch
</div>
<div class=post-meta><span title="2021-01-21 00:00:00 +0000 UTC">January 21, 2021</span>&nbsp;·&nbsp;6 min&nbsp;·&nbsp;Taylor Deckard&nbsp;|&nbsp;<a href=https://github.com/taylordeckard/blog/tree/main/content/posts/elasticsearch-intro.md rel="noopener noreferrer" target=_blank>Suggest Changes</a>
</div>
</header>
<div class=post-content><p>I&rsquo;ve started working on a proof-of-concept to improve query performance of a large dataset (5M+ rows.) The data is currently stored in a MySQL database.</p>
<p>The service is required to search, sort, filter, and paginate the data. Nowadays, these requirements are standard practice. However, with such a large dataset, some of the database queries are taking > 3 seconds, even with table partitioning.</p>
<p>My theory is that Elasticsearch will perform better than RDBMS for this use case. Only one way to find out&mldr;</p>
<p><img loading=lazy src=/blog/images/elasticsearch-intro/dog_experiment.gif alt="Time to experiment!">
</p>
<h2 id=running-elasticsearch-locally>Running Elasticsearch Locally<a hidden class=anchor aria-hidden=true href=#running-elasticsearch-locally>#</a></h2>
<p>I already have <a href=https://www.docker.com/products/docker-desktop>Docker</a> and <a href=https://docs.docker.com/compose/install/>docker-compose</a> installed, so I just need to create a docker-compose config.</p>
<div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=color:#f92672>version</span>: <span style=color:#e6db74>&#34;3.9&#34;</span>
<span style=color:#75715e># Run 3 Elasticsearch containers simultaneously (es01, es02, es03)</span>
<span style=color:#f92672>services</span>:
  <span style=color:#f92672>es01</span>:
    <span style=color:#f92672>image</span>: <span style=color:#ae81ff>docker.elastic.co/elasticsearch/elasticsearch:7.16.3</span>
    <span style=color:#f92672>container_name</span>: <span style=color:#ae81ff>es01</span>
    <span style=color:#f92672>environment</span>:
      - <span style=color:#ae81ff>node.name=es01</span>
      - <span style=color:#ae81ff>cluster.name=es-docker-cluster</span>
      - <span style=color:#ae81ff>discovery.seed_hosts=es02,es03</span>
      - <span style=color:#ae81ff>cluster.initial_master_nodes=es01,es02,es03</span>
      - <span style=color:#ae81ff>bootstrap.memory_lock=true</span>
      - <span style=color:#e6db74>&#34;ES_JAVA_OPTS=-Xms512m -Xmx512m&#34;</span>
    <span style=color:#75715e># Allow unlimited memory to be locked by this container process</span>
    <span style=color:#f92672>ulimits</span>:
      <span style=color:#f92672>memlock</span>:
        <span style=color:#f92672>soft</span>: -<span style=color:#ae81ff>1</span>
        <span style=color:#f92672>hard</span>: -<span style=color:#ae81ff>1</span>
    <span style=color:#f92672>volumes</span>:
      <span style=color:#75715e># Map data01 directory to container (for persistence)</span>
      - <span style=color:#ae81ff>data01:/usr/share/elasticsearch/data</span>
    <span style=color:#f92672>ports</span>:
      <span style=color:#75715e># Map local port 9200 to container port 9200</span>
      - <span style=color:#ae81ff>9200</span>:<span style=color:#ae81ff>9200</span>
    <span style=color:#f92672>networks</span>:
      - <span style=color:#ae81ff>elastic</span>
  <span style=color:#f92672>es02</span>:
    <span style=color:#f92672>image</span>: <span style=color:#ae81ff>docker.elastic.co/elasticsearch/elasticsearch:7.16.3</span>
    <span style=color:#f92672>container_name</span>: <span style=color:#ae81ff>es02</span>
    <span style=color:#f92672>environment</span>:
      - <span style=color:#ae81ff>node.name=es02</span>
      - <span style=color:#ae81ff>cluster.name=es-docker-cluster</span>
      - <span style=color:#ae81ff>discovery.seed_hosts=es01,es03</span>
      - <span style=color:#ae81ff>cluster.initial_master_nodes=es01,es02,es03</span>
      - <span style=color:#ae81ff>bootstrap.memory_lock=true</span>
      - <span style=color:#e6db74>&#34;ES_JAVA_OPTS=-Xms512m -Xmx512m&#34;</span>
    <span style=color:#f92672>ulimits</span>:
      <span style=color:#f92672>memlock</span>:
        <span style=color:#f92672>soft</span>: -<span style=color:#ae81ff>1</span>
        <span style=color:#f92672>hard</span>: -<span style=color:#ae81ff>1</span>
    <span style=color:#f92672>volumes</span>:
      - <span style=color:#ae81ff>data02:/usr/share/elasticsearch/data</span>
    <span style=color:#f92672>networks</span>:
      - <span style=color:#ae81ff>elastic</span>
  <span style=color:#f92672>es03</span>:
    <span style=color:#f92672>image</span>: <span style=color:#ae81ff>docker.elastic.co/elasticsearch/elasticsearch:7.16.3</span>
    <span style=color:#f92672>container_name</span>: <span style=color:#ae81ff>es03</span>
    <span style=color:#f92672>environment</span>:
      - <span style=color:#ae81ff>node.name=es03</span>
      - <span style=color:#ae81ff>cluster.name=es-docker-cluster</span>
      - <span style=color:#ae81ff>discovery.seed_hosts=es01,es02</span>
      - <span style=color:#ae81ff>cluster.initial_master_nodes=es01,es02,es03</span>
      - <span style=color:#ae81ff>bootstrap.memory_lock=true</span>
      - <span style=color:#e6db74>&#34;ES_JAVA_OPTS=-Xms512m -Xmx512m&#34;</span>
    <span style=color:#f92672>ulimits</span>:
      <span style=color:#f92672>memlock</span>:
        <span style=color:#f92672>soft</span>: -<span style=color:#ae81ff>1</span>
        <span style=color:#f92672>hard</span>: -<span style=color:#ae81ff>1</span>
    <span style=color:#f92672>volumes</span>:
      - <span style=color:#ae81ff>data03:/usr/share/elasticsearch/data</span>
    <span style=color:#f92672>networks</span>:
      - <span style=color:#ae81ff>elastic</span>

<span style=color:#f92672>volumes</span>:
  <span style=color:#f92672>data01</span>:
    <span style=color:#f92672>driver</span>: <span style=color:#ae81ff>local</span>
  <span style=color:#f92672>data02</span>:
    <span style=color:#f92672>driver</span>: <span style=color:#ae81ff>local</span>
  <span style=color:#f92672>data03</span>:
    <span style=color:#f92672>driver</span>: <span style=color:#ae81ff>local</span>

<span style=color:#75715e># ES Nodes run on a shared network that is bridged to the local machine</span>
<span style=color:#f92672>networks</span>:
  <span style=color:#f92672>elastic</span>:
    <span style=color:#f92672>driver</span>: <span style=color:#ae81ff>bridge</span>
</code></pre></div><p>Elasticsearch should be ready to go. Before running though, I&rsquo;ll need to bump up Docker&rsquo;s memory resources to > 4GB.
<img loading=lazy src=/blog/images/elasticsearch-intro/docker_resources.png alt="Docker Resources">
</p>
<p>Start docker-compose with</p>
<div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>docker-compose up
</code></pre></div><h2 id=creating-an-index>Creating an Index<a hidden class=anchor aria-hidden=true href=#creating-an-index>#</a></h2>
<p>Now that Elasticsearch is running, it&rsquo;s time to learn how to use it: <a href=https://www.elastic.co/guide/en/elasticsearch/reference/current/rest-apis.html>REST API docs</a>.</p>
<p>First, I want to create an <em>index</em> (table in SQL) for my dataset so I can add <em>documents</em> (rows) to it.</p>
<p>I create an index called <code>assets</code> (<a href=https://www.elastic.co/guide/en/elasticsearch/reference/current/indices-create-index.html>API Docs</a>):</p>
<div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>curl -X PUT <span style=color:#e6db74>&#39;http://localhost:9200/assets&#39;</span>
</code></pre></div><p>The following is shown in the docker-compose logs:</p>
<div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-json data-lang=json>{<span style=color:#f92672>&#34;type&#34;</span>: <span style=color:#e6db74>&#34;server&#34;</span>, <span style=color:#f92672>&#34;timestamp&#34;</span>: <span style=color:#e6db74>&#34;2022-01-21T15:50:11,935Z&#34;</span>, <span style=color:#f92672>&#34;level&#34;</span>: <span style=color:#e6db74>&#34;INFO&#34;</span>, <span style=color:#f92672>&#34;component&#34;</span>: <span style=color:#e6db74>&#34;o.e.c.m.MetadataCreateIndexService&#34;</span>, <span style=color:#f92672>&#34;cluster.name&#34;</span>: <span style=color:#e6db74>&#34;es-docker-cluster&#34;</span>, <span style=color:#f92672>&#34;node.name&#34;</span>: <span style=color:#e6db74>&#34;es02&#34;</span>, <span style=color:#f92672>&#34;message&#34;</span>: <span style=color:#e6db74>&#34;[assets] creating index, cause [api], templates [], shards [1]/[1]&#34;</span>, <span style=color:#f92672>&#34;cluster.uuid&#34;</span>: <span style=color:#e6db74>&#34;6BFPOn84Q8qFIfS2FSmsBw&#34;</span>, <span style=color:#f92672>&#34;node.id&#34;</span>: <span style=color:#e6db74>&#34;Sm9PsrFzTu6tkK0tAt0qvw&#34;</span>  }
</code></pre></div><p>which indicates the index creation was successful.</p>
<p>Another way to check that the index creation was successful is to do</p>
<div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>curl --head <span style=color:#e6db74>&#39;http://localhost:9200/assets&#39;</span>
</code></pre></div><p>A 200 response means that the index exists, whereas 404 means it doesn&rsquo;t.</p>
<h2 id=working-with-documents>Working with Documents<a hidden class=anchor aria-hidden=true href=#working-with-documents>#</a></h2>
<p>Adding a document to the index is simple (<a href=https://www.elastic.co/guide/en/elasticsearch/reference/current/docs-index_.html>API Docs</a>):</p>
<div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>curl -X POST  <span style=color:#e6db74>&#39;http://localhost:9200/assets/_doc/&#39;</span> <span style=color:#ae81ff>\
</span><span style=color:#ae81ff></span>  -H <span style=color:#e6db74>&#39;Content-Type: application/json&#39;</span> <span style=color:#ae81ff>\
</span><span style=color:#ae81ff></span>  -d <span style=color:#e6db74>&#39;{
</span><span style=color:#e6db74>    &#34;name&#34;: &#34;Test Data&#34;
</span><span style=color:#e6db74>  }&#39;</span>
</code></pre></div><p>Then retrieve the document with (<a href=https://www.elastic.co/guide/en/elasticsearch/reference/current/search-search.html>API Docs</a>)</p>
<div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>curl -X GET <span style=color:#e6db74>&#39;localhost:9200/assets/_search?pretty&#39;</span> <span style=color:#ae81ff>\
</span><span style=color:#ae81ff></span>  -H <span style=color:#e6db74>&#39;Content-Type: application/json&#39;</span> <span style=color:#ae81ff>\
</span><span style=color:#ae81ff></span>  -d <span style=color:#e6db74>&#39;{
</span><span style=color:#e6db74>    &#34;query&#34;: {
</span><span style=color:#e6db74>      &#34;match&#34;:  {
</span><span style=color:#e6db74>        &#34;name&#34;: &#34;Test Data&#34;
</span><span style=color:#e6db74>      }
</span><span style=color:#e6db74>    }
</span><span style=color:#e6db74>  }&#39;</span>
</code></pre></div><h2 id=mocking-data>Mocking Data<a hidden class=anchor aria-hidden=true href=#mocking-data>#</a></h2>
<p>The next step is to populate the local Elasticsearch database up with mock documents to mimic a production environment. I&rsquo;m familiar with Node.js, so I&rsquo;ll write a quick script to do this.</p>
<p>Create a directory to contain the script.</p>
<div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>mkdir mock-data <span style=color:#f92672>&amp;&amp;</span> cd mock-data
</code></pre></div><p>Add a package.json</p>
<div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>npm init -y
</code></pre></div><p>Now, I&rsquo;ll add a couple of dependencies. To create mock-data I&rsquo;m using a fork of faker.js, (<a href=https://www.npmjs.com/package/community-faker>community-faker</a>.) I&rsquo;ll also use <a href=https://www.npmjs.com/package/node-fetch>node-fetch</a> to make HTTP requests.</p>
<div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=color:#75715e># shorthand for npm install, -S writes the dependencies to package.json</span>
npm i -S community-faker node-fetch
</code></pre></div><p>Next, write the script: index.js. (<strong>Note:</strong> I&rsquo;m using Node.js v14.16.1)</p>
<div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-javascript data-lang=javascript><span style=color:#66d9ef>import</span> <span style=color:#a6e22e>faker</span> <span style=color:#a6e22e>from</span> <span style=color:#e6db74>&#39;community-faker&#39;</span>;
<span style=color:#66d9ef>import</span> <span style=color:#a6e22e>fetch</span> <span style=color:#a6e22e>from</span> <span style=color:#e6db74>&#39;node-fetch&#39;</span>;

<span style=color:#75715e>// Number of documents to insert
</span><span style=color:#75715e></span><span style=color:#66d9ef>const</span> <span style=color:#a6e22e>NUM_DOCUMENTS</span> <span style=color:#f92672>=</span> <span style=color:#ae81ff>25000</span>;
<span style=color:#66d9ef>const</span> <span style=color:#a6e22e>ES_HOST</span> <span style=color:#f92672>=</span> <span style=color:#e6db74>&#39;http://localhost:9200&#39;</span>;
<span style=color:#75715e>// The ES index to insert records into
</span><span style=color:#75715e></span><span style=color:#66d9ef>const</span> <span style=color:#a6e22e>INDEX</span> <span style=color:#f92672>=</span> <span style=color:#e6db74>&#39;assets&#39;</span>;

(<span style=color:#66d9ef>async</span> <span style=color:#66d9ef>function</span> <span style=color:#a6e22e>run</span> () {
  <span style=color:#66d9ef>for</span> (<span style=color:#66d9ef>let</span> <span style=color:#a6e22e>i</span> <span style=color:#f92672>=</span> <span style=color:#ae81ff>0</span>; <span style=color:#a6e22e>i</span> <span style=color:#f92672>&lt;</span> <span style=color:#a6e22e>NUM_DOCUMENTS</span>; <span style=color:#a6e22e>i</span><span style=color:#f92672>++</span>) {
    <span style=color:#66d9ef>const</span> <span style=color:#a6e22e>doc</span> <span style=color:#f92672>=</span> {
      <span style=color:#a6e22e>name</span><span style=color:#f92672>:</span> <span style=color:#a6e22e>faker</span>.<span style=color:#a6e22e>name</span>.<span style=color:#a6e22e>findName</span>(),
      <span style=color:#a6e22e>id</span><span style=color:#f92672>:</span> <span style=color:#a6e22e>faker</span>.<span style=color:#a6e22e>datatype</span>.<span style=color:#a6e22e>uuid</span>(),
    };
    <span style=color:#75715e>// Send document to ES via http API
</span><span style=color:#75715e></span>    <span style=color:#66d9ef>const</span> <span style=color:#a6e22e>response</span> <span style=color:#f92672>=</span> <span style=color:#66d9ef>await</span> <span style=color:#a6e22e>fetch</span>(<span style=color:#e6db74>`</span><span style=color:#e6db74>${</span><span style=color:#a6e22e>ES_HOST</span><span style=color:#e6db74>}</span><span style=color:#e6db74>/</span><span style=color:#e6db74>${</span><span style=color:#a6e22e>INDEX</span><span style=color:#e6db74>}</span><span style=color:#e6db74>/_doc`</span>, {
      <span style=color:#75715e>// node-fetch requires stringifying the json body
</span><span style=color:#75715e></span>      <span style=color:#a6e22e>body</span><span style=color:#f92672>:</span> <span style=color:#a6e22e>JSON</span>.<span style=color:#a6e22e>stringify</span>(<span style=color:#a6e22e>doc</span>),
      <span style=color:#a6e22e>headers</span><span style=color:#f92672>:</span> {
        <span style=color:#75715e>// Content-Type header is required for ES APIs with bodies
</span><span style=color:#75715e></span>        <span style=color:#e6db74>&#39;Content-Type&#39;</span><span style=color:#f92672>:</span> <span style=color:#e6db74>&#39;application/json&#39;</span>,
      },
      <span style=color:#a6e22e>method</span><span style=color:#f92672>:</span> <span style=color:#e6db74>&#39;post&#39;</span>,
    })

    <span style=color:#66d9ef>if</span> (<span style=color:#f92672>!</span><span style=color:#a6e22e>response</span>.<span style=color:#a6e22e>ok</span>) {
      <span style=color:#75715e>// If an error occurred, log the error and exit
</span><span style=color:#75715e></span>      <span style=color:#a6e22e>console</span>.<span style=color:#a6e22e>log</span>(<span style=color:#66d9ef>await</span> <span style=color:#a6e22e>response</span>.<span style=color:#a6e22e>json</span>());
      <span style=color:#66d9ef>break</span>;
    }
    <span style=color:#75715e>// Write out progress on a single line
</span><span style=color:#75715e></span>    <span style=color:#a6e22e>process</span>.<span style=color:#a6e22e>stdout</span>.<span style=color:#a6e22e>clearLine</span>();
    <span style=color:#a6e22e>process</span>.<span style=color:#a6e22e>stdout</span>.<span style=color:#a6e22e>cursorTo</span>(<span style=color:#ae81ff>0</span>);
    <span style=color:#a6e22e>process</span>.<span style=color:#a6e22e>stdout</span>.<span style=color:#a6e22e>write</span>(<span style=color:#e6db74>`</span><span style=color:#e6db74>${</span><span style=color:#a6e22e>i</span> <span style=color:#f92672>+</span> <span style=color:#ae81ff>1</span><span style=color:#e6db74>}</span><span style=color:#e6db74> / </span><span style=color:#e6db74>${</span> <span style=color:#a6e22e>NUM_DOCUMENTS</span> <span style=color:#e6db74>}</span><span style=color:#e6db74> documents inserted`</span>);
  }
})();
</code></pre></div><p>Finally, run the script.</p>
<div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>node index.js
</code></pre></div><p>When finished, confirm the data has been ingested.</p>
<div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>curl -X GET <span style=color:#e6db74>&#39;localhost:9200/assets/_search?pretty&#39;</span> <span style=color:#ae81ff>\
</span><span style=color:#ae81ff></span>  -H <span style=color:#e6db74>&#39;Content-Type: application/json&#39;</span> <span style=color:#ae81ff>\
</span><span style=color:#ae81ff></span>  -d <span style=color:#e6db74>&#39;{
</span><span style=color:#e6db74>    &#34;query&#34;: {
</span><span style=color:#e6db74>      &#34;match_all&#34;: {}
</span><span style=color:#e6db74>    },
</span><span style=color:#e6db74>    &#34;size&#34;: 10,
</span><span style=color:#e6db74>    &#34;from&#34;: 0
</span><span style=color:#e6db74>  }&#39;</span>
</code></pre></div><h2 id=evaluating-response-time>Evaluating Response Time<a hidden class=anchor aria-hidden=true href=#evaluating-response-time>#</a></h2>
<p>With 25k documents, queries are speedy at around 10ms. I noticed a problem though when trying to query for the last page:</p>
<div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-json data-lang=json>{
  <span style=color:#f92672>&#34;error&#34;</span>: {
    <span style=color:#f92672>&#34;root_cause&#34;</span>: [
      {
        <span style=color:#f92672>&#34;type&#34;</span>: <span style=color:#e6db74>&#34;illegal_argument_exception&#34;</span>,
        <span style=color:#f92672>&#34;reason&#34;</span>: <span style=color:#e6db74>&#34;Result window is too large, from + size must be less than or equal to: [10000] but was [20010]. See the scroll api for a more efficient way to request large data sets. This limit can be set by changing the [index.max_result_window] index level setting.&#34;</span>
      }
    ],
  <span style=color:#960050;background-color:#1e0010>...</span>
}
</code></pre></div><p>This is a nice descriptive error message. The default maximum documents allowed for from/size pagination is 10k and an error was thrown when I tried to start the query at 20k. I could of course, raise the max_result_window setting to something higher, but this is inefficient. The error message recommends using the scroll api, documented <a href=https://www.elastic.co/guide/en/elasticsearch/reference/current/paginate-search-results.html#scroll-search-results>here</a>. However&mldr;</p>
<blockquote>
<p>We no longer recommend using the scroll API for deep pagination. If you need to preserve the index state while paging through more than 10,000 hits, use the <code>search_after</code> parameter with a point in time (PIT).</p>
</blockquote>
<p>So what is <a href=https://www.elastic.co/guide/en/elasticsearch/reference/current/paginate-search-results.html#search-after><code>search_after</code> with PIT</a>? It&rsquo;s actually pretty simple.</p>
<p>Before making calls that require deep pagination, first request a Point in Time ID from the index.</p>
<div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>curl -X POST <span style=color:#e6db74>&#39;localhost:9200/assets/_pit?keep_alive=1m&#39;</span>
</code></pre></div><p>This will return a json response that looks like this:</p>
<div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-json data-lang=json>{<span style=color:#f92672>&#34;id&#34;</span>:<span style=color:#e6db74>&#34;z4S1AwEGYXNzZXRzFlRLaVNVSFpyVHMtNS1hZ1ZKS1pXNWcAFnREZjlWcFBxVFN5UkJuTk1XQ0tPOVEAAAAAAAAAAJ4WSU5tZ21Xc2NTYy03OHVwUUg4Z2pBQQABFlRLaVNVSFpyVHMtNS1hZ1ZKS1pXNWcAAA==&#34;</span>}
</code></pre></div><p>The PIT <code>id</code> above can be used in a search query like this:</p>
<div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>curl -X GET <span style=color:#e6db74>&#39;localhost:9200/_search&#39;</span> <span style=color:#ae81ff>\
</span><span style=color:#ae81ff></span>  -H <span style=color:#e6db74>&#39;Content-Type: application/json&#39;</span> <span style=color:#ae81ff>\
</span><span style=color:#ae81ff></span>  -d <span style=color:#e6db74>&#39;{
</span><span style=color:#e6db74>    &#34;query&#34;: {
</span><span style=color:#e6db74>      &#34;match&#34;: {
</span><span style=color:#e6db74>        &#34;name&#34;: &#34;Test Data&#34;
</span><span style=color:#e6db74>      }
</span><span style=color:#e6db74>    },
</span><span style=color:#e6db74>    &#34;sort&#34;: [{ &#34;name.keyword&#34;: &#34;desc&#34; }],
</span><span style=color:#e6db74>    &#34;size&#34;: 10,
</span><span style=color:#e6db74>    &#34;pit&#34;: {
</span><span style=color:#e6db74>      &#34;id&#34;:&#34;z4S1AwEGYXNzZXRzFlRLaVNVSFpyVHMtNS1hZ1ZKS1pXNWcAFnREZjlWcFBxVFN5UkJuTk1XQ0tPOVEAAAAAAAAAAJ4WSU5tZ21Xc2NTYy03OHVwUUg4Z2pBQQABFlRLaVNVSFpyVHMtNS1hZ1ZKS1pXNWcAAA==&#34;,
</span><span style=color:#e6db74>      &#34;keep_alive&#34;: &#34;1m&#34;
</span><span style=color:#e6db74>    }
</span><span style=color:#e6db74>  }&#39;</span>
</code></pre></div><p>In response, I will receive documents 1-10 along with a <code>sort</code> field.</p>
<div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-json data-lang=json>{
  <span style=color:#960050;background-color:#1e0010>...</span>
  <span style=color:#f92672>&#34;sort&#34;</span>: [
    <span style=color:#e6db74>&#34;fffbfc0&#34;</span>,
    <span style=color:#ae81ff>25358</span>
  ]
}
</code></pre></div><p>To query for documents 11-20, I&rsquo;ll set the <code>search_after</code> parameter in my next query to the <code>sort</code> value, like this:</p>
<div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>curl -X GET <span style=color:#e6db74>&#39;localhost:9200/_search&#39;</span> <span style=color:#ae81ff>\
</span><span style=color:#ae81ff></span>  -H <span style=color:#e6db74>&#39;Content-Type: application/json&#39;</span> <span style=color:#ae81ff>\
</span><span style=color:#ae81ff></span>  -d <span style=color:#e6db74>&#39;{
</span><span style=color:#e6db74>    &#34;query&#34;: {
</span><span style=color:#e6db74>      &#34;match&#34;: {
</span><span style=color:#e6db74>        &#34;name&#34;: &#34;Test Data&#34;
</span><span style=color:#e6db74>      }
</span><span style=color:#e6db74>    },
</span><span style=color:#e6db74>    &#34;sort&#34;: [{ &#34;name.keyword&#34;: &#34;desc&#34; }],
</span><span style=color:#e6db74>    &#34;size&#34;: 10,
</span><span style=color:#e6db74>    &#34;pit&#34;: {
</span><span style=color:#e6db74>      &#34;id&#34;:&#34;z4S1AwEGYXNzZXRzFlRLaVNVSFpyVHMtNS1hZ1ZKS1pXNWcAFnREZjlWcFBxVFN5UkJuTk1XQ0tPOVEAAAAAAAAAAJ4WSU5tZ21Xc2NTYy03OHVwUUg4Z2pBQQABFlRLaVNVSFpyVHMtNS1hZ1ZKS1pXNWcAAA==&#34;,
</span><span style=color:#e6db74>      &#34;keep_alive&#34;: &#34;1m&#34;
</span><span style=color:#e6db74>    },
</span><span style=color:#e6db74>    &#34;search_after&#34;: [
</span><span style=color:#e6db74>      &#34;fffbfc0&#34;,
</span><span style=color:#e6db74>      25358
</span><span style=color:#e6db74>    ]
</span><span style=color:#e6db74>  }&#39;</span>
</code></pre></div><p>A downside to this approach is that querying the last page is not as simple as with the <code>from</code> parameter. An arbitrary page cannot be selected. Instead, a starting page must first be retreived. This will work fine for pagination with simple &ldquo;Previous&rdquo; and &ldquo;Next&rdquo; buttons. It does not work, however, for pagination that allows the user to select a specific page.</p>
<p>As a compromise, the UI could offer Previous/Next pagination until total query results are reduced below 10k, then offer the option to select a specific page. This could be suitable for cases when the need for deep pagination is present but uncommon.</p>
<h2 id=finishing-up-for-the-day>Finishing Up for the Day<a hidden class=anchor aria-hidden=true href=#finishing-up-for-the-day>#</a></h2>
<p>There is still more investigation needed. Analyzing performance on 25k documents is a good start, but I will need to increase that number quite a bit to get a realistic estimate of improvement over the current system. Also, I&rsquo;ve barely scratched the surface of Elasticsearch&rsquo;s capabilities. The research continues&mldr;</p>
</div>
<footer class=post-footer>
<nav class=paginav>
<a class=next href=https://www.taylordeckard.me/blog/posts/hugo-tutorial/>
<span class=title>Next Page »</span>
<br>
<span>Meta Post (Create a Blog with Hugo)</span>
</a>
</nav>
</footer>
</article>
</main>
<a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a>
<script>let menu=document.getElementById('menu');menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(a=>{a.addEventListener("click",function(b){b.preventDefault();var a=this.getAttribute("href").substr(1);window.matchMedia('(prefers-reduced-motion: reduce)').matches?document.querySelector(`[id='${decodeURIComponent(a)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(a)}']`).scrollIntoView({behavior:"smooth"}),a==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${a}`)})})</script>
<script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script>
<script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove('dark'),localStorage.setItem("pref-theme",'light')):(document.body.classList.add('dark'),localStorage.setItem("pref-theme",'dark'))})</script>
<script>document.querySelectorAll('pre > code').forEach(b=>{const c=b.parentNode.parentNode,a=document.createElement('button');a.classList.add('copy-code'),a.innerText='copy';function d(){a.innerText='copied!',setTimeout(()=>{a.innerText='copy'},2e3)}a.addEventListener('click',e=>{if('clipboard'in navigator){navigator.clipboard.writeText(b.textContent),d();return}const a=document.createRange();a.selectNodeContents(b);const c=window.getSelection();c.removeAllRanges(),c.addRange(a);try{document.execCommand('copy'),d()}catch(a){}c.removeRange(a)}),c.classList.contains("highlight")?c.appendChild(a):c.parentNode.firstChild==c||(b.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?b.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(a):b.parentNode.appendChild(a))})</script>
</body>
</html>