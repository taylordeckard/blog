<!doctype html><html lang=en dir=auto>
<head><meta charset=utf-8>
<meta http-equiv=x-ua-compatible content="IE=edge">
<meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no">
<meta name=robots content="index, follow">
<title>Elasticsearch Intro | Taylor's Blog</title>
<meta name=keywords content="programming,database,node.js">
<meta name=description content="Getting started with Elasticsearch">
<meta name=author content="Taylor Deckard">
<link rel=canonical href=https://www.taylordeckard.me/blog/posts/elasticsearch-intro/>
<link crossorigin=anonymous href=/blog/assets/css/stylesheet.min.css rel="preload stylesheet" as=style>
<script defer crossorigin=anonymous src=/blog/assets/js/highlight.min.js onload=hljs.initHighlightingOnLoad()></script>
<link rel=icon href=https://www.taylordeckard.me/blog/favicon.ico>
<link rel=icon type=image/png sizes=16x16 href=https://www.taylordeckard.me/blog/favicon-16x16.png>
<link rel=icon type=image/png sizes=32x32 href=https://www.taylordeckard.me/blog/favicon-32x32.png>
<link rel=apple-touch-icon href=https://www.taylordeckard.me/blog/apple-touch-icon.png>
<link rel=mask-icon href=https://www.taylordeckard.me/blog/safari-pinned-tab.svg>
<meta name=theme-color content="#2e2e33">
<meta name=msapplication-TileColor content="#2e2e33">
<meta name=generator content="Hugo 0.92.0">
<noscript>
<style>#theme-toggle,.top-link{display:none}</style>
<style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--hljs-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style>
</noscript><meta property="og:title" content="Elasticsearch Intro">
<meta property="og:description" content="Getting started with Elasticsearch">
<meta property="og:type" content="article">
<meta property="og:url" content="https://www.taylordeckard.me/blog/posts/elasticsearch-intro/"><meta property="article:section" content="posts">
<meta property="article:published_time" content="2022-01-21T00:00:00+00:00">
<meta property="article:modified_time" content="2022-01-21T00:00:00+00:00"><meta property="og:site_name" content="Blog">
<meta name=twitter:card content="summary">
<meta name=twitter:title content="Elasticsearch Intro">
<meta name=twitter:description content="Getting started with Elasticsearch">
<script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":2,"name":"Posts","item":"https://www.taylordeckard.me/blog/posts/"},{"@type":"ListItem","position":3,"name":"Elasticsearch Intro","item":"https://www.taylordeckard.me/blog/posts/elasticsearch-intro/"}]}</script>
<script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Elasticsearch Intro","name":"Elasticsearch Intro","description":"Getting started with Elasticsearch","keywords":["programming","database","node.js"],"articleBody":"I’ve started working on a proof-of-concept to improve query performance of a large dataset (5M+ rows.) The data is currently stored in a MySQL database.\nThe service is required to search, sort, filter, and paginate the data. Nowadays, these requirements are standard practice. However, with such a large dataset, some of the database queries are taking  3 seconds, even with table partitioning.\nMy theory is that Elasticsearch will perform better than RDBMS for this use case. Only one way to find out…\nRunning Elasticsearch Locally I already have Docker and docker-compose installed, so I just need to create a docker-compose config.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75  version:\"3.9\"# Run 3 Elasticsearch containers simultaneously (es01, es02, es03)services:es01:image:docker.elastic.co/elasticsearch/elasticsearch:7.16.3container_name:es01environment:- node.name=es01- cluster.name=es-docker-cluster- discovery.seed_hosts=es02,es03- cluster.initial_master_nodes=es01,es02,es03- bootstrap.memory_lock=true- \"ES_JAVA_OPTS=-Xms512m -Xmx512m\"# Allow unlimited memory to be locked by this container processulimits:memlock:soft:-1hard:-1volumes:# Map data01 directory to container (for persistence)- data01:/usr/share/elasticsearch/dataports:# Map local port 9200 to container port 9200- 9200:9200networks:- elastices02:image:docker.elastic.co/elasticsearch/elasticsearch:7.16.3container_name:es02environment:- node.name=es02- cluster.name=es-docker-cluster- discovery.seed_hosts=es01,es03- cluster.initial_master_nodes=es01,es02,es03- bootstrap.memory_lock=true- \"ES_JAVA_OPTS=-Xms512m -Xmx512m\"ulimits:memlock:soft:-1hard:-1volumes:- data02:/usr/share/elasticsearch/datanetworks:- elastices03:image:docker.elastic.co/elasticsearch/elasticsearch:7.16.3container_name:es03environment:- node.name=es03- cluster.name=es-docker-cluster- discovery.seed_hosts=es01,es02- cluster.initial_master_nodes=es01,es02,es03- bootstrap.memory_lock=true- \"ES_JAVA_OPTS=-Xms512m -Xmx512m\"ulimits:memlock:soft:-1hard:-1volumes:- data03:/usr/share/elasticsearch/datanetworks:- elasticvolumes:data01:driver:localdata02:driver:localdata03:driver:local# ES Nodes run on a shared network that is bridged to the local machinenetworks:elastic:driver:bridge  Elasticsearch should be ready to go. Before running though, I’ll need to bump up Docker’s memory resources to  4GB. Start docker-compose with\n1  docker-compose up   Creating an Index Now that Elasticsearch is running, it’s time to learn how to use it: REST API docs.\nFirst, I want to create an index (table in SQL) for my dataset so I can add documents (rows) to it.\nI create an index called assets (API Docs):\n1  curl -X PUT 'http://localhost:9200/assets'   The following is shown in the docker-compose logs:\n1  {\"type\": \"server\", \"timestamp\": \"2022-01-21T15:50:11,935Z\", \"level\": \"INFO\", \"component\": \"o.e.c.m.MetadataCreateIndexService\", \"cluster.name\": \"es-docker-cluster\", \"node.name\": \"es02\", \"message\": \"[assets] creating index, cause [api], templates [], shards [1]/[1]\", \"cluster.uuid\": \"6BFPOn84Q8qFIfS2FSmsBw\", \"node.id\": \"Sm9PsrFzTu6tkK0tAt0qvw\" }   which indicates the index creation was successful.\nAnother way to check that the index creation was successful is to do\n1  curl --head 'http://localhost:9200/assets'   A 200 response means that the index exists, whereas 404 means it doesn’t.\nWorking with Documents Adding a document to the index is simple (API Docs):\n1 2 3 4 5  curl -X POST 'http://localhost:9200/assets/_doc/' \\  -H 'Content-Type: application/json' \\  -d '{ \"name\": \"Test Data\" }'   Then retrieve the document with (API Docs)\n1 2 3 4 5 6 7 8 9  curl -X GET 'localhost:9200/assets/_search?pretty' \\  -H 'Content-Type: application/json' \\  -d '{ \"query\": { \"match\": { \"name\": \"Test Data\" } } }'   Mocking Data The next step is to populate the local Elasticsearch database up with mock documents to mimic a production environment. I’m familiar with Node.js, so I’ll write a quick script to do this.\nCreate a directory to contain the script.\n1  mkdir mock-data \u0026\u0026 cd mock-data   Add a package.json\n1  npm init -y   Now, I’ll add a couple of dependencies. To create mock-data I’m using a fork of faker.js, (community-faker.) I’ll also use node-fetch to make HTTP requests.\n1 2  # shorthand for npm install, -S writes the dependencies to package.json npm i -S community-faker node-fetch   Next, write the script: index.js. (Note: I’m using Node.js v14.16.1)\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37  import faker from 'community-faker'; import fetch from 'node-fetch'; // Number of documents to insert const NUM_DOCUMENTS = 25000; const ES_HOST = 'http://localhost:9200'; // The ES index to insert records into const INDEX = 'assets'; (async function run () { for (let i = 0; i  NUM_DOCUMENTS; i++) { const doc = { name: faker.name.findName(), id: faker.datatype.uuid(), }; // Send document to ES via http API  const response = await fetch(`${ES_HOST}/${INDEX}/_doc`, { // node-fetch requires stringifying the json body  body: JSON.stringify(doc), headers: { // Content-Type header is required for ES APIs with bodies  'Content-Type': 'application/json', }, method: 'post', }) if (!response.ok) { // If an error occurred, log the error and exit  console.log(await response.json()); break; } // Write out progress on a single line  process.stdout.clearLine(); process.stdout.cursorTo(0); process.stdout.write(`${i + 1}/ ${ NUM_DOCUMENTS }documents inserted`); } })();   Finally, run the script.\n1  node index.js   When finished, confirm the data has been ingested.\n1 2 3 4 5 6 7 8 9  curl -X GET 'localhost:9200/assets/_search?pretty' \\  -H 'Content-Type: application/json' \\  -d '{ \"query\": { \"match_all\": {} }, \"size\": 10, \"from\": 0 }'   This works fine, but takes a while. There is a more optimal solution for bulk insert: make use of the Elasticsearch bulk helper. I can use the javascript Elasticsearch client @elastic/elasticsearch.\n1  npm i -S @elastic/elasticsearch   I’ll also get rid of node-fetch since I’ll be using the ES Client to make requests to Elasticsearch.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41  import faker from 'community-faker'; import { Client } from '@elastic/elasticsearch'; const NUM_DOCUMENTS = 10000; const HOST = 'http://localhost:9200'; const INDEX = 'assets'; // Global variable to count the number of records that have been inserted let numInserted = 0; // Function to generate a random document function getRandomDocument () { return { name: faker.name.findName(), id: faker.datatype.uuid(), }; } // Generator that yields new random documents async function * docGenerator () { for (let i = 0; i  NUM_DOCUMENTS; i++) { yield getRandomDocument(); } } (async function run () { const client = new Client({ node: ES_HOST }); const result = await client.helpers.bulk({ // The bulk API accepts a generator as input.  datasource: docGenerator(), onDocument (doc) { numInserted++; process.stdout.clearLine(); process.stdout.cursorTo(0); process.stdout.write(`${ numInserted }/ ${ NUM_DOCUMENTS }documents inserted`); return { // Creates a new document in the index  create: { _index: INDEX }, }; }, }) })();   Running the script again only takes a few seconds. (Before, it could take a few minutes, depending on how many documents were being inserted.)\nEvaluating Response Time With 25k documents, queries are speedy at around 10ms. I noticed a problem though when trying to query for the last page:\n1 2 3 4 5 6 7 8 9 10  { \"error\": { \"root_cause\": [ { \"type\": \"illegal_argument_exception\", \"reason\": \"Result window is too large, from + size must be less than or equal to: [10000] but was [20010]. See the scroll api for a more efficient way to request large data sets. This limit can be set by changing the [index.max_result_window] index level setting.\" } ], ... }   This is a nice descriptive error message. The default maximum documents allowed for from/size pagination is 10k and an error was thrown when I tried to start the query at 20k. I could of course, raise the max_result_window setting to something higher, but this is inefficient. The error message recommends using the scroll api, documented here. However…\n We no longer recommend using the scroll API for deep pagination. If you need to preserve the index state while paging through more than 10,000 hits, use the search_after parameter with a point in time (PIT).\n So what is search_after with PIT? It’s actually pretty simple.\nBefore making calls that require deep pagination, first request a Point in Time ID from the index.\n1  curl -X POST 'localhost:9200/assets/_pit?keep_alive=1m'   This will return a json response that looks like this:\n1  {\"id\":\"z4S1AwEGYXNzZXRzFlRLaVNVSFpyVHMtNS1hZ1ZKS1pXNWcAFnREZjlWcFBxVFN5UkJuTk1XQ0tPOVEAAAAAAAAAAJ4WSU5tZ21Xc2NTYy03OHVwUUg4Z2pBQQABFlRLaVNVSFpyVHMtNS1hZ1ZKS1pXNWcAAA==\"}   The PIT id above can be used in a search query like this:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15  curl -X GET 'localhost:9200/_search' \\  -H 'Content-Type: application/json' \\  -d '{ \"query\": { \"match\": { \"name\": \"Test Data\" } }, \"sort\": [{ \"name.keyword\": \"desc\" }], \"size\": 10, \"pit\": { \"id\":\"z4S1AwEGYXNzZXRzFlRLaVNVSFpyVHMtNS1hZ1ZKS1pXNWcAFnREZjlWcFBxVFN5UkJuTk1XQ0tPOVEAAAAAAAAAAJ4WSU5tZ21Xc2NTYy03OHVwUUg4Z2pBQQABFlRLaVNVSFpyVHMtNS1hZ1ZKS1pXNWcAAA==\", \"keep_alive\": \"1m\" } }'   In response, I will receive documents 1-10 along with a sort field.\n1 2 3 4 5 6 7  { ... \"sort\": [ \"fffbfc0\", 25358 ] }   To query for documents 11-20, I’ll set the search_after parameter in my next query to the sort value, like this:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19  curl -X GET 'localhost:9200/_search' \\  -H 'Content-Type: application/json' \\  -d '{ \"query\": { \"match\": { \"name\": \"Test Data\" } }, \"sort\": [{ \"name.keyword\": \"desc\" }], \"size\": 10, \"pit\": { \"id\":\"z4S1AwEGYXNzZXRzFlRLaVNVSFpyVHMtNS1hZ1ZKS1pXNWcAFnREZjlWcFBxVFN5UkJuTk1XQ0tPOVEAAAAAAAAAAJ4WSU5tZ21Xc2NTYy03OHVwUUg4Z2pBQQABFlRLaVNVSFpyVHMtNS1hZ1ZKS1pXNWcAAA==\", \"keep_alive\": \"1m\" }, \"search_after\": [ \"fffbfc0\", 25358 ] }'   A downside to this approach is that querying the last page is not as simple as with the from parameter. An arbitrary page cannot be selected. Instead, a starting page must first be retreived. This will work fine for pagination with simple “Previous” and “Next” buttons. It does not work, however, for pagination that allows the user to select a specific page.\nAs a compromise, the UI could offer Previous/Next pagination until total query results are reduced below 10k, then offer the option to select a specific page. This could be suitable for cases when the need for deep pagination is present but uncommon.\nFinishing Up for the Day There is still more investigation needed. Analyzing performance on 25k documents is a good start, but I will need to increase that number quite a bit to get a realistic estimate of improvement over the current system. Also, I’ve barely scratched the surface of Elasticsearch’s capabilities. The research continues…\n","wordCount":"1619","inLanguage":"en","datePublished":"2022-01-21T00:00:00Z","dateModified":"2022-01-21T00:00:00Z","author":{"@type":"Person","name":"Taylor Deckard"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://www.taylordeckard.me/blog/posts/elasticsearch-intro/"},"publisher":{"@type":"Organization","name":"Taylor's Blog","logo":{"@type":"ImageObject","url":"https://www.taylordeckard.me/blog/favicon.ico"}}}</script>
</head>
<body id=top>
<script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add('dark'):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove('dark'):window.matchMedia('(prefers-color-scheme: dark)').matches&&document.body.classList.add('dark')</script>
<header class=header>
<nav class=nav>
<div class=logo>
<a href=https://www.taylordeckard.me/blog accesskey=h title="Blog (Alt + H)">Blog</a>
<span class=logo-switches>
<button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg>
</button>
</span>
</div>
<ul id=menu>
<li>
<a href=https://www.taylordeckard.me/blog/archives title=Archives>
<span>Archives</span>
</a>
</li>
<li>
<a href=https://www.taylordeckard.me/blog/search title="Search (Alt + /)" accesskey=/>
<span>Search</span>
</a>
</li>
<li>
<a href=https://www.taylordeckard.me/blog/tags title=Tags>
<span>Tags</span>
</a>
</li>
<li>
<a href=https://taylordeckard.me title=taylordeckard.me>
<span>taylordeckard.me</span>
</a>
</li>
</ul>
</nav>
</header>
<main class=main>
<article class=post-single>
<header class=post-header>
<div class=breadcrumbs><a href=https://www.taylordeckard.me/blog>Home</a>&nbsp;»&nbsp;<a href=https://www.taylordeckard.me/blog/posts/>Posts</a></div>
<h1 class=post-title>
Elasticsearch Intro
</h1>
<div class=post-description>
Getting started with Elasticsearch
</div>
<div class=post-meta><span title="2022-01-21 00:00:00 +0000 UTC">January 21, 2022</span>&nbsp;·&nbsp;8 min&nbsp;·&nbsp;Taylor Deckard&nbsp;|&nbsp;<a href=https://github.com/taylordeckard/blog/tree/main/content/posts/elasticsearch-intro.md rel="noopener noreferrer" target=_blank>Suggest Changes</a>
</div>
</header>
<div class=post-content><p>I&rsquo;ve started working on a proof-of-concept to improve query performance of a large dataset (5M+ rows.) The data is currently stored in a MySQL database.</p>
<p>The service is required to search, sort, filter, and paginate the data. Nowadays, these requirements are standard practice. However, with such a large dataset, some of the database queries are taking > 3 seconds, even with table partitioning.</p>
<p>My theory is that Elasticsearch will perform better than RDBMS for this use case. Only one way to find out&mldr;</p>
<p><img loading=lazy src=/blog/images/elasticsearch-intro/dog_experiment.gif alt="Time to experiment!">
</p>
<h2 id=running-elasticsearch-locally>Running Elasticsearch Locally<a hidden class=anchor aria-hidden=true href=#running-elasticsearch-locally>#</a></h2>
<p>I already have <a href=https://www.docker.com/products/docker-desktop>Docker</a> and <a href=https://docs.docker.com/compose/install/>docker-compose</a> installed, so I just need to create a docker-compose config.</p>
<div class=highlight><div class=chroma>
<table class=lntable><tr><td class=lntd>
<pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span><span class=lnt>17
</span><span class=lnt>18
</span><span class=lnt>19
</span><span class=lnt>20
</span><span class=lnt>21
</span><span class=lnt>22
</span><span class=lnt>23
</span><span class=lnt>24
</span><span class=lnt>25
</span><span class=lnt>26
</span><span class=lnt>27
</span><span class=lnt>28
</span><span class=lnt>29
</span><span class=lnt>30
</span><span class=lnt>31
</span><span class=lnt>32
</span><span class=lnt>33
</span><span class=lnt>34
</span><span class=lnt>35
</span><span class=lnt>36
</span><span class=lnt>37
</span><span class=lnt>38
</span><span class=lnt>39
</span><span class=lnt>40
</span><span class=lnt>41
</span><span class=lnt>42
</span><span class=lnt>43
</span><span class=lnt>44
</span><span class=lnt>45
</span><span class=lnt>46
</span><span class=lnt>47
</span><span class=lnt>48
</span><span class=lnt>49
</span><span class=lnt>50
</span><span class=lnt>51
</span><span class=lnt>52
</span><span class=lnt>53
</span><span class=lnt>54
</span><span class=lnt>55
</span><span class=lnt>56
</span><span class=lnt>57
</span><span class=lnt>58
</span><span class=lnt>59
</span><span class=lnt>60
</span><span class=lnt>61
</span><span class=lnt>62
</span><span class=lnt>63
</span><span class=lnt>64
</span><span class=lnt>65
</span><span class=lnt>66
</span><span class=lnt>67
</span><span class=lnt>68
</span><span class=lnt>69
</span><span class=lnt>70
</span><span class=lnt>71
</span><span class=lnt>72
</span><span class=lnt>73
</span><span class=lnt>74
</span><span class=lnt>75
</span></code></pre></td>
<td class=lntd>
<pre tabindex=0 class=chroma><code class=language-yaml data-lang=yaml><span class=nt>version</span><span class=p>:</span><span class=w> </span><span class=s2>&#34;3.9&#34;</span><span class=w>
</span><span class=w></span><span class=c># Run 3 Elasticsearch containers simultaneously (es01, es02, es03)</span><span class=w>
</span><span class=w></span><span class=nt>services</span><span class=p>:</span><span class=w>
</span><span class=w>  </span><span class=nt>es01</span><span class=p>:</span><span class=w>
</span><span class=w>    </span><span class=nt>image</span><span class=p>:</span><span class=w> </span><span class=l>docker.elastic.co/elasticsearch/elasticsearch:7.16.3</span><span class=w>
</span><span class=w>    </span><span class=nt>container_name</span><span class=p>:</span><span class=w> </span><span class=l>es01</span><span class=w>
</span><span class=w>    </span><span class=nt>environment</span><span class=p>:</span><span class=w>
</span><span class=w>      </span>- <span class=l>node.name=es01</span><span class=w>
</span><span class=w>      </span>- <span class=l>cluster.name=es-docker-cluster</span><span class=w>
</span><span class=w>      </span>- <span class=l>discovery.seed_hosts=es02,es03</span><span class=w>
</span><span class=w>      </span>- <span class=l>cluster.initial_master_nodes=es01,es02,es03</span><span class=w>
</span><span class=w>      </span>- <span class=l>bootstrap.memory_lock=true</span><span class=w>
</span><span class=w>      </span>- <span class=s2>&#34;ES_JAVA_OPTS=-Xms512m -Xmx512m&#34;</span><span class=w>
</span><span class=w>    </span><span class=c># Allow unlimited memory to be locked by this container process</span><span class=w>
</span><span class=w>    </span><span class=nt>ulimits</span><span class=p>:</span><span class=w>
</span><span class=w>      </span><span class=nt>memlock</span><span class=p>:</span><span class=w>
</span><span class=w>        </span><span class=nt>soft</span><span class=p>:</span><span class=w> </span>-<span class=m>1</span><span class=w>
</span><span class=w>        </span><span class=nt>hard</span><span class=p>:</span><span class=w> </span>-<span class=m>1</span><span class=w>
</span><span class=w>    </span><span class=nt>volumes</span><span class=p>:</span><span class=w>
</span><span class=w>      </span><span class=c># Map data01 directory to container (for persistence)</span><span class=w>
</span><span class=w>      </span>- <span class=l>data01:/usr/share/elasticsearch/data</span><span class=w>
</span><span class=w>    </span><span class=nt>ports</span><span class=p>:</span><span class=w>
</span><span class=w>      </span><span class=c># Map local port 9200 to container port 9200</span><span class=w>
</span><span class=w>      </span>- <span class=m>9200</span><span class=p>:</span><span class=m>9200</span><span class=w>
</span><span class=w>    </span><span class=nt>networks</span><span class=p>:</span><span class=w>
</span><span class=w>      </span>- <span class=l>elastic</span><span class=w>
</span><span class=w>  </span><span class=nt>es02</span><span class=p>:</span><span class=w>
</span><span class=w>    </span><span class=nt>image</span><span class=p>:</span><span class=w> </span><span class=l>docker.elastic.co/elasticsearch/elasticsearch:7.16.3</span><span class=w>
</span><span class=w>    </span><span class=nt>container_name</span><span class=p>:</span><span class=w> </span><span class=l>es02</span><span class=w>
</span><span class=w>    </span><span class=nt>environment</span><span class=p>:</span><span class=w>
</span><span class=w>      </span>- <span class=l>node.name=es02</span><span class=w>
</span><span class=w>      </span>- <span class=l>cluster.name=es-docker-cluster</span><span class=w>
</span><span class=w>      </span>- <span class=l>discovery.seed_hosts=es01,es03</span><span class=w>
</span><span class=w>      </span>- <span class=l>cluster.initial_master_nodes=es01,es02,es03</span><span class=w>
</span><span class=w>      </span>- <span class=l>bootstrap.memory_lock=true</span><span class=w>
</span><span class=w>      </span>- <span class=s2>&#34;ES_JAVA_OPTS=-Xms512m -Xmx512m&#34;</span><span class=w>
</span><span class=w>    </span><span class=nt>ulimits</span><span class=p>:</span><span class=w>
</span><span class=w>      </span><span class=nt>memlock</span><span class=p>:</span><span class=w>
</span><span class=w>        </span><span class=nt>soft</span><span class=p>:</span><span class=w> </span>-<span class=m>1</span><span class=w>
</span><span class=w>        </span><span class=nt>hard</span><span class=p>:</span><span class=w> </span>-<span class=m>1</span><span class=w>
</span><span class=w>    </span><span class=nt>volumes</span><span class=p>:</span><span class=w>
</span><span class=w>      </span>- <span class=l>data02:/usr/share/elasticsearch/data</span><span class=w>
</span><span class=w>    </span><span class=nt>networks</span><span class=p>:</span><span class=w>
</span><span class=w>      </span>- <span class=l>elastic</span><span class=w>
</span><span class=w>  </span><span class=nt>es03</span><span class=p>:</span><span class=w>
</span><span class=w>    </span><span class=nt>image</span><span class=p>:</span><span class=w> </span><span class=l>docker.elastic.co/elasticsearch/elasticsearch:7.16.3</span><span class=w>
</span><span class=w>    </span><span class=nt>container_name</span><span class=p>:</span><span class=w> </span><span class=l>es03</span><span class=w>
</span><span class=w>    </span><span class=nt>environment</span><span class=p>:</span><span class=w>
</span><span class=w>      </span>- <span class=l>node.name=es03</span><span class=w>
</span><span class=w>      </span>- <span class=l>cluster.name=es-docker-cluster</span><span class=w>
</span><span class=w>      </span>- <span class=l>discovery.seed_hosts=es01,es02</span><span class=w>
</span><span class=w>      </span>- <span class=l>cluster.initial_master_nodes=es01,es02,es03</span><span class=w>
</span><span class=w>      </span>- <span class=l>bootstrap.memory_lock=true</span><span class=w>
</span><span class=w>      </span>- <span class=s2>&#34;ES_JAVA_OPTS=-Xms512m -Xmx512m&#34;</span><span class=w>
</span><span class=w>    </span><span class=nt>ulimits</span><span class=p>:</span><span class=w>
</span><span class=w>      </span><span class=nt>memlock</span><span class=p>:</span><span class=w>
</span><span class=w>        </span><span class=nt>soft</span><span class=p>:</span><span class=w> </span>-<span class=m>1</span><span class=w>
</span><span class=w>        </span><span class=nt>hard</span><span class=p>:</span><span class=w> </span>-<span class=m>1</span><span class=w>
</span><span class=w>    </span><span class=nt>volumes</span><span class=p>:</span><span class=w>
</span><span class=w>      </span>- <span class=l>data03:/usr/share/elasticsearch/data</span><span class=w>
</span><span class=w>    </span><span class=nt>networks</span><span class=p>:</span><span class=w>
</span><span class=w>      </span>- <span class=l>elastic</span><span class=w>
</span><span class=w>
</span><span class=w></span><span class=nt>volumes</span><span class=p>:</span><span class=w>
</span><span class=w>  </span><span class=nt>data01</span><span class=p>:</span><span class=w>
</span><span class=w>    </span><span class=nt>driver</span><span class=p>:</span><span class=w> </span><span class=l>local</span><span class=w>
</span><span class=w>  </span><span class=nt>data02</span><span class=p>:</span><span class=w>
</span><span class=w>    </span><span class=nt>driver</span><span class=p>:</span><span class=w> </span><span class=l>local</span><span class=w>
</span><span class=w>  </span><span class=nt>data03</span><span class=p>:</span><span class=w>
</span><span class=w>    </span><span class=nt>driver</span><span class=p>:</span><span class=w> </span><span class=l>local</span><span class=w>
</span><span class=w>
</span><span class=w></span><span class=c># ES Nodes run on a shared network that is bridged to the local machine</span><span class=w>
</span><span class=w></span><span class=nt>networks</span><span class=p>:</span><span class=w>
</span><span class=w>  </span><span class=nt>elastic</span><span class=p>:</span><span class=w>
</span><span class=w>    </span><span class=nt>driver</span><span class=p>:</span><span class=w> </span><span class=l>bridge</span><span class=w>
</span></code></pre></td></tr></table>
</div>
</div><p>Elasticsearch should be ready to go. Before running though, I&rsquo;ll need to bump up Docker&rsquo;s memory resources to > 4GB.
<img loading=lazy src=/blog/images/elasticsearch-intro/docker_resources.png alt="Docker Resources">
</p>
<p>Start docker-compose with</p>
<div class=highlight><div class=chroma>
<table class=lntable><tr><td class=lntd>
<pre tabindex=0 class=chroma><code><span class=lnt>1
</span></code></pre></td>
<td class=lntd>
<pre tabindex=0 class=chroma><code class=language-bash data-lang=bash>docker-compose up
</code></pre></td></tr></table>
</div>
</div><h2 id=creating-an-index>Creating an Index<a hidden class=anchor aria-hidden=true href=#creating-an-index>#</a></h2>
<p>Now that Elasticsearch is running, it&rsquo;s time to learn how to use it: <a href=https://www.elastic.co/guide/en/elasticsearch/reference/current/rest-apis.html>REST API docs</a>.</p>
<p>First, I want to create an <em>index</em> (table in SQL) for my dataset so I can add <em>documents</em> (rows) to it.</p>
<p>I create an index called <code>assets</code> (<a href=https://www.elastic.co/guide/en/elasticsearch/reference/current/indices-create-index.html>API Docs</a>):</p>
<div class=highlight><div class=chroma>
<table class=lntable><tr><td class=lntd>
<pre tabindex=0 class=chroma><code><span class=lnt>1
</span></code></pre></td>
<td class=lntd>
<pre tabindex=0 class=chroma><code class=language-bash data-lang=bash>curl -X PUT <span class=s1>&#39;http://localhost:9200/assets&#39;</span>
</code></pre></td></tr></table>
</div>
</div><p>The following is shown in the docker-compose logs:</p>
<div class=highlight><div class=chroma>
<table class=lntable><tr><td class=lntd>
<pre tabindex=0 class=chroma><code><span class=lnt>1
</span></code></pre></td>
<td class=lntd>
<pre tabindex=0 class=chroma><code class=language-json data-lang=json><span class=p>{</span><span class=nt>&#34;type&#34;</span><span class=p>:</span> <span class=s2>&#34;server&#34;</span><span class=p>,</span> <span class=nt>&#34;timestamp&#34;</span><span class=p>:</span> <span class=s2>&#34;2022-01-21T15:50:11,935Z&#34;</span><span class=p>,</span> <span class=nt>&#34;level&#34;</span><span class=p>:</span> <span class=s2>&#34;INFO&#34;</span><span class=p>,</span> <span class=nt>&#34;component&#34;</span><span class=p>:</span> <span class=s2>&#34;o.e.c.m.MetadataCreateIndexService&#34;</span><span class=p>,</span> <span class=nt>&#34;cluster.name&#34;</span><span class=p>:</span> <span class=s2>&#34;es-docker-cluster&#34;</span><span class=p>,</span> <span class=nt>&#34;node.name&#34;</span><span class=p>:</span> <span class=s2>&#34;es02&#34;</span><span class=p>,</span> <span class=nt>&#34;message&#34;</span><span class=p>:</span> <span class=s2>&#34;[assets] creating index, cause [api], templates [], shards [1]/[1]&#34;</span><span class=p>,</span> <span class=nt>&#34;cluster.uuid&#34;</span><span class=p>:</span> <span class=s2>&#34;6BFPOn84Q8qFIfS2FSmsBw&#34;</span><span class=p>,</span> <span class=nt>&#34;node.id&#34;</span><span class=p>:</span> <span class=s2>&#34;Sm9PsrFzTu6tkK0tAt0qvw&#34;</span>  <span class=p>}</span>
</code></pre></td></tr></table>
</div>
</div><p>which indicates the index creation was successful.</p>
<p>Another way to check that the index creation was successful is to do</p>
<div class=highlight><div class=chroma>
<table class=lntable><tr><td class=lntd>
<pre tabindex=0 class=chroma><code><span class=lnt>1
</span></code></pre></td>
<td class=lntd>
<pre tabindex=0 class=chroma><code class=language-bash data-lang=bash>curl --head <span class=s1>&#39;http://localhost:9200/assets&#39;</span>
</code></pre></td></tr></table>
</div>
</div><p>A 200 response means that the index exists, whereas 404 means it doesn&rsquo;t.</p>
<h2 id=working-with-documents>Working with Documents<a hidden class=anchor aria-hidden=true href=#working-with-documents>#</a></h2>
<p>Adding a document to the index is simple (<a href=https://www.elastic.co/guide/en/elasticsearch/reference/current/docs-index_.html>API Docs</a>):</p>
<div class=highlight><div class=chroma>
<table class=lntable><tr><td class=lntd>
<pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span><span class=lnt>5
</span></code></pre></td>
<td class=lntd>
<pre tabindex=0 class=chroma><code class=language-bash data-lang=bash>curl -X POST  <span class=s1>&#39;http://localhost:9200/assets/_doc/&#39;</span> <span class=se>\
</span><span class=se></span>  -H <span class=s1>&#39;Content-Type: application/json&#39;</span> <span class=se>\
</span><span class=se></span>  -d <span class=s1>&#39;{
</span><span class=s1>    &#34;name&#34;: &#34;Test Data&#34;
</span><span class=s1>  }&#39;</span>
</code></pre></td></tr></table>
</div>
</div><p>Then retrieve the document with (<a href=https://www.elastic.co/guide/en/elasticsearch/reference/current/search-search.html>API Docs</a>)</p>
<div class=highlight><div class=chroma>
<table class=lntable><tr><td class=lntd>
<pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span><span class=lnt>5
</span><span class=lnt>6
</span><span class=lnt>7
</span><span class=lnt>8
</span><span class=lnt>9
</span></code></pre></td>
<td class=lntd>
<pre tabindex=0 class=chroma><code class=language-bash data-lang=bash>curl -X GET <span class=s1>&#39;localhost:9200/assets/_search?pretty&#39;</span> <span class=se>\
</span><span class=se></span>  -H <span class=s1>&#39;Content-Type: application/json&#39;</span> <span class=se>\
</span><span class=se></span>  -d <span class=s1>&#39;{
</span><span class=s1>    &#34;query&#34;: {
</span><span class=s1>      &#34;match&#34;:  {
</span><span class=s1>        &#34;name&#34;: &#34;Test Data&#34;
</span><span class=s1>      }
</span><span class=s1>    }
</span><span class=s1>  }&#39;</span>
</code></pre></td></tr></table>
</div>
</div><h2 id=mocking-data>Mocking Data<a hidden class=anchor aria-hidden=true href=#mocking-data>#</a></h2>
<p>The next step is to populate the local Elasticsearch database up with mock documents to mimic a production environment. I&rsquo;m familiar with Node.js, so I&rsquo;ll write a quick script to do this.</p>
<p>Create a directory to contain the script.</p>
<div class=highlight><div class=chroma>
<table class=lntable><tr><td class=lntd>
<pre tabindex=0 class=chroma><code><span class=lnt>1
</span></code></pre></td>
<td class=lntd>
<pre tabindex=0 class=chroma><code class=language-bash data-lang=bash>mkdir mock-data <span class=o>&amp;&amp;</span> <span class=nb>cd</span> mock-data
</code></pre></td></tr></table>
</div>
</div><p>Add a package.json</p>
<div class=highlight><div class=chroma>
<table class=lntable><tr><td class=lntd>
<pre tabindex=0 class=chroma><code><span class=lnt>1
</span></code></pre></td>
<td class=lntd>
<pre tabindex=0 class=chroma><code class=language-bash data-lang=bash>npm init -y
</code></pre></td></tr></table>
</div>
</div><p>Now, I&rsquo;ll add a couple of dependencies. To create mock-data I&rsquo;m using a fork of faker.js, (<a href=https://www.npmjs.com/package/community-faker>community-faker</a>.) I&rsquo;ll also use <a href=https://www.npmjs.com/package/node-fetch>node-fetch</a> to make HTTP requests.</p>
<div class=highlight><div class=chroma>
<table class=lntable><tr><td class=lntd>
<pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span></code></pre></td>
<td class=lntd>
<pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=c1># shorthand for npm install, -S writes the dependencies to package.json</span>
npm i -S community-faker node-fetch
</code></pre></td></tr></table>
</div>
</div><p>Next, write the script: index.js. (<strong>Note:</strong> I&rsquo;m using Node.js v14.16.1)</p>
<div class=highlight><div class=chroma>
<table class=lntable><tr><td class=lntd>
<pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span><span class=lnt>17
</span><span class=lnt>18
</span><span class=lnt>19
</span><span class=lnt>20
</span><span class=lnt>21
</span><span class=lnt>22
</span><span class=lnt>23
</span><span class=lnt>24
</span><span class=lnt>25
</span><span class=lnt>26
</span><span class=lnt>27
</span><span class=lnt>28
</span><span class=lnt>29
</span><span class=lnt>30
</span><span class=lnt>31
</span><span class=lnt>32
</span><span class=lnt>33
</span><span class=lnt>34
</span><span class=lnt>35
</span><span class=lnt>36
</span><span class=lnt>37
</span></code></pre></td>
<td class=lntd>
<pre tabindex=0 class=chroma><code class=language-javascript data-lang=javascript><span class=kr>import</span> <span class=nx>faker</span> <span class=nx>from</span> <span class=s1>&#39;community-faker&#39;</span><span class=p>;</span>
<span class=kr>import</span> <span class=nx>fetch</span> <span class=nx>from</span> <span class=s1>&#39;node-fetch&#39;</span><span class=p>;</span>

<span class=c1>// Number of documents to insert
</span><span class=c1></span><span class=kr>const</span> <span class=nx>NUM_DOCUMENTS</span> <span class=o>=</span> <span class=mi>25000</span><span class=p>;</span>
<span class=kr>const</span> <span class=nx>ES_HOST</span> <span class=o>=</span> <span class=s1>&#39;http://localhost:9200&#39;</span><span class=p>;</span>
<span class=c1>// The ES index to insert records into
</span><span class=c1></span><span class=kr>const</span> <span class=nx>INDEX</span> <span class=o>=</span> <span class=s1>&#39;assets&#39;</span><span class=p>;</span>

<span class=p>(</span><span class=kr>async</span> <span class=kd>function</span> <span class=nx>run</span> <span class=p>()</span> <span class=p>{</span>
  <span class=k>for</span> <span class=p>(</span><span class=kd>let</span> <span class=nx>i</span> <span class=o>=</span> <span class=mi>0</span><span class=p>;</span> <span class=nx>i</span> <span class=o>&lt;</span> <span class=nx>NUM_DOCUMENTS</span><span class=p>;</span> <span class=nx>i</span><span class=o>++</span><span class=p>)</span> <span class=p>{</span>
    <span class=kr>const</span> <span class=nx>doc</span> <span class=o>=</span> <span class=p>{</span>
      <span class=nx>name</span><span class=o>:</span> <span class=nx>faker</span><span class=p>.</span><span class=nx>name</span><span class=p>.</span><span class=nx>findName</span><span class=p>(),</span>
      <span class=nx>id</span><span class=o>:</span> <span class=nx>faker</span><span class=p>.</span><span class=nx>datatype</span><span class=p>.</span><span class=nx>uuid</span><span class=p>(),</span>
    <span class=p>};</span>
    <span class=c1>// Send document to ES via http API
</span><span class=c1></span>    <span class=kr>const</span> <span class=nx>response</span> <span class=o>=</span> <span class=kr>await</span> <span class=nx>fetch</span><span class=p>(</span><span class=sb>`</span><span class=si>${</span><span class=nx>ES_HOST</span><span class=si>}</span><span class=sb>/</span><span class=si>${</span><span class=nx>INDEX</span><span class=si>}</span><span class=sb>/_doc`</span><span class=p>,</span> <span class=p>{</span>
      <span class=c1>// node-fetch requires stringifying the json body
</span><span class=c1></span>      <span class=nx>body</span><span class=o>:</span> <span class=nx>JSON</span><span class=p>.</span><span class=nx>stringify</span><span class=p>(</span><span class=nx>doc</span><span class=p>),</span>
      <span class=nx>headers</span><span class=o>:</span> <span class=p>{</span>
        <span class=c1>// Content-Type header is required for ES APIs with bodies
</span><span class=c1></span>        <span class=s1>&#39;Content-Type&#39;</span><span class=o>:</span> <span class=s1>&#39;application/json&#39;</span><span class=p>,</span>
      <span class=p>},</span>
      <span class=nx>method</span><span class=o>:</span> <span class=s1>&#39;post&#39;</span><span class=p>,</span>
    <span class=p>})</span>

    <span class=k>if</span> <span class=p>(</span><span class=o>!</span><span class=nx>response</span><span class=p>.</span><span class=nx>ok</span><span class=p>)</span> <span class=p>{</span>
      <span class=c1>// If an error occurred, log the error and exit
</span><span class=c1></span>      <span class=nx>console</span><span class=p>.</span><span class=nx>log</span><span class=p>(</span><span class=kr>await</span> <span class=nx>response</span><span class=p>.</span><span class=nx>json</span><span class=p>());</span>
      <span class=k>break</span><span class=p>;</span>
    <span class=p>}</span>
    <span class=c1>// Write out progress on a single line
</span><span class=c1></span>    <span class=nx>process</span><span class=p>.</span><span class=nx>stdout</span><span class=p>.</span><span class=nx>clearLine</span><span class=p>();</span>
    <span class=nx>process</span><span class=p>.</span><span class=nx>stdout</span><span class=p>.</span><span class=nx>cursorTo</span><span class=p>(</span><span class=mi>0</span><span class=p>);</span>
    <span class=nx>process</span><span class=p>.</span><span class=nx>stdout</span><span class=p>.</span><span class=nx>write</span><span class=p>(</span><span class=sb>`</span><span class=si>${</span><span class=nx>i</span> <span class=o>+</span> <span class=mi>1</span><span class=si>}</span><span class=sb> / </span><span class=si>${</span> <span class=nx>NUM_DOCUMENTS</span> <span class=si>}</span><span class=sb> documents inserted`</span><span class=p>);</span>
  <span class=p>}</span>
<span class=p>})();</span>
</code></pre></td></tr></table>
</div>
</div><p>Finally, run the script.</p>
<div class=highlight><div class=chroma>
<table class=lntable><tr><td class=lntd>
<pre tabindex=0 class=chroma><code><span class=lnt>1
</span></code></pre></td>
<td class=lntd>
<pre tabindex=0 class=chroma><code class=language-bash data-lang=bash>node index.js
</code></pre></td></tr></table>
</div>
</div><p>When finished, confirm the data has been ingested.</p>
<div class=highlight><div class=chroma>
<table class=lntable><tr><td class=lntd>
<pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span><span class=lnt>5
</span><span class=lnt>6
</span><span class=lnt>7
</span><span class=lnt>8
</span><span class=lnt>9
</span></code></pre></td>
<td class=lntd>
<pre tabindex=0 class=chroma><code class=language-bash data-lang=bash>curl -X GET <span class=s1>&#39;localhost:9200/assets/_search?pretty&#39;</span> <span class=se>\
</span><span class=se></span>  -H <span class=s1>&#39;Content-Type: application/json&#39;</span> <span class=se>\
</span><span class=se></span>  -d <span class=s1>&#39;{
</span><span class=s1>    &#34;query&#34;: {
</span><span class=s1>      &#34;match_all&#34;: {}
</span><span class=s1>    },
</span><span class=s1>    &#34;size&#34;: 10,
</span><span class=s1>    &#34;from&#34;: 0
</span><span class=s1>  }&#39;</span>
</code></pre></td></tr></table>
</div>
</div><p>This works fine, but takes a while. There is a more optimal solution for bulk insert: make use of the <a href=https://www.elastic.co/guide/en/elasticsearch/client/javascript-api/current/client-helpers.html#bulk-helper>Elasticsearch bulk helper</a>. I can use the javascript Elasticsearch client <code>@elastic/elasticsearch</code>.</p>
<div class=highlight><div class=chroma>
<table class=lntable><tr><td class=lntd>
<pre tabindex=0 class=chroma><code><span class=lnt>1
</span></code></pre></td>
<td class=lntd>
<pre tabindex=0 class=chroma><code class=language-bash data-lang=bash>npm i -S  @elastic/elasticsearch
</code></pre></td></tr></table>
</div>
</div><p>I&rsquo;ll also get rid of <code>node-fetch</code> since I&rsquo;ll be using the ES Client to make requests to Elasticsearch.</p>
<div class=highlight><div class=chroma>
<table class=lntable><tr><td class=lntd>
<pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span><span class=lnt>17
</span><span class=lnt>18
</span><span class=lnt>19
</span><span class=lnt>20
</span><span class=lnt>21
</span><span class=lnt>22
</span><span class=lnt>23
</span><span class=lnt>24
</span><span class=lnt>25
</span><span class=lnt>26
</span><span class=lnt>27
</span><span class=lnt>28
</span><span class=lnt>29
</span><span class=lnt>30
</span><span class=lnt>31
</span><span class=lnt>32
</span><span class=lnt>33
</span><span class=lnt>34
</span><span class=lnt>35
</span><span class=lnt>36
</span><span class=lnt>37
</span><span class=lnt>38
</span><span class=lnt>39
</span><span class=lnt>40
</span><span class=lnt>41
</span></code></pre></td>
<td class=lntd>
<pre tabindex=0 class=chroma><code class=language-javascript data-lang=javascript><span class=kr>import</span> <span class=nx>faker</span> <span class=nx>from</span> <span class=s1>&#39;community-faker&#39;</span><span class=p>;</span>
<span class=kr>import</span> <span class=p>{</span> <span class=nx>Client</span> <span class=p>}</span> <span class=nx>from</span> <span class=s1>&#39;@elastic/elasticsearch&#39;</span><span class=p>;</span>

<span class=kr>const</span> <span class=nx>NUM_DOCUMENTS</span> <span class=o>=</span> <span class=mi>10000</span><span class=p>;</span>
<span class=kr>const</span> <span class=nx>HOST</span> <span class=o>=</span> <span class=s1>&#39;http://localhost:9200&#39;</span><span class=p>;</span>
<span class=kr>const</span> <span class=nx>INDEX</span> <span class=o>=</span> <span class=s1>&#39;assets&#39;</span><span class=p>;</span>
<span class=c1>// Global variable to count the number of records that have been inserted
</span><span class=c1></span><span class=kd>let</span> <span class=nx>numInserted</span> <span class=o>=</span> <span class=mi>0</span><span class=p>;</span>

<span class=c1>// Function to generate a random document
</span><span class=c1></span><span class=kd>function</span> <span class=nx>getRandomDocument</span> <span class=p>()</span> <span class=p>{</span>
  <span class=k>return</span> <span class=p>{</span>
    <span class=nx>name</span><span class=o>:</span> <span class=nx>faker</span><span class=p>.</span><span class=nx>name</span><span class=p>.</span><span class=nx>findName</span><span class=p>(),</span>
    <span class=nx>id</span><span class=o>:</span> <span class=nx>faker</span><span class=p>.</span><span class=nx>datatype</span><span class=p>.</span><span class=nx>uuid</span><span class=p>(),</span>
  <span class=p>};</span>
<span class=p>}</span>

<span class=c1>// Generator that yields new random documents
</span><span class=c1></span><span class=kr>async</span> <span class=kd>function</span> <span class=o>*</span> <span class=nx>docGenerator</span> <span class=p>()</span> <span class=p>{</span>
  <span class=k>for</span> <span class=p>(</span><span class=kd>let</span> <span class=nx>i</span> <span class=o>=</span> <span class=mi>0</span><span class=p>;</span> <span class=nx>i</span> <span class=o>&lt;</span> <span class=nx>NUM_DOCUMENTS</span><span class=p>;</span> <span class=nx>i</span><span class=o>++</span><span class=p>)</span> <span class=p>{</span>
    <span class=k>yield</span> <span class=nx>getRandomDocument</span><span class=p>();</span>
  <span class=p>}</span>
<span class=p>}</span>

<span class=p>(</span><span class=kr>async</span> <span class=kd>function</span> <span class=nx>run</span> <span class=p>()</span> <span class=p>{</span>
  <span class=kr>const</span> <span class=nx>client</span> <span class=o>=</span> <span class=k>new</span> <span class=nx>Client</span><span class=p>({</span> <span class=nx>node</span><span class=o>:</span> <span class=nx>ES_HOST</span> <span class=p>});</span>
  <span class=kr>const</span> <span class=nx>result</span> <span class=o>=</span> <span class=kr>await</span> <span class=nx>client</span><span class=p>.</span><span class=nx>helpers</span><span class=p>.</span><span class=nx>bulk</span><span class=p>({</span>
    <span class=c1>// The bulk API accepts a generator as input.
</span><span class=c1></span>    <span class=nx>datasource</span><span class=o>:</span> <span class=nx>docGenerator</span><span class=p>(),</span>
    <span class=nx>onDocument</span> <span class=p>(</span><span class=nx>doc</span><span class=p>)</span> <span class=p>{</span>
      <span class=nx>numInserted</span><span class=o>++</span><span class=p>;</span>
      <span class=nx>process</span><span class=p>.</span><span class=nx>stdout</span><span class=p>.</span><span class=nx>clearLine</span><span class=p>();</span>
      <span class=nx>process</span><span class=p>.</span><span class=nx>stdout</span><span class=p>.</span><span class=nx>cursorTo</span><span class=p>(</span><span class=mi>0</span><span class=p>);</span>
      <span class=nx>process</span><span class=p>.</span><span class=nx>stdout</span><span class=p>.</span><span class=nx>write</span><span class=p>(</span><span class=sb>`</span><span class=si>${</span> <span class=nx>numInserted</span> <span class=si>}</span><span class=sb> / </span><span class=si>${</span> <span class=nx>NUM_DOCUMENTS</span> <span class=si>}</span><span class=sb> documents inserted`</span><span class=p>);</span>
      <span class=k>return</span> <span class=p>{</span>
        <span class=c1>// Creates a new document in the index
</span><span class=c1></span>        <span class=nx>create</span><span class=o>:</span> <span class=p>{</span> <span class=nx>_index</span><span class=o>:</span> <span class=nx>INDEX</span> <span class=p>},</span>
      <span class=p>};</span>
    <span class=p>},</span>
  <span class=p>})</span>
<span class=p>})();</span>
</code></pre></td></tr></table>
</div>
</div><p>Running the script again only takes a few seconds. (Before, it could take a few minutes, depending on how many documents were being inserted.)</p>
<h2 id=evaluating-response-time>Evaluating Response Time<a hidden class=anchor aria-hidden=true href=#evaluating-response-time>#</a></h2>
<p>With 25k documents, queries are speedy at around 10ms. I noticed a problem though when trying to query for the last page:</p>
<div class=highlight><div class=chroma>
<table class=lntable><tr><td class=lntd>
<pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span></code></pre></td>
<td class=lntd>
<pre tabindex=0 class=chroma><code class=language-json data-lang=json><span class=p>{</span>
  <span class=nt>&#34;error&#34;</span><span class=p>:</span> <span class=p>{</span>
    <span class=nt>&#34;root_cause&#34;</span><span class=p>:</span> <span class=p>[</span>
      <span class=p>{</span>
        <span class=nt>&#34;type&#34;</span><span class=p>:</span> <span class=s2>&#34;illegal_argument_exception&#34;</span><span class=p>,</span>
        <span class=nt>&#34;reason&#34;</span><span class=p>:</span> <span class=s2>&#34;Result window is too large, from + size must be less than or equal to: [10000] but was [20010]. See the scroll api for a more efficient way to request large data sets. This limit can be set by changing the [index.max_result_window] index level setting.&#34;</span>
      <span class=p>}</span>
    <span class=p>],</span>
  <span class=err>...</span>
<span class=p>}</span>
</code></pre></td></tr></table>
</div>
</div><p>This is a nice descriptive error message. The default maximum documents allowed for from/size pagination is 10k and an error was thrown when I tried to start the query at 20k. I could of course, raise the max_result_window setting to something higher, but this is inefficient. The error message recommends using the scroll api, documented <a href=https://www.elastic.co/guide/en/elasticsearch/reference/current/paginate-search-results.html#scroll-search-results>here</a>. However&mldr;</p>
<blockquote>
<p>We no longer recommend using the scroll API for deep pagination. If you need to preserve the index state while paging through more than 10,000 hits, use the <code>search_after</code> parameter with a point in time (PIT).</p>
</blockquote>
<p>So what is <a href=https://www.elastic.co/guide/en/elasticsearch/reference/current/paginate-search-results.html#search-after><code>search_after</code> with PIT</a>? It&rsquo;s actually pretty simple.</p>
<p>Before making calls that require deep pagination, first request a Point in Time ID from the index.</p>
<div class=highlight><div class=chroma>
<table class=lntable><tr><td class=lntd>
<pre tabindex=0 class=chroma><code><span class=lnt>1
</span></code></pre></td>
<td class=lntd>
<pre tabindex=0 class=chroma><code class=language-bash data-lang=bash>curl -X POST <span class=s1>&#39;localhost:9200/assets/_pit?keep_alive=1m&#39;</span>
</code></pre></td></tr></table>
</div>
</div><p>This will return a json response that looks like this:</p>
<div class=highlight><div class=chroma>
<table class=lntable><tr><td class=lntd>
<pre tabindex=0 class=chroma><code><span class=lnt>1
</span></code></pre></td>
<td class=lntd>
<pre tabindex=0 class=chroma><code class=language-json data-lang=json><span class=p>{</span><span class=nt>&#34;id&#34;</span><span class=p>:</span><span class=s2>&#34;z4S1AwEGYXNzZXRzFlRLaVNVSFpyVHMtNS1hZ1ZKS1pXNWcAFnREZjlWcFBxVFN5UkJuTk1XQ0tPOVEAAAAAAAAAAJ4WSU5tZ21Xc2NTYy03OHVwUUg4Z2pBQQABFlRLaVNVSFpyVHMtNS1hZ1ZKS1pXNWcAAA==&#34;</span><span class=p>}</span>
</code></pre></td></tr></table>
</div>
</div><p>The PIT <code>id</code> above can be used in a search query like this:</p>
<div class=highlight><div class=chroma>
<table class=lntable><tr><td class=lntd>
<pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span></code></pre></td>
<td class=lntd>
<pre tabindex=0 class=chroma><code class=language-bash data-lang=bash>curl -X GET <span class=s1>&#39;localhost:9200/_search&#39;</span> <span class=se>\
</span><span class=se></span>  -H <span class=s1>&#39;Content-Type: application/json&#39;</span> <span class=se>\
</span><span class=se></span>  -d <span class=s1>&#39;{
</span><span class=s1>    &#34;query&#34;: {
</span><span class=s1>      &#34;match&#34;: {
</span><span class=s1>        &#34;name&#34;: &#34;Test Data&#34;
</span><span class=s1>      }
</span><span class=s1>    },
</span><span class=s1>    &#34;sort&#34;: [{ &#34;name.keyword&#34;: &#34;desc&#34; }],
</span><span class=s1>    &#34;size&#34;: 10,
</span><span class=s1>    &#34;pit&#34;: {
</span><span class=s1>      &#34;id&#34;:&#34;z4S1AwEGYXNzZXRzFlRLaVNVSFpyVHMtNS1hZ1ZKS1pXNWcAFnREZjlWcFBxVFN5UkJuTk1XQ0tPOVEAAAAAAAAAAJ4WSU5tZ21Xc2NTYy03OHVwUUg4Z2pBQQABFlRLaVNVSFpyVHMtNS1hZ1ZKS1pXNWcAAA==&#34;,
</span><span class=s1>      &#34;keep_alive&#34;: &#34;1m&#34;
</span><span class=s1>    }
</span><span class=s1>  }&#39;</span>
</code></pre></td></tr></table>
</div>
</div><p>In response, I will receive documents 1-10 along with a <code>sort</code> field.</p>
<div class=highlight><div class=chroma>
<table class=lntable><tr><td class=lntd>
<pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span><span class=lnt>5
</span><span class=lnt>6
</span><span class=lnt>7
</span></code></pre></td>
<td class=lntd>
<pre tabindex=0 class=chroma><code class=language-json data-lang=json><span class=p>{</span>
  <span class=err>...</span>
  <span class=nt>&#34;sort&#34;</span><span class=p>:</span> <span class=p>[</span>
    <span class=s2>&#34;fffbfc0&#34;</span><span class=p>,</span>
    <span class=mi>25358</span>
  <span class=p>]</span>
<span class=p>}</span>
</code></pre></td></tr></table>
</div>
</div><p>To query for documents 11-20, I&rsquo;ll set the <code>search_after</code> parameter in my next query to the <code>sort</code> value, like this:</p>
<div class=highlight><div class=chroma>
<table class=lntable><tr><td class=lntd>
<pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span><span class=lnt>17
</span><span class=lnt>18
</span><span class=lnt>19
</span></code></pre></td>
<td class=lntd>
<pre tabindex=0 class=chroma><code class=language-bash data-lang=bash>curl -X GET <span class=s1>&#39;localhost:9200/_search&#39;</span> <span class=se>\
</span><span class=se></span>  -H <span class=s1>&#39;Content-Type: application/json&#39;</span> <span class=se>\
</span><span class=se></span>  -d <span class=s1>&#39;{
</span><span class=s1>    &#34;query&#34;: {
</span><span class=s1>      &#34;match&#34;: {
</span><span class=s1>        &#34;name&#34;: &#34;Test Data&#34;
</span><span class=s1>      }
</span><span class=s1>    },
</span><span class=s1>    &#34;sort&#34;: [{ &#34;name.keyword&#34;: &#34;desc&#34; }],
</span><span class=s1>    &#34;size&#34;: 10,
</span><span class=s1>    &#34;pit&#34;: {
</span><span class=s1>      &#34;id&#34;:&#34;z4S1AwEGYXNzZXRzFlRLaVNVSFpyVHMtNS1hZ1ZKS1pXNWcAFnREZjlWcFBxVFN5UkJuTk1XQ0tPOVEAAAAAAAAAAJ4WSU5tZ21Xc2NTYy03OHVwUUg4Z2pBQQABFlRLaVNVSFpyVHMtNS1hZ1ZKS1pXNWcAAA==&#34;,
</span><span class=s1>      &#34;keep_alive&#34;: &#34;1m&#34;
</span><span class=s1>    },
</span><span class=s1>    &#34;search_after&#34;: [
</span><span class=s1>      &#34;fffbfc0&#34;,
</span><span class=s1>      25358
</span><span class=s1>    ]
</span><span class=s1>  }&#39;</span>
</code></pre></td></tr></table>
</div>
</div><p>A downside to this approach is that querying the last page is not as simple as with the <code>from</code> parameter. An arbitrary page cannot be selected. Instead, a starting page must first be retreived. This will work fine for pagination with simple &ldquo;Previous&rdquo; and &ldquo;Next&rdquo; buttons. It does not work, however, for pagination that allows the user to select a specific page.</p>
<p>As a compromise, the UI could offer Previous/Next pagination until total query results are reduced below 10k, then offer the option to select a specific page. This could be suitable for cases when the need for deep pagination is present but uncommon.</p>
<h2 id=finishing-up-for-the-day>Finishing Up for the Day<a hidden class=anchor aria-hidden=true href=#finishing-up-for-the-day>#</a></h2>
<p>There is still more investigation needed. Analyzing performance on 25k documents is a good start, but I will need to increase that number quite a bit to get a realistic estimate of improvement over the current system. Also, I&rsquo;ve barely scratched the surface of Elasticsearch&rsquo;s capabilities. The research continues&mldr;</p>
</div>
<footer class=post-footer>
<ul class=post-tags>
<li><a href=https://www.taylordeckard.me/blog/tags/programming/>programming</a></li>
<li><a href=https://www.taylordeckard.me/blog/tags/database/>database</a></li>
<li><a href=https://www.taylordeckard.me/blog/tags/node.js/>node.js</a></li>
</ul>
<nav class=paginav>
<a class=prev href=https://www.taylordeckard.me/blog/posts/vim-1/>
<span class=title>« Prev Page</span>
<br>
<span>Vim Intro</span>
</a>
<a class=next href=https://www.taylordeckard.me/blog/posts/hugo-tutorial/>
<span class=title>Next Page »</span>
<br>
<span>Meta Post (Create a Blog with Hugo)</span>
</a>
</nav>
</footer>
</article>
</main>
<a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a>
<script>let menu=document.getElementById('menu');menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(a=>{a.addEventListener("click",function(b){b.preventDefault();var a=this.getAttribute("href").substr(1);window.matchMedia('(prefers-reduced-motion: reduce)').matches?document.querySelector(`[id='${decodeURIComponent(a)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(a)}']`).scrollIntoView({behavior:"smooth"}),a==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${a}`)})})</script>
<script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script>
<script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove('dark'),localStorage.setItem("pref-theme",'light')):(document.body.classList.add('dark'),localStorage.setItem("pref-theme",'dark'))})</script>
<script>document.querySelectorAll('pre > code').forEach(b=>{const c=b.parentNode.parentNode,a=document.createElement('button');a.classList.add('copy-code'),a.innerText='copy';function d(){a.innerText='copied!',setTimeout(()=>{a.innerText='copy'},2e3)}a.addEventListener('click',e=>{if('clipboard'in navigator){navigator.clipboard.writeText(b.textContent),d();return}const a=document.createRange();a.selectNodeContents(b);const c=window.getSelection();c.removeAllRanges(),c.addRange(a);try{document.execCommand('copy'),d()}catch(a){}c.removeRange(a)}),c.classList.contains("highlight")?c.appendChild(a):c.parentNode.firstChild==c||(b.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?b.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(a):b.parentNode.appendChild(a))})</script>
</body>
</html>