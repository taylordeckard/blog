[{"content":"I\u0026rsquo;ve started working on a proof-of-concept to improve query performance of a large dataset (5M+ rows.) The data is currently stored in a MySQL database.\nThe service is required to search, sort, filter, and paginate the data. Nowadays, these requirements are standard practice. However, with such a large dataset, some of the database queries are taking \u0026gt; 3 seconds, even with table partitioning.\nMy theory is that Elasticsearch will perform better than RDBMS for this use case. Only one way to find out\u0026hellip;\nRunning Elasticsearch Locally I already have Docker and docker-compose installed, so I just need to create a docker-compose config.\nversion: \u0026#34;3.9\u0026#34; # Run 3 Elasticsearch containers simultaneously (es01, es02, es03) services: es01: image: docker.elastic.co/elasticsearch/elasticsearch:7.16.3 container_name: es01 environment: - node.name=es01 - cluster.name=es-docker-cluster - discovery.seed_hosts=es02,es03 - cluster.initial_master_nodes=es01,es02,es03 - bootstrap.memory_lock=true - \u0026#34;ES_JAVA_OPTS=-Xms512m -Xmx512m\u0026#34; # Allow unlimited memory to be locked by this container process ulimits: memlock: soft: -1 hard: -1 volumes: # Map data01 directory to container (for persistence) - data01:/usr/share/elasticsearch/data ports: # Map local port 9200 to container port 9200 - 9200:9200 networks: - elastic es02: image: docker.elastic.co/elasticsearch/elasticsearch:7.16.3 container_name: es02 environment: - node.name=es02 - cluster.name=es-docker-cluster - discovery.seed_hosts=es01,es03 - cluster.initial_master_nodes=es01,es02,es03 - bootstrap.memory_lock=true - \u0026#34;ES_JAVA_OPTS=-Xms512m -Xmx512m\u0026#34; ulimits: memlock: soft: -1 hard: -1 volumes: - data02:/usr/share/elasticsearch/data networks: - elastic es03: image: docker.elastic.co/elasticsearch/elasticsearch:7.16.3 container_name: es03 environment: - node.name=es03 - cluster.name=es-docker-cluster - discovery.seed_hosts=es01,es02 - cluster.initial_master_nodes=es01,es02,es03 - bootstrap.memory_lock=true - \u0026#34;ES_JAVA_OPTS=-Xms512m -Xmx512m\u0026#34; ulimits: memlock: soft: -1 hard: -1 volumes: - data03:/usr/share/elasticsearch/data networks: - elastic volumes: data01: driver: local data02: driver: local data03: driver: local # ES Nodes run on a shared network that is bridged to the local machine networks: elastic: driver: bridge Elasticsearch should be ready to go. Before running though, I\u0026rsquo;ll need to bump up Docker\u0026rsquo;s memory resources to \u0026gt; 4GB. Start docker-compose with\ndocker-compose up Creating an Index Now that Elasticsearch is running, it\u0026rsquo;s time to learn how to use it: REST API docs.\nFirst, I want to create an index (table in SQL) for my dataset so I can add documents (rows) to it.\nI create an index called assets (API Docs):\ncurl -X PUT \u0026#39;http://localhost:9200/assets\u0026#39; The following is shown in the docker-compose logs:\n{\u0026#34;type\u0026#34;: \u0026#34;server\u0026#34;, \u0026#34;timestamp\u0026#34;: \u0026#34;2022-01-21T15:50:11,935Z\u0026#34;, \u0026#34;level\u0026#34;: \u0026#34;INFO\u0026#34;, \u0026#34;component\u0026#34;: \u0026#34;o.e.c.m.MetadataCreateIndexService\u0026#34;, \u0026#34;cluster.name\u0026#34;: \u0026#34;es-docker-cluster\u0026#34;, \u0026#34;node.name\u0026#34;: \u0026#34;es02\u0026#34;, \u0026#34;message\u0026#34;: \u0026#34;[assets] creating index, cause [api], templates [], shards [1]/[1]\u0026#34;, \u0026#34;cluster.uuid\u0026#34;: \u0026#34;6BFPOn84Q8qFIfS2FSmsBw\u0026#34;, \u0026#34;node.id\u0026#34;: \u0026#34;Sm9PsrFzTu6tkK0tAt0qvw\u0026#34; } which indicates the index creation was successful.\nAnother way to check that the index creation was successful is to do\ncurl --head \u0026#39;http://localhost:9200/assets\u0026#39; A 200 response means that the index exists, whereas 404 means it doesn\u0026rsquo;t.\nWorking with Documents Adding a document to the index is simple (API Docs):\ncurl -X POST \u0026#39;http://localhost:9200/assets/_doc/\u0026#39; \\  -H \u0026#39;Content-Type: application/json\u0026#39; \\  -d \u0026#39;{ \u0026#34;name\u0026#34;: \u0026#34;Test Data\u0026#34; }\u0026#39; Then retrieve the document with (API Docs)\ncurl -X GET \u0026#39;localhost:9200/assets/_search?pretty\u0026#39; \\  -H \u0026#39;Content-Type: application/json\u0026#39; \\  -d \u0026#39;{ \u0026#34;query\u0026#34;: { \u0026#34;match\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;Test Data\u0026#34; } } }\u0026#39; Mocking Data The next step is to populate the local Elasticsearch database up with mock documents to mimic a production environment. I\u0026rsquo;m familiar with Node.js, so I\u0026rsquo;ll write a quick script to do this.\nCreate a directory to contain the script.\nmkdir mock-data \u0026amp;\u0026amp; cd mock-data Add a package.json\nnpm init -y Now, I\u0026rsquo;ll add a couple of dependencies. To create mock-data I\u0026rsquo;m using a fork of faker.js, (community-faker.) I\u0026rsquo;ll also use node-fetch to make HTTP requests.\n# shorthand for npm install, -S writes the dependencies to package.json npm i -S community-faker node-fetch Next, write the script: index.js. (Note: I\u0026rsquo;m using Node.js v14.16.1)\nimport faker from \u0026#39;community-faker\u0026#39;; import fetch from \u0026#39;node-fetch\u0026#39;; // Number of documents to insert const NUM_DOCUMENTS = 25000; const ES_HOST = \u0026#39;http://localhost:9200\u0026#39;; // The ES index to insert records into const INDEX = \u0026#39;assets\u0026#39;; (async function run () { for (let i = 0; i \u0026lt; NUM_DOCUMENTS; i++) { const doc = { name: faker.name.findName(), id: faker.datatype.uuid(), }; // Send document to ES via http API  const response = await fetch(`${ES_HOST}/${INDEX}/_doc`, { // node-fetch requires stringifying the json body  body: JSON.stringify(doc), headers: { // Content-Type header is required for ES APIs with bodies  \u0026#39;Content-Type\u0026#39;: \u0026#39;application/json\u0026#39;, }, method: \u0026#39;post\u0026#39;, }) if (!response.ok) { // If an error occurred, log the error and exit  console.log(await response.json()); break; } // Write out progress on a single line  process.stdout.clearLine(); process.stdout.cursorTo(0); process.stdout.write(`${i + 1}/ ${ NUM_DOCUMENTS }documents inserted`); } })(); Finally, run the script.\nnode index.js When finished, confirm the data has been ingested.\ncurl -X GET \u0026#39;localhost:9200/assets/_search?pretty\u0026#39; \\  -H \u0026#39;Content-Type: application/json\u0026#39; \\  -d \u0026#39;{ \u0026#34;query\u0026#34;: { \u0026#34;match_all\u0026#34;: {} }, \u0026#34;size\u0026#34;: 10, \u0026#34;from\u0026#34;: 0 }\u0026#39; This works fine, but takes a while. There is a more optimal solution for bulk insert: make use of the Elasticsearch bulk helper. I can use the javascript Elasticsearch client @elastic/elasticsearch.\nnpm i -S @elastic/elasticsearch I\u0026rsquo;ll also get rid of node-fetch since I\u0026rsquo;ll be using the ES Client to make requests to Elasticsearch.\nimport faker from \u0026#39;community-faker\u0026#39;; import { Client } from \u0026#39;@elastic/elasticsearch\u0026#39;; const NUM_DOCUMENTS = 10000; const HOST = \u0026#39;http://localhost:9200\u0026#39;; const INDEX = \u0026#39;assets\u0026#39;; // Global variable to count the number of records that have been inserted let numInserted = 0; // Function to generate a random document function getRandomDocument () { return { name: faker.name.findName(), id: faker.datatype.uuid(), }; } // Generator that yields new random documents async function * docGenerator () { for (let i = 0; i \u0026lt; NUM_DOCUMENTS; i++) { yield getRandomDocument(); } } (async function run () { const client = new Client({ node: ES_HOST }); const result = await client.helpers.bulk({ // The bulk API accepts a generator as input.  datasource: docGenerator(), onDocument (doc) { numInserted++; process.stdout.clearLine(); process.stdout.cursorTo(0); process.stdout.write(`${ numInserted }/ ${ NUM_DOCUMENTS }documents inserted`); return { // Creates a new document in the index  create: { _index: INDEX }, }; }, }) })(); Running the script again only takes a few seconds. (Before, it could take a few minutes, depending on how many documents were being inserted.)\nEvaluating Response Time With 25k documents, queries are speedy at around 10ms. I noticed a problem though when trying to query for the last page:\n{ \u0026#34;error\u0026#34;: { \u0026#34;root_cause\u0026#34;: [ { \u0026#34;type\u0026#34;: \u0026#34;illegal_argument_exception\u0026#34;, \u0026#34;reason\u0026#34;: \u0026#34;Result window is too large, from + size must be less than or equal to: [10000] but was [20010]. See the scroll api for a more efficient way to request large data sets. This limit can be set by changing the [index.max_result_window] index level setting.\u0026#34; } ], ... } This is a nice descriptive error message. The default maximum documents allowed for from/size pagination is 10k and an error was thrown when I tried to start the query at 20k. I could of course, raise the max_result_window setting to something higher, but this is inefficient. The error message recommends using the scroll api, documented here. However\u0026hellip;\n We no longer recommend using the scroll API for deep pagination. If you need to preserve the index state while paging through more than 10,000 hits, use the search_after parameter with a point in time (PIT).\n So what is search_after with PIT? It\u0026rsquo;s actually pretty simple.\nBefore making calls that require deep pagination, first request a Point in Time ID from the index.\ncurl -X POST \u0026#39;localhost:9200/assets/_pit?keep_alive=1m\u0026#39; This will return a json response that looks like this:\n{\u0026#34;id\u0026#34;:\u0026#34;z4S1AwEGYXNzZXRzFlRLaVNVSFpyVHMtNS1hZ1ZKS1pXNWcAFnREZjlWcFBxVFN5UkJuTk1XQ0tPOVEAAAAAAAAAAJ4WSU5tZ21Xc2NTYy03OHVwUUg4Z2pBQQABFlRLaVNVSFpyVHMtNS1hZ1ZKS1pXNWcAAA==\u0026#34;} The PIT id above can be used in a search query like this:\ncurl -X GET \u0026#39;localhost:9200/_search\u0026#39; \\  -H \u0026#39;Content-Type: application/json\u0026#39; \\  -d \u0026#39;{ \u0026#34;query\u0026#34;: { \u0026#34;match\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;Test Data\u0026#34; } }, \u0026#34;sort\u0026#34;: [{ \u0026#34;name.keyword\u0026#34;: \u0026#34;desc\u0026#34; }], \u0026#34;size\u0026#34;: 10, \u0026#34;pit\u0026#34;: { \u0026#34;id\u0026#34;:\u0026#34;z4S1AwEGYXNzZXRzFlRLaVNVSFpyVHMtNS1hZ1ZKS1pXNWcAFnREZjlWcFBxVFN5UkJuTk1XQ0tPOVEAAAAAAAAAAJ4WSU5tZ21Xc2NTYy03OHVwUUg4Z2pBQQABFlRLaVNVSFpyVHMtNS1hZ1ZKS1pXNWcAAA==\u0026#34;, \u0026#34;keep_alive\u0026#34;: \u0026#34;1m\u0026#34; } }\u0026#39; In response, I will receive documents 1-10 along with a sort field.\n{ ... \u0026#34;sort\u0026#34;: [ \u0026#34;fffbfc0\u0026#34;, 25358 ] } To query for documents 11-20, I\u0026rsquo;ll set the search_after parameter in my next query to the sort value, like this:\ncurl -X GET \u0026#39;localhost:9200/_search\u0026#39; \\  -H \u0026#39;Content-Type: application/json\u0026#39; \\  -d \u0026#39;{ \u0026#34;query\u0026#34;: { \u0026#34;match\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;Test Data\u0026#34; } }, \u0026#34;sort\u0026#34;: [{ \u0026#34;name.keyword\u0026#34;: \u0026#34;desc\u0026#34; }], \u0026#34;size\u0026#34;: 10, \u0026#34;pit\u0026#34;: { \u0026#34;id\u0026#34;:\u0026#34;z4S1AwEGYXNzZXRzFlRLaVNVSFpyVHMtNS1hZ1ZKS1pXNWcAFnREZjlWcFBxVFN5UkJuTk1XQ0tPOVEAAAAAAAAAAJ4WSU5tZ21Xc2NTYy03OHVwUUg4Z2pBQQABFlRLaVNVSFpyVHMtNS1hZ1ZKS1pXNWcAAA==\u0026#34;, \u0026#34;keep_alive\u0026#34;: \u0026#34;1m\u0026#34; }, \u0026#34;search_after\u0026#34;: [ \u0026#34;fffbfc0\u0026#34;, 25358 ] }\u0026#39; A downside to this approach is that querying the last page is not as simple as with the from parameter. An arbitrary page cannot be selected. Instead, a starting page must first be retreived. This will work fine for pagination with simple \u0026ldquo;Previous\u0026rdquo; and \u0026ldquo;Next\u0026rdquo; buttons. It does not work, however, for pagination that allows the user to select a specific page.\nAs a compromise, the UI could offer Previous/Next pagination until total query results are reduced below 10k, then offer the option to select a specific page. This could be suitable for cases when the need for deep pagination is present but uncommon.\nFinishing Up for the Day There is still more investigation needed. Analyzing performance on 25k documents is a good start, but I will need to increase that number quite a bit to get a realistic estimate of improvement over the current system. Also, I\u0026rsquo;ve barely scratched the surface of Elasticsearch\u0026rsquo;s capabilities. The research continues\u0026hellip;\n","permalink":"https://www.taylordeckard.me/blog/posts/elasticsearch-intro/","summary":"I\u0026rsquo;ve started working on a proof-of-concept to improve query performance of a large dataset (5M+ rows.) The data is currently stored in a MySQL database.\nThe service is required to search, sort, filter, and paginate the data. Nowadays, these requirements are standard practice. However, with such a large dataset, some of the database queries are taking \u0026gt; 3 seconds, even with table partitioning.\nMy theory is that Elasticsearch will perform better than RDBMS for this use case.","title":"Elasticsearch Intro"},{"content":"As an introductory post, I\u0026rsquo;ll detail the steps to create a blog like this. It should only take a few minutes\u0026hellip;\nInstall Hugo Follow the steps here to install Hugo on your system. For me, on macOS, the command was:\nbrew install hugo Create a Site Next, you\u0026rsquo;ll want to use the hugo binary to create the project scaffolding for your site. Open up a terminal and enter the following command (substitute my_blog with the name of the directory you want to create the site in):\nhugo new site my_blog -f yml Then cd into the directory:\ncd my_blog At this point, you\u0026rsquo;ll want to pick a theme. I chose PaperMod. Instructions for adding a theme to your project can usually be found on the theme\u0026rsquo;s GitHub page. In my case, the install consisted of:\ngit clone https://github.com/adityatelange/hugo-PaperMod themes/PaperMod --depth=1 Also as part of the theme install, a line was added to config.yml:\ntheme: \u0026#34;PaperMod\u0026#34; At this point, your project should look something like this:\n. ├── archetypes │ └── default.md ├── config.yml ├── content ├── data ├── layouts ├── resources │ └── _gen ├── static └── themes └── PaperMod Configuring the Theme Usually detailed on the theme\u0026rsquo;s GitHub page, there are a variety of configuration options available. These can be set in config.yml. I copied the PaperMod example configuration and made a few adjustments.\nbaseURL: https://www.taylordeckard.me/ languageCode: en-us title: Taylor\u0026#39;s Blog theme: \u0026#34;PaperMod\u0026#34; baseURL: https://www.taylordeckard.me/blog enableRobotsTXT: true buildDrafts: false buildFuture: false buildExpired: false minify: disableXML: true minifyOutput: true params: env: production title: Taylor\u0026#39;s Blog description: \u0026#34;Things that happen to me\u0026#34; keywords: [Blog] author: taylordeckard DateFormat: \u0026#34;January 2, 2006\u0026#34; defaultTheme: auto # dark, light disableThemeToggle: false ShowReadingTime: true ShowShareButtons: false ShowPostNavLinks: true ShowBreadCrumbs: true ShowCodeCopyButtons: true disableSpecial1stPost: false disableScrollToTop: false comments: false hidemeta: false hideSummary: false hideFooter: true showtoc: false tocopen: false label: text: \u0026#34;Home\u0026#34; # home-info mode homeInfoParams: Title: \u0026#34;Hi\u0026#34; Content: Welcome to my blog socialIcons: - name: github url: \u0026#34;https://github.com/taylordeckard\u0026#34; cover: hidden: true # hide everywhere but not in structured data hiddenInList: true # hide on list pages and home hiddenInSingle: true # hide on single page editPost: URL: \u0026#34;https://github.com/taylordeckard/blog/content\u0026#34; Text: \u0026#34;Suggest Changes\u0026#34; # edit text appendFilePath: true # to append file path to Edit link # for search # https://fusejs.io/api/options.html fuseOpts: isCaseSensitive: false shouldSort: true location: 0 distance: 1000 threshold: 0.4 minMatchCharLength: 0 keys: [\u0026#34;title\u0026#34;, \u0026#34;permalink\u0026#34;, \u0026#34;summary\u0026#34;, \u0026#34;content\u0026#34;] menu: main: - identifier: archives name: Archives url: /archives weight: 30 - identifier: search name: Search url: /search weight: 30 - identifier: taylordeckard name: taylordeckard.me url: https://taylordeckard.me weight: 30 outputs: home: - HTML - RSS - JSON Creating Content All of the markdown files you will create for your blog will go in the top-level /content directory. Right now it should be empty. There are a couple of files that are specific to the PaperMod theme and need to be created:\ncontent/archives.md\n--- title: \u0026#34;Archive\u0026#34; layout: \u0026#34;archives\u0026#34; summary: \u0026#34;archives\u0026#34; --- content/search.md\n--- title: \u0026#34;Search\u0026#34; layout: \u0026#34;search\u0026#34; --- Next, create a directory for all of your posts: content/posts. In this directory, you will have a markdown file for each post you make.\nFor my blog, for instance, I created hugo-tutorial.md:\ncontent ├── archives.md ├── posts │ └── hugo-tutorial.md └── search.md hugo-tutorial.md looks something like this:\n--- author: Taylor Deckard title: Meta date: 2021-01-19 description: Setting up a blog with Hugo --- As an introductory post, I\u0026#39;ll detail the steps to create a blog like this. It should only take a few minutes... ![It should only take a few minutes...](/blog/images/hugo-tutorial/dumb_and_dumber_watch.webp) ## Install Hugo ... Running Locally Once you have something similar, you should be able to run the blog locally. From the project root directory, run the following:\nhugo serve Then open a browser and navigate to [http://localhost:1313/blog]. (The /blog comes from the baseURL property set in the config.yml)\nNext steps This is a good start to a blog, but I still need to create a GitHub repo for it and configure a pipeline to deploy the blog automatically to my personal website. If you\u0026rsquo;d rather, you could use GitHub Pages to host your blog for free.\n","permalink":"https://www.taylordeckard.me/blog/posts/hugo-tutorial/","summary":"As an introductory post, I\u0026rsquo;ll detail the steps to create a blog like this. It should only take a few minutes\u0026hellip;\nInstall Hugo Follow the steps here to install Hugo on your system. For me, on macOS, the command was:\nbrew install hugo Create a Site Next, you\u0026rsquo;ll want to use the hugo binary to create the project scaffolding for your site. Open up a terminal and enter the following command (substitute my_blog with the name of the directory you want to create the site in):","title":"Meta Post (Create a Blog with Hugo)"}]