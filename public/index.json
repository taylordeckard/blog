[{"content":"After my last post on Creating a Networked Client to Power Centralized Insights, I started to think about how I could use a similar design to create a decentralized website. Users could run their clients locally, store information on their own machines, and connect to other clients from the central hub. An interesting design choice, I thought, would be for the website to only be accessible from the local network when the client is running.This would ensure that to be a participant in the network, you are also contributing to the network.\nThe Design sequenceDiagram actor User participant Client A participant Hub participant Client B Client B-\u003e\u003eHub: Connects via gRPC stream User-\u003e\u003eClient A: Runs client program Client A-\u003e\u003eHub: Connects via gRPC stream User-\u003e\u003eClient A: Requests website (http://localhost:8080) Client A-\u003e\u003eHub: Calls gRPC to retrieve website data Hub-\u003e\u003eClient A: Responds with website data Client A-\u003e\u003eUser: Serves website User-\u003e\u003eClient A: Requests info from Client B Client A-\u003e\u003eHub: Sends gRPC request Hub-\u003e\u003eClient B: Notifies via gRPC stream Client B-\u003e\u003eHub: Responds with data Hub-\u003e\u003eClient A: Responds with data Client A-\u003e\u003eUser: Serves data Now this may look a bit complicated so let\u0026rsquo;s break it down. The user runs a client program on their machine. This client connects to the hub and serves a website on http://localhost:8080. The user can then request data from other clients on the network. The client program will make a gRPC call to the hub, which will then notify the other client to send the data. The hub will then send the data back to the client program, which will serve it to the user.\nOk, so how do I implement this?\ngRPC and Rust First, let\u0026rsquo;s create a new Rust project. I am calling it irly because I am not very creative with names. (Side note: I typed in \u0026ldquo;I am calling it irly because \u0026quot; and copilot suggested the rest. ðŸ˜€)\ncargo new irly Before writing any code in Rust though, I need to make a proto definition for gRPC.\nsyntax = \u0026#34;proto3\u0026#34;; package irly.v1; service Irly { rpc GetFile (GetFileRequest) returns (GetFileResponse); } message GetFileRequest { string file_path = 1; } message GetFileResponse { string file_path = 1; string file_content = 2; } This proto file defines a service called Irly with a single method GetFile. This method takes a GetFileRequest and returns a GetFileResponse. The request contains a file_path and the response contains the file_path and the file_content. The idea is to imitate a file server in gRPC.\nTonic To set up a gRPC server in Rust, I used tonic. To install the dependencies, I first installed cargo-edit.\ncargo install cargo-edit This allows me to add dependencies to my Cargo.toml file with the cargo add command.\ncargo add async-trait # prost serializes and deserializes protobuf cargo add prost # tokio is needed for async cargo add tokio -F full # tonic is the gRPC library cargo add tonic # tonic-reflection is needed for reflection cargo add tonic-reflection # tonic-build is needed to compile the proto cargo add --build tonic-build Next, I have to add a step to the build so that the protobuf definition is compiled into Rust code. Fortunately, placing a file named build.rs in the root of a package will cause Cargo to compile that script and execute it just before building the package.\nuse std::error::Error; use std::{env, path::PathBuf}; fn main() -\u0026gt; Result\u0026lt;(), Box\u0026lt;dyn Error\u0026gt;\u0026gt; { // Cargo sets OUT_DIR environment variable to the output directory let out_dir = PathBuf::from(env::var(\u0026#34;OUT_DIR\u0026#34;).unwrap()); // Output the file descriptor for reflection tonic_build::configure() .file_descriptor_set_path(out_dir.join(\u0026#34;irly_descriptor.bin\u0026#34;)) .compile_protos(\u0026amp;[\u0026#34;proto/irly/v1/files.proto\u0026#34;], \u0026amp;[\u0026#34;proto\u0026#34;])?; // Compile the protobuf tonic_build::compile_protos(\u0026#34;proto/irly/v1/files.proto\u0026#34;)?; Ok(()) } Next, I will setup the server. In src/main.rs I will add the following code.\n// Create a module for the proto definition mod proto { tonic::include_proto!(\u0026#34;irly.v1\u0026#34;); // Include the file descriptor set pub(crate) const FILE_DESCRIPTOR_SET: \u0026amp;[u8] = tonic::include_file_descriptor_set!(\u0026#34;irly_descriptor\u0026#34;); } use proto::irly_server::Irly; // Implement the service #[derive(Debug, Default)] struct IrlyService {} #[tonic::async_trait] impl Irly for IrlyService { // Implement the GetFile method defined in the proto async fn get_file( \u0026amp;self, request: tonic::Request\u0026lt;proto::GetFileRequest\u0026gt;, ) -\u0026gt; Result\u0026lt;tonic::Response\u0026lt;proto::GetFileResponse\u0026gt;, tonic::Status\u0026gt; { let input = request.get_ref(); if input.file_path.is_empty() { return Err(tonic::Status::invalid_argument(\u0026#34;file_path is empty\u0026#34;)); } println!(\u0026#34;Request for file: {:?}\u0026#34;, \u0026amp;input.file_path); let response = proto::GetFileResponse { file_path: input.file_path.clone(), file_content: String::from(\u0026#34;content\u0026#34;), }; Ok(tonic::Response::new(response)) } } #[tokio::main] async fn main() -\u0026gt; Result\u0026lt;(), Box\u0026lt;dyn std::error::Error\u0026gt;\u0026gt; { let addr = \u0026#34;[::1]:50051\u0026#34;.parse()?; let irly = IrlyService::default(); // Create a reflection service // (This is so clients do not need to know the proto definition) let service = tonic_reflection::server::Builder::configure() .register_encoded_file_descriptor_set(proto::FILE_DESCRIPTOR_SET) .build_v1()?; // Create the server tonic::transport::Server::builder() .add_service(service) .add_service(proto::irly_server::IrlyServer::new(irly)) .serve(addr) .await?; Ok(()) } This code sets up a gRPC server that listens on port 50051 and implements the GetFile method. The method simply returns a string \u0026ldquo;content\u0026rdquo; for any file path requested. I can manually call this method on command-line using the grpcurl tool.\ngrpcurl -plaintext \\ -d \u0026#39;{\u0026#34;file_path\u0026#34;: \u0026#34;test.html\u0026#34;}\u0026#39; \\ \u0026#39;localhost:50051\u0026#39; irly.v1.Irly.GetFile Response:\n{ \u0026#34;file_path\u0026#34;: \u0026#34;test.html\u0026#34;, \u0026#34;file_content\u0026#34;: \u0026#34;content\u0026#34; } The Client Now, I need to create a client that can connect to the server and request a file. I will create a new file src/client.rs and add the following code. In cargo.toml, I will add the following line to include the client in the build.\n[[bin]] name = \u0026#34;hub\u0026#34; path = \u0026#34;src/hub.rs\u0026#34; [[bin]] name = \u0026#34;client\u0026#34; path = \u0026#34;src/client.rs\u0026#34; With this I can run the different programs using cargo run --bin \u0026lt;client|hub\u0026gt;.\nTo run a quick test before implementing the desired functionality, I will add the following code to src/client.rs.\nmod proto { tonic::include_proto!(\u0026#34;irly.v1\u0026#34;); } // Use the Client instead of the Server use proto::irly_client::IrlyClient; #[tokio::main] async fn main() -\u0026gt; Result\u0026lt;(), Box\u0026lt;dyn std::error::Error\u0026gt;\u0026gt; { let addr = \u0026#34;http://localhost:50051\u0026#34;; let mut client = IrlyClient::connect(addr).await?; let req = proto::GetFileRequest { file_path: \u0026#34;file.txt\u0026#34;.to_string(), }; let request = tonic::Request::new(req); let response = client.get_file(request).await?; println!(\u0026#34;Response: {:?}\u0026#34;, response.get_ref().file_content); Ok(()) } Now, to run the gRPC server:\ncargo run --bin hub Then (in another shell) to run the client:\ncargo run --bin client Result:\nResponse: \u0026#34;content\u0026#34; Serving a Website Ok, so the server is working and the client can request a file. Now I need to actually have the hub respond with file contents. Later, I might do something more interesting like serve an Angular or React app, but for now I\u0026rsquo;ll create a simple index.html file in a new directory called public.\n\u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html lang=\u0026#34;en\u0026#34;\u0026gt; \u0026lt;head\u0026gt; \u0026lt;meta charset=\u0026#34;UTF-8\u0026#34;\u0026gt; \u0026lt;meta name=\u0026#34;viewport\u0026#34; content=\u0026#34;width=device-width, initial-scale=1.0\u0026#34;\u0026gt; \u0026lt;title\u0026gt;irly\u0026lt;/title\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;h1\u0026gt;irly\u0026lt;/h1\u0026gt; \u0026lt;p\u0026gt;Welcome to irly. You have successfully joined the network!\u0026lt;/p\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; When a client requests the index.html file, the hub should respond with the contents of this file. To do this, I will read the file and return the contents in the response.\nFirst, I\u0026rsquo;ll add a function to read an file from disk. I created a new file src/file_reader.rs and added the following code.\nuse tokio::fs; use std::path::Path; const PUBLIC_PATH: \u0026amp;\u0026#39;static str = \u0026#34;./public/\u0026#34;; pub async fn read(path: \u0026amp;str) -\u0026gt; Result\u0026lt;String, Box\u0026lt;dyn std::error::Error\u0026gt;\u0026gt; { let public_path = Path::new(PUBLIC_PATH).join(path); let text = match fs::read_to_string(public_path).await { Ok(text) =\u0026gt; text, Err(e) =\u0026gt; return Err(Box::new(e)), }; Ok(text) } Then in hub.rs, I\u0026rsquo;ll add a few lines\nmod file_reader; 1#[tonic::async_trait] 2impl Irly for IrlyService { 3 async fn get_file( 4 \u0026amp;self, 5 request: tonic::Request\u0026lt;proto::GetFileRequest\u0026gt;, 6 ) -\u0026gt; Result\u0026lt;tonic::Response\u0026lt;proto::GetFileResponse\u0026gt;, tonic::Status\u0026gt; { 7 let input = request.get_ref(); 8 9 if input.file_path.is_empty() { 10 return Err(tonic::Status::invalid_argument(\u0026#34;file_path is empty\u0026#34;)); 11 } 12 13 println!(\u0026#34;Request for file: {:?}\u0026#34;, \u0026amp;input.file_path); 14 15 let file = file_reader::read(\u0026amp;input.file_path).await; 16 17 if file.is_err() { 18 return Err(tonic::Status::not_found(\u0026#34;file not found\u0026#34;)); 19 } 20 21 let mut file_path = input.file_path.clone(); 22 // Remove leading slash from path 23 if input.file_path.starts_with(\u0026#39;/\u0026#39;) { 24 file_path.remove(0); 25 } 26 // Default a request for \u0026#34;/\u0026#34; to index.html 27 if \u0026amp;input.file_path == \u0026#34;/\u0026#34; { 28 file_path = \u0026#34;index.html\u0026#34;.to_string(); 29 } 30 31 let response = proto::GetFileResponse { 32 file_path: file_path.clone(), 33 file_content: file.unwrap(), 34 }; 35 36 Ok(tonic::Response::new(response)) 37 } 38} First, I added the module definition for the file reader. Then (starting on line 15) I added a call to the file reader in the get_file method. If the file is not found, I return a not_found status. Otherwise, I return the file contents in the response.\nIf I update the client to request index.html, I should see the contents of the file in the response.\nResult:\n% cargo run --bin client Finished `dev` profile [unoptimized + debuginfo] target(s) in 0.20s Running `target/debug/client` Response: \u0026#34;\u0026lt;!DOCTYPE html\u0026gt;\\n\u0026lt;html lang=\\\u0026#34;en\\\u0026#34;\u0026gt;\\n\u0026lt;head\u0026gt;\\n \u0026lt;meta charset=\\\u0026#34;UTF-8\\\u0026#34;\u0026gt;\\n \u0026lt;meta name=\\\u0026#34;viewport\\\u0026#34; content=\\\u0026#34;width=device-width, initial-scale=1.0\\\u0026#34;\u0026gt;\\n \u0026lt;title\u0026gt;irly\u0026lt;/title\u0026gt;\\n\u0026lt;/head\u0026gt;\\n\u0026lt;body\u0026gt;\\n \u0026lt;h1\u0026gt;irly\u0026lt;/h1\u0026gt;\\n \u0026lt;p\u0026gt;Welcome to irly. You have successfully joined the network!\u0026lt;/p\u0026gt;\\n\u0026lt;/body\u0026gt;\\n\u0026lt;/html\u0026gt;\\n\u0026#34; Back to the Client Now that the hub is serving the website, I need to update the client to serve the website. I will use warp again since it worked well in the previous post.\nFirst, I will add the warp dependency to the client.\ncargo add warp Then, I will update the main function in src/client.rs to serve the website.\n1#[tokio::main] 2async fn main() -\u0026gt; Result\u0026lt;(), Box\u0026lt;dyn std::error::Error\u0026gt;\u0026gt; { 3 let proxy_route = warp::path::full().and(warp::get()).and_then(web_handler); 4 5 let routes = proxy_route.recover(handle_rejection); 6 7 warp::serve(routes).run(([127, 0, 0, 1], 8080)).await; 8 9 Ok(()) 10} Notice on line 3, I am registering a route that will call the web_handler function whenever any route is requested. The full path is passed into the web_handler function as a FullPath struct. The path can be extracted and then forwarded to the hub to request the file contents.\n1async fn web_handler(path: warp::path::FullPath) -\u0026gt; Result\u0026lt;impl warp::Reply, warp::Rejection\u0026gt; { 2 let addr = \u0026#34;http://localhost:50051\u0026#34;; 3 let mut client = IrlyClient::connect(addr).await.unwrap(); 4 let Ok(response) = request_file(\u0026amp;mut client, path.as_str()).await else { 5 return Err(warp::reject::not_found()); 6 }; 7 8 let res_ref = response.get_ref(); 9 10 Ok(warp::reply::with_header( 11 res_ref.file_content.clone(), 12 \u0026#34;content-type\u0026#34;, 13 mime_guess::from_path(\u0026amp;res_ref.file_path) 14 .first_or_octet_stream() 15 .to_string(), 16 )) 17} This creates a gRPC client and passes it, along with the path, to the request_file function. If the file is not found, a not_found rejection is returned. Otherwise, the file contents are returned with the appropriate content type (using mime_guess for this.)\nThe request_file function is defined as follows:\nasync fn request_file( client: \u0026amp;mut IrlyClient\u0026lt;tonic::transport::channel::Channel\u0026gt;, path: \u0026amp;str, ) -\u0026gt; Result\u0026lt;tonic::Response\u0026lt;proto::GetFileResponse\u0026gt;, Box\u0026lt;dyn std::error::Error\u0026gt;\u0026gt; { let req = proto::GetFileRequest { file_path: path.to_string(), }; let request = tonic::Request::new(req); let response = match client.get_file(request).await { Ok(response) =\u0026gt; response, Err(e) =\u0026gt; return Err(Box::new(e)), }; Ok(response) } This function creates a GetFileRequest and sends it to the hub. If the request is successful, the response is returned. Otherwise, an error is returned.\nDoes it work? After spinning up the hub and the client again, here are the results of navigating to http://localhost:8080 in the browser.\nSo yes, it works! The client is serving the website from the hub. This is a simple example, but it demonstrates how a decentralized website could be created using gRPC and Rust. I think this could be a bit more interesting if the hub can facilitate peer discovery and the transfer of data between clients. That might be a topic for another post. ðŸ˜„\nCheck out the code for this post on GitHub.\n","permalink":"https://www.taylordeckard.me/blog/posts/irly/","summary":"After my last post on Creating a Networked Client to Power Centralized Insights, I started to think about how I could use a similar design to create a decentralized website. Users could run their clients locally, store information on their own machines, and connect to other clients from the central hub. An interesting design choice, I thought, would be for the website to only be accessible from the local network when the client is running.","title":"Creating a Website on a Hidden Network"},{"content":"I have been wanting to try a project in rust for a few years now, and a few days ago, I finally started. My idea seemed simple. I wanted to create a \u0026ldquo;hub\u0026rdquo; service that can be connected to from an arbitrary number clients. Once clients are connected, a viewer can make a request to the hub and quickly pull data from any of the connected clients. Here is a diagram of how I envisioned it:\nsequenceDiagram actor Viewer participant Hub Client A-\u003e\u003eHub: Connects via WebSocket Viewer-\u003e\u003eHub: Request data from Client A Hub-\u003e\u003eClient A: Notifies via WebSocket that request is pending Client A-\u003e\u003eHub: Responds with Data Hub-\u003e\u003eViewer: Responds with Data This design allows for disparate data to be accessed from a central location.\nIf you\u0026rsquo;d like a preview, I have deployed the code to my website. A client IP address and port should be listed. If you click it, the browser will initiate the request detailed in the above diagram. The returned dataset is the last 40 minutes of CPU utilization metrics for the client program host machine.\nThe Hub A single hub program keeps track of and facilitates communication to the network of clients. It does this using warp. Warp is web server framework in rust that allows for simultaneous websocket and http endpoints. Its configuration looks like this:\n// This stores info for each WebSocket client pub type Users = Arc\u0026lt;RwLock\u0026lt;HashMap\u0026lt;usize, Client\u0026gt;\u0026gt;\u0026gt;; // The browser hits this route to retrieve data from a specific Client ID let proxy_route = warp::path!(\u0026#34;api\u0026#34; / \u0026#34;proxy\u0026#34; / String) .and(warp::get()) .and(users.clone()) .and_then(proxy::handler); // The Client posts its data to this route let response_route = warp::path!(\u0026#34;api\u0026#34; / \u0026#34;proxy\u0026#34; / \u0026#34;response\u0026#34; / String) .and(warp::post()) .and(warp::body::bytes()) .and_then(client_response_handler); // This route lists all of the clients currently connected to the hub let clients_route = warp::path!(\u0026#34;api\u0026#34; / \u0026#34;clients\u0026#34;) .and(warp::get()) .and(users.clone()) .and_then(clients::handler); // This is the WebSocket server route let ws_route = warp::path!(\u0026#34;ws\u0026#34;) .and(warp::ws()) .and(warp::addr::remote()) .and(users.clone()) .map(|ws: warp::ws::Ws, addr: Option\u0026lt;SocketAddr\u0026gt;, users| { ws.on_upgrade(move |socket| user_connected(socket, addr, users)) }); // Serve files from the \u0026#34;public\u0026#34; directory let static_route = warp::fs::dir(\u0026#34;public\u0026#34;).with(log); // Define all of the routes let routes = proxy_route .or(clients_route) .or(response_route) .or(ws_route) .or(static_route) .recover(handle_rejection); // Bind the server with graceful shutdown let (addr, server_future) = warp::serve(routes).bind_with_graceful_shutdown( (host, config.http_server.port), shutdown_signal(running.clone()), ); The first route handler takes a String as path parameter (i.e the browser sends a GET request to /api/proxy/{some_client_id}). The function handler for these requests looks like this:\npub async fn handler(source: String, users: Users) -\u0026gt; Result\u0026lt;impl warp::Reply, warp::Rejection\u0026gt; { let id: usize = source .parse() .map_err(|_e| warp::reject::custom(ParseError))?; let user_map = users.read().await; let user = user_map.get(\u0026amp;id); if let Some(user) = user { let body = ClientMessage { request_id: Uuid::new_v4().to_string(), }; let mut req_map = PENDING_REQUESTS.write().await; req_map.insert(body.request_id.clone(), None); drop(req_map); let msg = Message::text(serde_json::to_string(\u0026amp;body).unwrap()); if let Err(_disconnected) = user.sender.send(msg) { eprintln!(\u0026#34;Could not reach client through websocket.\u0026#34;); }; let response = poll_for_response(\u0026amp;body.request_id).await; let json_res: Value = serde_json::from_str(\u0026amp;response.unwrap()).expect(\u0026#34;Expected response to be valid JSON\u0026#34;); Ok(warp::reply::json(\u0026amp;json_res)) } else { Err(warp::reject::not_found()) } } This performs the following steps:\nReads the path parameter source and converts it to unsigned integer. Attempts to find the client with an id matching the supplied parameter. If a client is found, a WebSocket message is sent to the client along with a generated uuid. Waits for a client to post a response with the same uuid. Responds with the data from the client. The route handler for step 4 looks like this:\npub async fn client_response_handler( request_id: String, body: warp::hyper::body::Bytes, ) -\u0026gt; Result\u0026lt;impl warp::Reply, warp::Rejection\u0026gt; { let data = std::str::from_utf8(\u0026amp;body).unwrap(); let mut req_map = PENDING_REQUESTS.write().await; if req_map.get(\u0026amp;request_id).is_none() { eprintln!(\u0026#34;here\u0026#34;); drop(req_map); return Err(warp::reject::custom(RequestIdNotFound)); } req_map.insert(request_id, Some(data.to_string())); Ok(warp::reply::html(\u0026#34;Success\u0026#34;)) } The data from the post body is written to a hashmap where the key is the generated uuid for the request. The poll_for_response method simply checks for a non-null value in HashMap where the key is the uuid and the value is the response from the client.\nCommand-Line Arguments In order to run the Hub and Client with the same binary, clap is used to parse CLI arguments.\nuse clap::{Parser, Subcommand}; /// Distributed Dashboard CLI #[derive(Parser)] #[command(version, about, long_about = None)] pub struct Args { #[command(subcommand)] pub command: Option\u0026lt;Commands\u0026gt;, } #[derive(Subcommand)] pub enum Commands { /// Runs the Client program Client {}, /// Runs the Hub program Hub {}, } The main function then runs different logic depending on the supplied argument, which looks like this:\nmatch \u0026amp;args.command { Some(Commands::Client {}) =\u0026gt; { println!(\u0026#34;Running the Client program\u0026#34;); ... } Some(Commands::Hub {}) =\u0026gt; { println!(\u0026#34;Running the Hub program\u0026#34;); ... } None =\u0026gt; { println!(\u0026#34;Invalid subcommand. See usage.\u0026#34;); } } The Client The client runs several threads in parallel.\nA WebSocket client that connects to the hub. A task that monitors CPU usage and records it in sqlite. A cleanup task that removes records from the sqlite database every day. To run these in parallel, tokio::spawn is used. Then, tokio::join waits for all to complete.\nlet cpu_task = tokio::spawn(cpu_monitor::cpu_monitoring_loop(running.clone())); let cleanup_task = tokio::spawn(cleanup::run(running.clone())); let websocket_task = tokio::spawn(websocket_client::connect_with_retry(running.clone())); let _ = tokio::join!(cpu_task, cleanup_task, websocket_task, ctrlc_task); Note: the ctrlc_task detects a user pressing ctrl-c on the keyboard so that all threads can be stopped gracefully.\nThe tokio-tungstenite crate is used for the WebSockets. Here is the portion that handles receiving messages from the hub and responding:\nMessage::Text(text) =\u0026gt; { println!(\u0026#34;Received: {text}\u0026#34;); let result = from_str::\u0026lt;ReqMsg\u0026gt;(\u0026amp;text).unwrap(); let client = Client::new(); match get_all_stats() { Ok(response) =\u0026gt; { let config = Options::new(); let server_url = format!( \u0026#34;{}/{}\u0026#34;, config.hub.proxy_response_uri, result.request_id, ); let res = client .post(server_url) .json(\u0026amp;response) .send() .await?; println!(\u0026#34;{}\u0026#34;, res.text().await?); }, Err(_e) =\u0026gt; { eprintln!(\u0026#34;An error occurred\u0026#34;); } } }, To monitor CPU usage, the sysinfo crate is used. This is super simple.\nlet mut sys = System::new_all(); sys.refresh_cpu_all(); let cpu_usage = sys.global_cpu_usage(); This value is written to sqlite every 5 seconds. The latest 500 rows of this data is sent to the hub when a WebSocket notification is received.\nAll of this code can be found on github. Remember, I am by no means rust expert yet, though I did learn a few things with this exercise.\nTry it yourself You can clone my repo and run the client to connect to my hub, assuming you have rust installed. First set some environment variables:\nexport HUB_PROXY_RESPONSE_URI=\u0026#34;https://taylordeckard.me/distributed-dashboard/api/proxy/response\u0026#34; export HUB_WS_URI=\u0026#34;wss://taylordeckard.me/distributed-dashboard/ws\u0026#34; Then compile and run the client.\ncargo run -- client After doing this, your client should show up (along with mine running in kubernetes) at https://taylordeckard.me/distributed-dashboard.\n","permalink":"https://www.taylordeckard.me/blog/posts/distributed-dashboard/","summary":"I have been wanting to try a project in rust for a few years now, and a few days ago, I finally started. My idea seemed simple. I wanted to create a \u0026ldquo;hub\u0026rdquo; service that can be connected to from an arbitrary number clients. Once clients are connected, a viewer can make a request to the hub and quickly pull data from any of the connected clients. Here is a diagram of how I envisioned it:","title":"Distributed Dashboard: Creating a Networked Client to Power Centralized Insights"},{"content":"Today I\u0026rsquo;m admitting a guilty pleasure: Bourbon. I read reviews online, hunt for rare bottles, and shell out (maybe a little too much) cash sometimes on interesting things I find. Bourbon has definitely experienced a surge in popularity over the past few years, particularly in the United States. In fact, bourbon has become so popular that the industry is struggling to keep up with demand. The reasons for bourbon\u0026rsquo;s rise in popularity are varied, but some contributing factors include the rise of the craft cocktail movement, a growing interest in American-made products, and an increased focus on artisanal and small-batch products.\nOne of the consequences of bourbon\u0026rsquo;s popularity is that certain bottles have become highly sought after, and they can command high prices on the secondary market. This is particularly true for limited edition or rare bottlings, as well as bottles that have garnered high ratings from influential critics.\nThe high demand for these bottles has created a secondary market where collectors and enthusiasts buy and sell bottles at prices far above their retail value. In some cases, these bottles can fetch thousands of dollars, and the prices can be driven up even further by auction houses or online bidding platforms.\nSome of the most sought-after bourbons include the Pappy Van Winkle line, which is produced in limited quantities and has a cult following among bourbon aficionados. Other popular bourbons include releases from Buffalo Trace\u0026rsquo;s Antique Collection, such as the George T. Stagg and William Larue Weller bottlings. These bottles can be difficult to find at retail, and some enthusiasts are willing to pay a premium to get their hands on them.\nBesides being a fan of whiskey, I\u0026rsquo;m fascinated by the sudden and explosive growth of markets for popular items. It\u0026rsquo;s intriguing to see how a previously niche product can become a mainstream sensation or a little-known brand can suddenly dominate the market. The study of these sudden market shifts is endlessly captivating, offering insights into the dynamism and unpredictability of consumer behavior and the economy.\nThanks to ChatGPT for helping me out with the filler text above. Now let\u0026rsquo;s talk about programming\u0026hellip;\nWeb Scraper I wanted an easier way to search for bourbon online. I have a few stores I like to browse and there are so many products that I just don\u0026rsquo;t care about. So I made a web scraper with typescript\u0026hellip;\nI wanted to design something extensible, since I knew there were several websites I wanted to scrape. For that reason, I started with a Scraper base class. Generally the scraping process requires three basic steps:\nScrape the total number of products you want to pull from the site. Paginate through the entire list of products and store the product information in memory. Save the data in a database. Incorporating this idea into the base Scraper class, I started with a simple scrape method:\nexport class Scraper { public async scrape(): Promise\u0026lt;void\u0026gt; { await this._db.connect(); await this._scrapeAllPages(); await this._storeData(); this._db.client.close(); } } I created a separate class to manage the DB connection:\nimport { MongoClient } from \u0026#34;mongodb\u0026#34;; export class DB { public client: MongoClient; constructor() { // All credentials are set via environment variables const user = process.env.DB_USER; const password = process.env.DB_PASSWORD; const host = process.env.DB_HOST; const port = process.env.DB_PORT; const url = `mongodb://${user}:${password}@${host}:${port}?retryWrites=true\u0026amp;w=majority`; this.client = new MongoClient(url); } public connect() { /** * This will return a promise that resolves after the connection * is made. If the client is already connected, the promise * immediately resolves. */ return this.client.connect(); } } Then imported it into the Scraper.\nimport { DB } from \u0026#34;./db\u0026#34;; export class Scraper { _db: DB; constructor() { /** * The scrapers will be executed in parallel, each within a worker. * For this reason each scraper will use a new DB class with separate * connections. */ this._db = new DB(); } public async scrape(): Promise\u0026lt;void\u0026gt; { await this._db.connect(); await this._scrapeAllPages(); await this._storeData(); this._db.client.close(); } } Ok, good. The database connection is in place. Now to implement step #1.\nexport class Scraper { ... async _getTotal(): Promise\u0026lt;number\u0026gt; { /** * This should return the total number products to scrape * from the website. This implementation will likely vary for * each website, so it just resolves a promise in the base class * and the child classes will override. */ return Promise.resolve(0); } async _scrapeAllPages() { /** * The first step is to get the number of products to scrape */ const totalProducts = await this._getTotal(); } ... } Step 2 contains the bulk of the work.\nexport class Scraper { _totalPages = 0; _productsPerPage = 20; _website = \u0026#34;\u0026#34;; ... async _scrapeAllPages() { const totalProducts = await this._getTotal(); /** * Add _totalPages and _productsPerPage as protected class variables * to be set by the child classes. */ this._totalPages = Math.ceil(totalProducts / this._productsPerPage); let currentPage = 1; while (currentPage \u0026lt;= this._totalPages) { /** * Since this is running in a worker, the next line reports progress * to the parent process. */ parentPort?.postMessage({ current: currentPage, scraper: this._website, total: this._totalPages, }); /** * Fetch the webpage, convert text to json, and parse the response. * These are methods that can be overridden by the child classes. */ this._parseResponse( this._convertTextToJson(await this._fetchUrl(currentPage)) ); currentPage += 1; } } ... } Using the total number of products along with the number of products per page, the total pages are calculated. Then, each page is queried with the _fetchUrl method, which looks like this:\nexport class Scraper { ... _pageKey = \u0026#34;page\u0026#34;; _queryParams: URLSearchParams = new URLSearchParams(); _url = \u0026#34;\u0026#34;; ... _buildUrl(page: number): string { /** * Sets the \u0026#34;page\u0026#34; query param and adds the query params to the * website URL. */ const url = new URL(this._url); const queryParams = new URLSearchParams(this._queryParams); /** * Note: The _pageKey is used in case a site uses a different * name for the \u0026#34;page\u0026#34; param, like \u0026#34;p\u0026#34; for instance. */ queryParams.set(this._pageKey, String(page)); url.search = queryParams.toString(); return url.toString(); } async _fetchUrl(page: number): Promise\u0026lt;string\u0026gt; { /** * Call the native fetch method to get a page of data. Returns * text output (html) to be parsed. Child classes that query * an API instead of a webpage can override this method to return * another format such as json. */ const response = await fetch(this._buildUrl(page)); const text = await response.text(); return text; } ... } The fetched page needs to be converted to json (in some cases) and parsed. The base class defines these methods to handle that:\nexport class Scraper { ... _convertTextToJson(response: string): any { try { return JSON.parse(response); } catch (e) { console.error(`Failed to parse json for ${this._website}`); return {}; } } _parseResponse(response: any): void { // Store products in this._data here (implemented in child class) } ... } The above _convertTextToJson method will only work if the response content is in json form. One of the child classes overrides that method like this, for instance:\nexport class Scraper { ... _convertTextToJson(response: string): any { const $ = cheerio.load(response); return $(\u0026#34;.main_box\u0026#34;) .filter((idx, elem) =\u0026gt; !$(elem).html()?.includes(\u0026#34;sold-out\u0026#34;)) .map((idx, elem) =\u0026gt; ({ handle: $(elem).find(\u0026#34;h5 a\u0026#34;).attr(\u0026#34;href\u0026#34;), price: this._formatPrice( $(elem).find(\u0026#34;.price .money:first-child\u0026#34;).text() ), title: $(elem).find(\u0026#34;h5 a\u0026#34;).text(), })) .toArray(); } ... } Here, cheerio is used to parse html text into an javascript object so that values can be easily extracted by the _parseResponse method called next in the flow. Here\u0026rsquo;s an example of that:\nexport class Scraper { ... _data: Bottle[] = []; _productLinkBase = \u0026#34;\u0026#34;; _scrapeId = 0; ... _buildLink(handle: string): string { return `${this._productLinkBase}${handle}`; } _parseResponse(response: any): void { if (response) { response.forEach((product: any) =\u0026gt; { this._data.push({ link: this._buildLink(product.handle), price: product.price, scrapeId: this._scrapeId, title: product.title, website: this._website, }); }); } } ... } The scraped data is stored in memory to the _data class variable. Other class variables used here are\n_productLinkBase: Used to build the link to an individual product\u0026rsquo;s page _scrapeId: See below. The interface for the stored product data looks like this:\nexport interface Bottle { /* Link to the individual product detail page */ link: string; /* Price of the product */ price: number; /** * Unique identifier of the current scrape execution. This is used to delete * any products from the previous pull that are not present in the current * pull. */ scrapeId: number; /* Product Title */ title: string; /* Website identifier (used for filtering) */ website: string; /** * Flag to denote a product that is present in the latest scrape but * was not present in the previous scrape. */ fresh?: boolean; } Now the data is stored in memory and it just needs to be written out to the database:\nexport class Scraper { ... _storeData() { if (!this._data.length) { // Do nothing if no data could be scraped return; } /** * Execute a bulk insert operation. If a specific title/website * already exists in the database, the fields will be updated for that * document, i.e. \u0026#34;upserted\u0026#34;. */ const bulkOp = this._db.client .db(\u0026#34;bottles\u0026#34;) .collection(\u0026#34;bottles\u0026#34;) .initializeUnorderedBulkOp(); this._data.forEach((d) =\u0026gt; { bulkOp .find({ title: d.title, website: d.website, }) .upsert() .updateOne({ $set: d, /** * On insert, the `fresh` field is set to true. In order for this * to work, another step must occur prior to the scrape. I\u0026#39;ll explain * further down the page. */ $setOnInsert: { fresh: true, }, }); }); return bulkOp.execute(); } ... } That\u0026rsquo;s enough to build a Scraper. View an example child Scraper class here.\nNow to run them in parallel using workers threads\u0026hellip; The main index.ts file looks like this:\nimport { scrapers } from \u0026#34;./scrapers\u0026#34;; import { DB } from \u0026#34;./db\u0026#34;; import { Worker, isMainThread, workerData } from \u0026#34;node:worker_threads\u0026#34;; import * as cliProgress from \u0026#34;cli-progress\u0026#34;; async function main() { if (isMainThread) { // On the main thread, all of the child worker threads are spawned. const workers = scrapers.map((_, scraperIdx) =\u0026gt; { return new Worker(__filename, { /** * The index is passed to the worker so that it knows * the Scraper to run */ workerData: { scraperIdx }, }); }); // A set of progress bars is displayed when running via CLI const multibar = new cliProgress.MultiBar( { clearOnComplete: false, hideCursor: true, format: \u0026#34; {bar} | {filename} | {value}/{total}\u0026#34;, }, cliProgress.Presets.shades_grey ); // Bar progress is tracked in a key-value store const bars: { [key: string]: cliProgress.Bar } = {}; // Inialize an array to track when each worker completes its job const done = workers.map(() =\u0026gt; false); workers.forEach((worker, idx) =\u0026gt; { /** * Create a message listener for each worker. Workers report progress * after each page is scraped and the bars are updated here accordingly. */ worker.on( \u0026#34;message\u0026#34;, (progress: { scraper: string; current: number; total: number }) =\u0026gt; { if (!Object.keys(bars).includes(progress.scraper)) { bars[progress.scraper] = multibar.create(progress.total, 0, { filename: progress.scraper, }); } bars[progress.scraper].update(progress.current); if (progress.total === progress.current) { bars[progress.scraper].stop(); done[idx] = true; // When all workers are done, stop the multibar if (done.every((d) =\u0026gt; d)) { multibar.stop(); } } } ); }); } else { // Worker threads hit this branch. new scrapers[workerData.scraperIdx]().scrape(); } } main(); The scrapers are exported in the scrapers/index.ts file.\nimport { Seelbachs } from \u0026#34;./seelbachs\u0026#34;; import { Sharedpour } from \u0026#34;./sharedpour\u0026#34;; import { Sipwhiskey } from \u0026#34;./sipwhiskey\u0026#34;; export const scrapers = [ Seelbachs, Sharedpour, Sipwhiskey, ]; A Few More Things The finalized scrape method looks like this:\nexport class Scraper { ... public async scrape(): Promise\u0026lt;void\u0026gt; { await this._db.connect(); await this._setScrapeId(); await this._setFreshness(); await this._scrapeAllPages(); await this._storeData(); await this._deletePreviousScrape(); this._db.client.close(); } ... } Notice there are a methods I didn\u0026rsquo;t mention before.\nThe scrapeId Every time a scraper runs, it will query the database to select a scrapeId from one of the documents. If any documents for the Scraper\u0026rsquo;s _website exist, the _scrapeId for the current scrape will 1 greater than the scrapeId of the document. Otherwise, if there are no documents, the _scrapeId for the current scrape is set to 0.\nexport class Scraper { ... async _setScrapeId() { this._scrapeId = (( await this._db.client .db(\u0026#34;bottles\u0026#34;) .collection(\u0026#34;bottles\u0026#34;) .findOne({ website: this._website }, { projection: { scrapeId: 1 } }) )?.scrapeId ?? -1) + 1; } ... } The upsert from the current scrape should update the scrapeId for all of the documents. After the newly scraped data is stored, any documents with the old scrapeId are deleted. This ensures that if a product was removed from the website, it is also removed from the database.\nexport class Scraper { ... _deletePreviousScrape() { return this._db.client .db(\u0026#34;bottles\u0026#34;) .collection(\u0026#34;bottles\u0026#34;) .deleteMany({ website: this._website, scrapeId: { $ne: this._scrapeId, }, }); } ... } Updating freshness Finally, in order to keep track of new products, a flag is added using the mongo $setOnInsert operator. However, for this to work, all of the existing documents for the current scrape need to be set to false prior to insertion.\nexport class Scraper { ... _setFreshness() { return this._db.client .db(\u0026#34;bottles\u0026#34;) .collection(\u0026#34;bottles\u0026#34;) .updateMany( { website: this._website, }, { $set: { fresh: false, }, } ); } ... } Conclusion All of the code described here can be found on github. In the next post, I\u0026rsquo;ll go over creating a frontend with Next.js and GraphQL.\n","permalink":"https://www.taylordeckard.me/blog/posts/bourbon/","summary":"Today I\u0026rsquo;m admitting a guilty pleasure: Bourbon. I read reviews online, hunt for rare bottles, and shell out (maybe a little too much) cash sometimes on interesting things I find. Bourbon has definitely experienced a surge in popularity over the past few years, particularly in the United States. In fact, bourbon has become so popular that the industry is struggling to keep up with demand. The reasons for bourbon\u0026rsquo;s rise in popularity are varied, but some contributing factors include the rise of the craft cocktail movement, a growing interest in American-made products, and an increased focus on artisanal and small-batch products.","title":"Bourbon Search pt. 1: Scraper"},{"content":"A few months ago, our team was tasked with leading a new hire programming bootcamp. My focus has been creating course assignments for JavaScript, TypeScript, and Node.js. As a learning exercise, I created a CLI clone of the popular game Wordle to use as an example project.\nFor the bootcamp, I\u0026rsquo;ve broken down the project into various learning exercises. These exercises are assessed by unit test cases and linting jobs that run in Concourse CI. The tasks are fairly basic introductory material and (in my opinion) are less interesting than the overall functionality of the project.\nFor instance, in order to accurately clone the actual Wordle, I did a bit of reverse-engineering that turned out to be fairly simple. Inspecting the Wordle web page JavaScript source reveals that all of the answers for each day are stored in an array of strings. The answer for the current day is determined by using an index derived from the number of days since the first day of Wordle (June 19, 2021.)\nWith this information, I began coding a program that:\nFetches the actual Wordle web page index.html file and parses it to get the JavaScript source file. public static parseWordleIndex(html: string) { return html.match(/\u0026lt;script src=\u0026#34;(main.*?js)\u0026#34;\u0026gt;\u0026lt;\\/script\u0026gt;/)?.[1] ?? \u0026#39;\u0026#39;; } Fetches the JavaScript file from step 1 and parses it to get the list of answers. public static parseWordleJavascript(jsFile: string) { try { const array = jsFile.match(/;var ..=(\\[.*?\\])/)?.[1]; return JSON.parse(array ?? \u0026#39;[]\u0026#39;); } catch (e) { return []; } } Finds the number of days since June 19, 2021 and uses that to get the current day\u0026rsquo;s answer. See the Pen Sample by Taylor Deckard (@taylordeckard) on CodePen. From this point, all that is left is to write the CLI and game logic. I chose to use commander.js, which is a nice wrapper for argument parsing and command execution. Basic setup looks like this:\nimport { program } from \u0026#39;commander\u0026#39;; import { commands } from \u0026#39;./commands\u0026#39;; program .name(\u0026#39;wordle\u0026#39;) .description(\u0026#39;A CLI Wordle clone\u0026#39;) .version(\u0026#39;1.0.0\u0026#39;); // Split commands out into separate modules to keep things tidy. // Add each command to the program here. commands.forEach((command) =\u0026gt; { let pgm = program .command(command.name) .description(command.description); if (command.options?.length) { command.options.forEach((opt) =\u0026gt; { pgm = pgm.option(opt.invocation, opt.description, opt.default); }); } pgm = pgm.action(command.action); }); program.parse(process.argv); The game logic is pretty straight-forward. It give the user 6 chances to guess the correct word. After each guess, log green/yellow colors to indicate correctness.\n/** * Note: This is a snippet from a class method. The DataService * is a singleton used to retrieve and store remote data, * such as the Wordle answer list. */ const ds = DataService.instance; this._solution = (await ds.solution) ?? \u0026#39;\u0026#39;; // See next code snippet for the Prompter logic const prompter = new Prompter(); // MAX_ATTEMPTS is 6 for (let i = 0; i \u0026lt; MAX_ATTEMPTS; i += 1) { // Pass in the number of attempts remaining to inform the user const { guess } = await prompter.promptUserGuess(MAX_ATTEMPTS - i); // Print results new Guess(guess, this._solution) .markGreen() .markYellow() .logOutput(); if (guess === this._solution) { Logger.printf(YOU_WIN); this._solved = true; break; } } if (!this._solved) { Logger.printf(YOU_LOSE, this._solution); } For prompting the user, I chose to use the inquirer.js module. It allows for input transformation and validation.\npublic async promptUserGuess(attemptsRemaining: number) { if (!this._acceptableGuesses) { const ds = DataService.instance; this._acceptableGuesses = await ds.wordlist; } return inquirer.prompt([ { message: `Guess a 5-letter word (${attemptsRemaining} attempts remaining):`, name: \u0026#39;guess\u0026#39;, transformer: (input: string) =\u0026gt; input.toLowerCase(), type: \u0026#39;input\u0026#39;, validate: Validator.checkGuess.bind(this, this._acceptableGuesses), }, ]); } Finally, the logic for colorizing the guesses is:\n/** * Marks letters of the guess as green (correct) * * @returns {Guess} guess */ public markGreen() { // iterate over each letter of the guess for (let i = 0; i \u0026lt; 5; i += 1) { const guessChar = this._guess[i]; const actualChar = this._solution[i]; // if the character in the guess matches the character at the // same index of the solution, it should be marked green if (guessChar === actualChar) { this._output[i] = Colorizer.green(guessChar); this._correctness[i] = Correctness.GREEN; this._remainingChars[i] = \u0026#39;\u0026#39;; } } return this; } /** * Marks letters of the guess as yellow (included in solution but wrong index) * * @returns {Guess} guess */ public markYellow() { // iterate over each letter of the guess for (let i = 0; i \u0026lt; 5; i += 1) { const guessChar = this._guess[i]; /* if the character is included in the _remainingChars and does not match the current index of the solution, mark yellow note: _remainingChars is an array of characters of the solution that have not been marked green */ if (this._remainingChars.includes(guessChar) \u0026amp;\u0026amp; guessChar !== this._solution[i]) { this._correctness[i] = Correctness.YELLOW; this._output[i] = Colorizer.yellow(guessChar); } } return this; } Now it\u0026rsquo;s done. The program can be executed with:\nnpm start -- start Later, I started working on another command that tries to solve the Wordle puzzle using an algorithm. For a start, I filtered the list of possible words based on previous guess outcomes. Then, I tested the algorithm against the list of wordle answers. I began tweaking the algorithm from there, doing things like trying different starting words or using strategies to eliminate letters.\nI was able adjust enough to achieve ~99.61% win accuracy. The automation can be run with:\nnpm start -- solve I\u0026rsquo;m sure the algorithm can be improved to reach 100%, though it\u0026rsquo;s a task for another time.\nCheck out the project on Github.\n","permalink":"https://www.taylordeckard.me/blog/posts/wordle/","summary":"A few months ago, our team was tasked with leading a new hire programming bootcamp. My focus has been creating course assignments for JavaScript, TypeScript, and Node.js. As a learning exercise, I created a CLI clone of the popular game Wordle to use as an example project.\nFor the bootcamp, I\u0026rsquo;ve broken down the project into various learning exercises. These exercises are assessed by unit test cases and linting jobs that run in Concourse CI.","title":"Wordle CLI with Node.js and TypeScript"},{"content":"\nI\u0026rsquo;ve been using Vim as my primary editor for ~7 years. When first starting to use it, I compared it to learning a musical instrument. When learning chord progressions I\u0026rsquo;d start by playing slowly at first, thinking for a bit about which fingers go on which positions. Similarly, I had to think for a few seconds about which keys to press for each edit I was about to make in Vim. As with an instrument, the number of times you repeat the sequences, the more fluid the motions become. After several weeks of coding in vim, I no longer had to pause.\nThe more I learned about things you can do with Vim, the more I wanted to use it\u0026hellip; and there is a lot you can do. For this post, I\u0026rsquo;ll go over how to get started using Vim from scratch.\nI spend a majority of my day in the macOS Terminal, which is one of the reasons I like using Vim so much. In order to start from a completely clean slate though, I\u0026rsquo;ll install vim fresh in a docker container running ubuntu linux. To do that in Terminal the command is:\ndocker run -ti ubuntu In the ubuntu shell, I\u0026rsquo;ll install vim with:\napt-get update \u0026amp;\u0026amp; apt-get install -y vim Then I\u0026rsquo;ll run Vim with vi. It looks like this: Now what? I\u0026rsquo;ll pretend for a moment that I have zero experience with Vim. The splash screen shown above gives a little context. Use :q\u0026lt;Enter\u0026gt;. Use :help\u0026lt;Enter\u0026gt; for on-line help. The Vim documentation is very good. You can also search for topics by using :help word\u0026lt;Enter\u0026gt; (where \u0026ldquo;word\u0026rdquo; is the topic you want to search for.)\nExecuting Commands First, note that typing : in normal mode (I\u0026rsquo;ll talk more about modes in a bit) will display the : at the bottom of the window. Any characters you type next will also be displayed here. This is the command-line input. Notice :help and :q are commands that do different things. There are many different commands. You can even run shell commands from Vim.\nFor now, just remember that :q!\u0026lt;Enter\u0026gt; quits Vim without saving. Notice I\u0026rsquo;ve added a !. This forces Vim to quit even if there are unsaved edits. Alternatively, :w\u0026lt;Enter\u0026gt; saves, and :wq!\u0026lt;Enter\u0026gt; quits and saves.\nMoving Around (part 1) Learning how to move the cursor around efficiently is probably one of the biggest challenges when starting out. The arrow keys can be used, but it is recommended to instead use h, j, k, and l. See the below mapping.\nUp: k Down: j Left: h Right: l Typing each of these keys individually will move one character for h or l and one line for j or k. If you want to move down 20 lines, you don\u0026rsquo;t have to press j 20 times. Instead, you can type 20j.\nWith that said, knowing which line to move down to can be difficult if you can\u0026rsquo;t see the line numbers. You can show them with :set number\u0026lt;Enter\u0026gt;. To make jumping around even easier, you can :set relativenumber\u0026lt;Enter\u0026gt; which will still display the line number of the current line, however all other lines display the number of lines away from the current line. See the example below. Sorry, your browser doesn't support embedded videos. I like to have the relativenumber setting active every time I open a file. Luckily, there is an easy to make Vim remember all of your settings.\nConfiguring ~/.vimrc To store Vim configurations, I\u0026rsquo;ll create a .vimrc file.\nvi ~/.vimrc Now I want to add the setting to show relative line numbers to the .vimrc file.\nset relativenumber There is a problem though. I haven\u0026rsquo;t mentioned how to insert text yet. To demonstrate, I\u0026rsquo;ll enter this key sequence: iset relativenumber\u0026lt;Esc\u0026gt;:wq!\u0026lt;Enter\u0026gt;. Now, to explain\u0026hellip;\nModes Vim has a few different editor modes that you can switch between:\nNormal: Cursor navigation, entering other modes. Command: Command execution mode. (Initiated with the : key from Normal mode) Insert: Text editing mode. Replace: Anything you type will replace the underlying text. Visual: Used to select text. There are also Visual Line and Visual Block modes which allow text selection in different ways. Consider the key sequence iset relativenumber\u0026lt;Esc\u0026gt;:wq!\u0026lt;Enter\u0026gt;. Vim starts in Normal mode. The i key initiates Insert mode. At this point, set relativenumber is recorded as it is typed. Then, \u0026lt;Esc\u0026gt; exits Insert mode and Vim is back in Normal mode. Typing : enters Command mode and wq!\u0026lt;Enter\u0026gt; triggers the command to save and quit. When actively using Vim, I am switching between modes almost constantly.\nMapping Keys An awesome feature of Vim is the ability to map a key or key sequence to an action. This allows you customize Vim so that common tasks are easier to execute.\nFor instance, a mapping that I use is:\ninoremap jk \u0026lt;esc\u0026gt; This makes it so typing jk (quickly) in Insert mode will trigger the Escape keypress to enter Normal mode. I find this is easier than reaching to the Escape key for such a commonly performed action. There are very few scenarios when I actually need to insert the letters jk and on those occasions I can type slow so that the mapping isn\u0026rsquo;t triggered. You can add the above line to your ~/.vimrc to try it out.\nThe mapping might seem confusing at first. The first term, inoremap is the type of mapping. We can break this down further into three parts:\ni - denotes Insert mode nore - not recursive (other mappings cannot trigger this mapping) map - basic map command The second term, jk, is the keypress sequence that triggers the mapping (input.) The third term, \u0026lt;esc\u0026gt;, is the key that the mapping executes (output.)\nAnother mapping that I use frequently is:\nnnoremap \u0026lt;C-n\u0026gt; :Explore\u0026lt;cr\u0026gt; Note the first term begins with a n which denotes the mapping will only trigger in Normal mode. The \u0026lt;C-n\u0026gt; represents a combined keypress of the Ctrl and n keys. The last term, :Explore\u0026lt;cr\u0026gt;, enters Command mode and executes netrw, the native Vim file explorer. (\u0026lt;cr\u0026gt; is \u0026ldquo;carriage return\u0026rdquo;, also known as the Enter key.)\nYou also have the option to make use of a \u0026ldquo;leader\u0026rdquo; key. This is a key of your choosing that you can use in mappings with the syntax: \u0026lt;leader\u0026gt;. I set my leader ket to be ; by adding the following line in my ~/.vimrc:\nlet mapleader = \u0026#34;;\u0026#34; Now, the leader can be used in mappings like this:\nnnoremap \u0026lt;leader\u0026gt;w :w!\u0026lt;cr\u0026gt; This mapping allows for quick saving with ;w in Normal mode.\nYou can also disable default mappings. In the past, to train myself to stop using the arrow keys for navigation, I added the following to my ~/.vimrc:\nnnoremap \u0026lt;Up\u0026gt; \u0026lt;nop\u0026gt; nnoremap \u0026lt;Down\u0026gt; \u0026lt;nop\u0026gt; nnoremap \u0026lt;Left\u0026gt; \u0026lt;nop\u0026gt; nnoremap \u0026lt;Right\u0026gt; \u0026lt;nop\u0026gt; vnoremap \u0026lt;Up\u0026gt; \u0026lt;nop\u0026gt; vnoremap \u0026lt;Down\u0026gt; \u0026lt;nop\u0026gt; vnoremap \u0026lt;Left\u0026gt; \u0026lt;nop\u0026gt; vnoremap \u0026lt;Right\u0026gt; \u0026lt;nop\u0026gt; inoremap \u0026lt;Up\u0026gt; \u0026lt;nop\u0026gt; inoremap \u0026lt;Down\u0026gt; \u0026lt;nop\u0026gt; inoremap \u0026lt;Left\u0026gt; \u0026lt;nop\u0026gt; inoremap \u0026lt;Right\u0026gt; \u0026lt;nop\u0026gt; This overrides the arrow keys to be no-ops in Normal, Visual, and Insert modes. With these set, you must use the h, j, k, and l keys instead.\nThere are many alternative ways to move the cursor around in Vim though\u0026hellip;\nMoving Around (part 2) When reading a large file, moving the cursor line by line is not ideal. Instead of using j/k to move, you can move half the page at a time with Ctrl-D/Ctrl-U.To go to the last line in the file, simply press G. To go back to the first line, press gg. Sorry, your browser doesn't support embedded videos. On a long line, you may want to move forward and backward by word instead of by character. For this, use w to move to the next word and b to move to the previous. Similarly to moving down 20 lines with 20j, you can move forward 20 words with 20w.\nEntering ^ moves the cursor to the first non-whitespace character on a line and $ moves it to the last character.\nIf there is a particular character you need to reach on a line, f followed by the character will move the cursor to the next occurence of the character on the line. Using F will move to the previous occurence.\nYou can also easily perform a search to move your cursor to a word anywhere in the file. Simply type / (to search forward, ? to search backward) followed by the word you\u0026rsquo;re searching for, then Enter. When searching, n moves to the next occurence of the searched term, while N moves to the previous occurrence. Add the following line to your ~/.vimrc for Vim to highlight the search matches for you:\nset hlsearch Also, I like to use the following settings:\nset ignorecase set smartcase This makes it so searching for a word with all lowercase characters ignores case. However, a search including uppercase letters will be case-sensitive. Sorry, your browser doesn't support embedded videos. When the cursor is placed on a character of a pair, like opening and closing brackets, % will move the cursor to the opposite character. For example, if you are editing a text file that looks like { \u0026quot;an\u0026quot;: \u0026quot;object\u0026quot; } and your cursor is on {, pressing % will move the cursor to }.\nIf at any time you make a bad move, you can go back to your previous position with Ctrl-o.\nEditing Text I\u0026rsquo;ve mentioned that the i key from Normal mode switches to Insert mode in which text is recorded as you type. There are other ways to enter Insert mode as well. All of the keys below will enter Insert mode and also move the cursor to a new position:\na: Moves the cursor one character to the right. A: Moves the cursor to the end of the line. I: Moves the cursor to the beginning of the line. o: Inserts a line below the current line. O: Inserts a line above the current line. Some keys enter Insert mode and also delete characters:\nc: Used in combination with a motion will delete characters within the motion. For instance, cw will delete the word all text from the cursor to the next word. cc: Deletes the entire line. C: Deletes all text on the line after the cursor. s: Deletes a single character. Alternatively, some keys delete characters without changing from Normal mode:\nd: Works similarly to c except does not enter Insert mode. dd: Deletes the entire line (same as cc.) D: Deletes all text on the line after the cursor (C.) x: Deletes a single character. If at any point you make a mistake, you can undo the action with u. (To redo the undo, use Ctrl-r.)\nSelecting Text Text selection is made possible in Vim with Visual mode. Ways to enter visual mode are:\nv: Selects a single character. V: Selects the entire line. Ctrl-v: Selects a single character, however subsequent cursor movement selects blocks of text vertically. Moving the cursor after entering Visual mode will add to selection until exiting Visual mode with Escape or Ctrl-C.\nOnce in visual mode, you can perform actions on the selected text such as:\nd/x: Delete the selected text. c/s: Delete the selected text and enter Insert mode. gu/gU/~: Change the case of all letters in the selection. \u0026lt;/\u0026gt;: Indent the selected text left or right. :norm @\u0026lt;register\u0026gt;: Execute a macro on all lines (read about these a bit further down.) y: Yank (copy) the selected text. Note that the above may not apply to Visual Block mode. Here\u0026rsquo;s an example of that: Sorry, your browser doesn't support embedded videos. The above video shows how to convert a ordered list to an unordered list using Visual Block mode. The key sequence is \u0026lt;Ctrl-v\u0026gt;Gls-\u0026lt;Esc\u0026gt; Starting with the cursor in the top-left position, Ctrl-v enters Visual Block mode. Then, G moves the cursor to the bottom of the file, creating a selection of the entire first column. l moves the cursor to the right one character, expanding the selection to the first two columns. s then deletes the selected text and enters Insert mode. It is important to note that Insert mode from Visual Block affects all rows that were selected. This means that - inserts the dash and Esc exits Insert mode while inserting the \u0026ldquo;-\u0026rdquo; on all lines that were selected in the Visual Block.\nCopying and Pasting After selecting some text, you can yank (copy) it with y and then paste it with :\np: Paste text immediately after the cursor. P: Paste text immediately before the cursor. When yanked, text is stored in a register - a space in memory. Specifically, when yanked, text is stored in the default register (\u0026quot;).\nUsing a delete key (x, d, etc.) is actually similar to a \u0026ldquo;cut\u0026rdquo; in that some text is deleted and also stored in a register so that it can later be pasted.\nRegisters You can view the contents of a register by pressing Ctrl-r in Insert mode followed by the name of the register. You can try this out by selecting some text and typing yi\u0026lt;Ctrl-r\u0026gt;\u0026quot;. This output is the same as paste.\nYou can also yank and paste to a specified register. Select some text and type \u0026quot;ay. This yanks the selection to register a. Now, you can paste the contents of the a register with \u0026quot;ap. You could also use \u0026lt;Ctrl-r\u0026gt; to do the same, as described above. (In Insert mode, type \u0026lt;Ctrl-r\u0026gt;a.)\nMacros Macros are a useful feature that also make use of registers. They allow you to record a sequence of key presses and repeat the sequence any number of times.\nIn the below example, I use a macro to create an ordered list: Sorry, your browser doesn't support embedded videos. First, i1. is entered on the first line. Then, qa begins recording the macro. Any key pressed at this point will be recorded until q is pressed in Normal mode. ^ moves the cursor to the begining of the line. v enters visual mode and f moves the cursor to the first space character. Then y yanks \u0026ldquo;1. \u0026quot; and moves the cursor back to the beginning of the line. j moves the cursor down. P pastes the \u0026ldquo;1. \u0026quot; before the cursor. ^ moves to the beginning of the 2nd line and \u0026lt;Ctrl-a\u0026gt; increments the number under the cursor by 1. Finally, q ends recording of the macro. The macro stored in the a register looks like this: ^vf yjP^\u0026lt;c-a\u0026gt;.\nTo use the macro, select the lines you want to execute the macro on and type :norm @a\u0026lt;Enter\u0026gt;. Another way to do this is to count the number of lines you need to apply the macro to and with the cursor on the first line in Normal mode type 7@a. (The number of lines is 7 for the example above.)\nIf you make a mistake when recording a macro, you can paste the register, correct the text, and then yank the fixed text back to the register. Then play the macro as you normally would.\nPlugins Check out VimAwesome for plugins. I personally use:\nALE - Linting YouCompleteMe - Autocompletion incsearch.vim - Highlight text as you search vim-airline - Enhanced Status Bar vim-commentary - Mappings to add code comments vim-fugitive - Git Integration vim-surround - Mappings to easily delete, change and add parentheses, brackets, quotes, XML tags, and more surroundings in pairs. For an example, I\u0026rsquo;ll install vim-airline using Vim\u0026rsquo;s native packages.\nFirst, I\u0026rsquo;ll need git:\napt install -y git Now, I\u0026rsquo;ll create a directory for the plugins:\nmkdir -p ~/.vim/pack/plugins/start Next, clone the vim-airline repository into the directory that was just created.\ngit clone https://github.com/vim-airline/vim-airline.git ~/.vim/pack/plugins/start/vim-airline Finally, add the following line to the ~/.vimrc to set the maximum number of colors that can be displayed by the host terminal:\nset t_Co=256 To check that the plugin is installed and working, restart Vim. It should now look like this: ","permalink":"https://www.taylordeckard.me/blog/posts/vim-1/","summary":"I\u0026rsquo;ve been using Vim as my primary editor for ~7 years. When first starting to use it, I compared it to learning a musical instrument. When learning chord progressions I\u0026rsquo;d start by playing slowly at first, thinking for a bit about which fingers go on which positions. Similarly, I had to think for a few seconds about which keys to press for each edit I was about to make in Vim.","title":"Vim Intro"},{"content":"I\u0026rsquo;ve started working on a proof-of-concept to improve query performance of a large dataset (5M+ rows.) The data is currently stored in a MySQL database.\nThe service is required to search, sort, filter, and paginate the data. Nowadays, these requirements are standard practice. However, with such a large dataset, some of the database queries are taking \u0026gt; 3 seconds, even with table partitioning.\nMy theory is that Elasticsearch will perform better than RDBMS for this use case. Only one way to find out\u0026hellip;\nRunning Elasticsearch Locally I already have Docker and docker-compose installed, so I just need to create a docker-compose config.\nversion: \u0026#34;3.9\u0026#34; # Run 3 Elasticsearch containers simultaneously (es01, es02, es03) services: es01: image: docker.elastic.co/elasticsearch/elasticsearch:7.16.3 container_name: es01 environment: - node.name=es01 - cluster.name=es-docker-cluster - discovery.seed_hosts=es02,es03 - cluster.initial_master_nodes=es01,es02,es03 - bootstrap.memory_lock=true - \u0026#34;ES_JAVA_OPTS=-Xms512m -Xmx512m\u0026#34; # Allow unlimited memory to be locked by this container process ulimits: memlock: soft: -1 hard: -1 volumes: # Map data01 directory to container (for persistence) - data01:/usr/share/elasticsearch/data ports: # Map local port 9200 to container port 9200 - 9200:9200 networks: - elastic es02: image: docker.elastic.co/elasticsearch/elasticsearch:7.16.3 container_name: es02 environment: - node.name=es02 - cluster.name=es-docker-cluster - discovery.seed_hosts=es01,es03 - cluster.initial_master_nodes=es01,es02,es03 - bootstrap.memory_lock=true - \u0026#34;ES_JAVA_OPTS=-Xms512m -Xmx512m\u0026#34; ulimits: memlock: soft: -1 hard: -1 volumes: - data02:/usr/share/elasticsearch/data networks: - elastic es03: image: docker.elastic.co/elasticsearch/elasticsearch:7.16.3 container_name: es03 environment: - node.name=es03 - cluster.name=es-docker-cluster - discovery.seed_hosts=es01,es02 - cluster.initial_master_nodes=es01,es02,es03 - bootstrap.memory_lock=true - \u0026#34;ES_JAVA_OPTS=-Xms512m -Xmx512m\u0026#34; ulimits: memlock: soft: -1 hard: -1 volumes: - data03:/usr/share/elasticsearch/data networks: - elastic volumes: data01: driver: local data02: driver: local data03: driver: local # ES Nodes run on a shared network that is bridged to the local machine networks: elastic: driver: bridge Elasticsearch should be ready to go. Before running though, I\u0026rsquo;ll need to bump up Docker\u0026rsquo;s memory resources to \u0026gt; 4GB. Start docker-compose with\ndocker-compose up Creating an Index Now that Elasticsearch is running, it\u0026rsquo;s time to learn how to use it: REST API docs.\nFirst, I want to create an index (table in SQL) for my dataset so I can add documents (rows) to it.\nI create an index called assets (API Docs):\ncurl -X PUT \u0026#39;http://localhost:9200/assets\u0026#39; The following is shown in the docker-compose logs:\n{\u0026#34;type\u0026#34;: \u0026#34;server\u0026#34;, \u0026#34;timestamp\u0026#34;: \u0026#34;2022-01-21T15:50:11,935Z\u0026#34;, \u0026#34;level\u0026#34;: \u0026#34;INFO\u0026#34;, \u0026#34;component\u0026#34;: \u0026#34;o.e.c.m.MetadataCreateIndexService\u0026#34;, \u0026#34;cluster.name\u0026#34;: \u0026#34;es-docker-cluster\u0026#34;, \u0026#34;node.name\u0026#34;: \u0026#34;es02\u0026#34;, \u0026#34;message\u0026#34;: \u0026#34;[assets] creating index, cause [api], templates [], shards [1]/[1]\u0026#34;, \u0026#34;cluster.uuid\u0026#34;: \u0026#34;6BFPOn84Q8qFIfS2FSmsBw\u0026#34;, \u0026#34;node.id\u0026#34;: \u0026#34;Sm9PsrFzTu6tkK0tAt0qvw\u0026#34; } which indicates the index creation was successful.\nAnother way to check that the index creation was successful is to do\ncurl --head \u0026#39;http://localhost:9200/assets\u0026#39; A 200 response means that the index exists, whereas 404 means it doesn\u0026rsquo;t.\nWorking with Documents Adding a document to the index is simple (API Docs):\ncurl -X POST \u0026#39;http://localhost:9200/assets/_doc/\u0026#39; \\ -H \u0026#39;Content-Type: application/json\u0026#39; \\ -d \u0026#39;{ \u0026#34;name\u0026#34;: \u0026#34;Test Data\u0026#34; }\u0026#39; Then retrieve the document with (API Docs)\ncurl -X GET \u0026#39;localhost:9200/assets/_search?pretty\u0026#39; \\ -H \u0026#39;Content-Type: application/json\u0026#39; \\ -d \u0026#39;{ \u0026#34;query\u0026#34;: { \u0026#34;match\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;Test Data\u0026#34; } } }\u0026#39; Mocking Data The next step is to populate the local Elasticsearch database up with mock documents to mimic a production environment. I\u0026rsquo;m familiar with Node.js, so I\u0026rsquo;ll write a quick script to do this.\nCreate a directory to contain the script.\nmkdir mock-data \u0026amp;\u0026amp; cd mock-data Add a package.json\nnpm init -y Now, I\u0026rsquo;ll add a couple of dependencies. To create mock-data I\u0026rsquo;m using a fork of faker.js, (community-faker.) I\u0026rsquo;ll also use node-fetch to make HTTP requests.\n# shorthand for npm install, -S writes the dependencies to package.json npm i -S community-faker node-fetch Next, write the script: index.js. (Note: I\u0026rsquo;m using Node.js v14.16.1)\nimport faker from \u0026#39;community-faker\u0026#39;; import fetch from \u0026#39;node-fetch\u0026#39;; // Number of documents to insert const NUM_DOCUMENTS = 25000; const ES_HOST = \u0026#39;http://localhost:9200\u0026#39;; // The ES index to insert records into const INDEX = \u0026#39;assets\u0026#39;; (async function run () { for (let i = 0; i \u0026lt; NUM_DOCUMENTS; i++) { const doc = { name: faker.name.findName(), id: faker.datatype.uuid(), }; // Send document to ES via http API const response = await fetch(`${ES_HOST}/${INDEX}/_doc`, { // node-fetch requires stringifying the json body body: JSON.stringify(doc), headers: { // Content-Type header is required for ES APIs with bodies \u0026#39;Content-Type\u0026#39;: \u0026#39;application/json\u0026#39;, }, method: \u0026#39;post\u0026#39;, }) if (!response.ok) { // If an error occurred, log the error and exit console.log(await response.json()); break; } // Write out progress on a single line process.stdout.clearLine(); process.stdout.cursorTo(0); process.stdout.write(`${i + 1} / ${ NUM_DOCUMENTS } documents inserted`); } })(); Finally, run the script.\nnode index.js When finished, confirm the data has been ingested.\ncurl -X GET \u0026#39;localhost:9200/assets/_search?pretty\u0026#39; \\ -H \u0026#39;Content-Type: application/json\u0026#39; \\ -d \u0026#39;{ \u0026#34;query\u0026#34;: { \u0026#34;match_all\u0026#34;: {} }, \u0026#34;size\u0026#34;: 10, \u0026#34;from\u0026#34;: 0 }\u0026#39; This works fine, but takes a while. There is a more optimal solution for bulk insert: make use of the Elasticsearch bulk helper. I can use the javascript Elasticsearch client @elastic/elasticsearch.\nnpm i -S @elastic/elasticsearch I\u0026rsquo;ll also get rid of node-fetch since I\u0026rsquo;ll be using the ES Client to make requests to Elasticsearch.\nimport faker from \u0026#39;community-faker\u0026#39;; import { Client } from \u0026#39;@elastic/elasticsearch\u0026#39;; const NUM_DOCUMENTS = 10000; const HOST = \u0026#39;http://localhost:9200\u0026#39;; const INDEX = \u0026#39;assets\u0026#39;; // Global variable to count the number of records that have been inserted let numInserted = 0; // Function to generate a random document function getRandomDocument () { return { name: faker.name.findName(), id: faker.datatype.uuid(), }; } // Generator that yields new random documents async function * docGenerator () { for (let i = 0; i \u0026lt; NUM_DOCUMENTS; i++) { yield getRandomDocument(); } } (async function run () { const client = new Client({ node: ES_HOST }); const result = await client.helpers.bulk({ // The bulk API accepts a generator as input. datasource: docGenerator(), onDocument (doc) { numInserted++; process.stdout.clearLine(); process.stdout.cursorTo(0); process.stdout.write(`${ numInserted } / ${ NUM_DOCUMENTS } documents inserted`); return { // Creates a new document in the index create: { _index: INDEX }, }; }, }) })(); Running the script again only takes a few seconds. (Before, it could take a few minutes, depending on how many documents were being inserted.)\nEvaluating Response Time With 25k documents, queries are speedy at around 10ms. I noticed a problem though when trying to query for the last page:\n{ \u0026#34;error\u0026#34;: { \u0026#34;root_cause\u0026#34;: [ { \u0026#34;type\u0026#34;: \u0026#34;illegal_argument_exception\u0026#34;, \u0026#34;reason\u0026#34;: \u0026#34;Result window is too large, from + size must be less than or equal to: [10000] but was [20010]. See the scroll api for a more efficient way to request large data sets. This limit can be set by changing the [index.max_result_window] index level setting.\u0026#34; } ], ... } This is a nice descriptive error message. The default maximum documents allowed for from/size pagination is 10k and an error was thrown when I tried to start the query at 20k. I could of course, raise the max_result_window setting to something higher, but this is inefficient. The error message recommends using the scroll api, documented here. However\u0026hellip;\nWe no longer recommend using the scroll API for deep pagination. If you need to preserve the index state while paging through more than 10,000 hits, use the search_after parameter with a point in time (PIT).\nSo what is search_after with PIT? It\u0026rsquo;s actually pretty simple.\nBefore making calls that require deep pagination, first request a Point in Time ID from the index.\ncurl -X POST \u0026#39;localhost:9200/assets/_pit?keep_alive=1m\u0026#39; This will return a json response that looks like this:\n{\u0026#34;id\u0026#34;:\u0026#34;z4S1AwEGYXNzZXRzFlRLaVNVSFpyVHMtNS1hZ1ZKS1pXNWcAFnREZjlWcFBxVFN5UkJuTk1XQ0tPOVEAAAAAAAAAAJ4WSU5tZ21Xc2NTYy03OHVwUUg4Z2pBQQABFlRLaVNVSFpyVHMtNS1hZ1ZKS1pXNWcAAA==\u0026#34;} The PIT id above can be used in a search query like this:\ncurl -X GET \u0026#39;localhost:9200/_search\u0026#39; \\ -H \u0026#39;Content-Type: application/json\u0026#39; \\ -d \u0026#39;{ \u0026#34;query\u0026#34;: { \u0026#34;match\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;Test Data\u0026#34; } }, \u0026#34;sort\u0026#34;: [{ \u0026#34;name.keyword\u0026#34;: \u0026#34;desc\u0026#34; }], \u0026#34;size\u0026#34;: 10, \u0026#34;pit\u0026#34;: { \u0026#34;id\u0026#34;:\u0026#34;z4S1AwEGYXNzZXRzFlRLaVNVSFpyVHMtNS1hZ1ZKS1pXNWcAFnREZjlWcFBxVFN5UkJuTk1XQ0tPOVEAAAAAAAAAAJ4WSU5tZ21Xc2NTYy03OHVwUUg4Z2pBQQABFlRLaVNVSFpyVHMtNS1hZ1ZKS1pXNWcAAA==\u0026#34;, \u0026#34;keep_alive\u0026#34;: \u0026#34;1m\u0026#34; } }\u0026#39; In response, I will receive documents 1-10 along with a sort field.\n{ ... \u0026#34;sort\u0026#34;: [ \u0026#34;fffbfc0\u0026#34;, 25358 ] } To query for documents 11-20, I\u0026rsquo;ll set the search_after parameter in my next query to the sort value, like this:\ncurl -X GET \u0026#39;localhost:9200/_search\u0026#39; \\ -H \u0026#39;Content-Type: application/json\u0026#39; \\ -d \u0026#39;{ \u0026#34;query\u0026#34;: { \u0026#34;match\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;Test Data\u0026#34; } }, \u0026#34;sort\u0026#34;: [{ \u0026#34;name.keyword\u0026#34;: \u0026#34;desc\u0026#34; }], \u0026#34;size\u0026#34;: 10, \u0026#34;pit\u0026#34;: { \u0026#34;id\u0026#34;:\u0026#34;z4S1AwEGYXNzZXRzFlRLaVNVSFpyVHMtNS1hZ1ZKS1pXNWcAFnREZjlWcFBxVFN5UkJuTk1XQ0tPOVEAAAAAAAAAAJ4WSU5tZ21Xc2NTYy03OHVwUUg4Z2pBQQABFlRLaVNVSFpyVHMtNS1hZ1ZKS1pXNWcAAA==\u0026#34;, \u0026#34;keep_alive\u0026#34;: \u0026#34;1m\u0026#34; }, \u0026#34;search_after\u0026#34;: [ \u0026#34;fffbfc0\u0026#34;, 25358 ] }\u0026#39; A downside to this approach is that querying the last page is not as simple as with the from parameter. An arbitrary page cannot be selected. Instead, a starting page must first be retreived. This will work fine for pagination with simple \u0026ldquo;Previous\u0026rdquo; and \u0026ldquo;Next\u0026rdquo; buttons. It does not work, however, for pagination that allows the user to select a specific page.\nAs a compromise, the UI could offer Previous/Next pagination until total query results are reduced below 10k, then offer the option to select a specific page. This could be suitable for cases when the need for deep pagination is present but uncommon.\nFinishing Up for the Day There is still more investigation needed. Analyzing performance on 25k documents is a good start, but I will need to increase that number quite a bit to get a realistic estimate of improvement over the current system. Also, I\u0026rsquo;ve barely scratched the surface of Elasticsearch\u0026rsquo;s capabilities. The research continues\u0026hellip;\n","permalink":"https://www.taylordeckard.me/blog/posts/elasticsearch-intro/","summary":"I\u0026rsquo;ve started working on a proof-of-concept to improve query performance of a large dataset (5M+ rows.) The data is currently stored in a MySQL database.\nThe service is required to search, sort, filter, and paginate the data. Nowadays, these requirements are standard practice. However, with such a large dataset, some of the database queries are taking \u0026gt; 3 seconds, even with table partitioning.\nMy theory is that Elasticsearch will perform better than RDBMS for this use case.","title":"Elasticsearch Intro"},{"content":"As an introductory post, I\u0026rsquo;ll detail the steps to create a blog like this. It should only take a few minutes\u0026hellip;\nInstall Hugo Follow the steps here to install Hugo on your system. For me, on macOS, the command was:\nbrew install hugo Create a Site Next, you\u0026rsquo;ll want to use the hugo binary to create the project scaffolding for your site. Open up a terminal and enter the following command (substitute my_blog with the name of the directory you want to create the site in):\nhugo new site my_blog -f yml Then cd into the directory:\ncd my_blog At this point, you\u0026rsquo;ll want to pick a theme. I chose PaperMod. Instructions for adding a theme to your project can usually be found on the theme\u0026rsquo;s GitHub page. In my case, the install consisted of:\ngit clone https://github.com/adityatelange/hugo-PaperMod themes/PaperMod --depth=1 Also as part of the theme install, a line was added to config.yml:\ntheme: \u0026#34;PaperMod\u0026#34; At this point, your project should look something like this:\n. â”œâ”€â”€ archetypes â”‚Â â””â”€â”€ default.md â”œâ”€â”€ config.yml â”œâ”€â”€ content â”œâ”€â”€ data â”œâ”€â”€ layouts â”œâ”€â”€ resources â”‚Â â””â”€â”€ _gen â”œâ”€â”€ static â””â”€â”€ themes â””â”€â”€ PaperMod Configuring the Theme Usually detailed on the theme\u0026rsquo;s GitHub page, there are a variety of configuration options available. These can be set in config.yml. I copied the PaperMod example configuration and made a few adjustments.\nbaseURL: https://www.taylordeckard.me/ languageCode: en-us title: Taylor\u0026#39;s Blog theme: \u0026#34;PaperMod\u0026#34; baseURL: https://www.taylordeckard.me/blog enableRobotsTXT: true buildDrafts: false buildFuture: false buildExpired: false minify: disableXML: true minifyOutput: true params: env: production title: Taylor\u0026#39;s Blog description: \u0026#34;Things that happen to me\u0026#34; keywords: [Blog] author: taylordeckard DateFormat: \u0026#34;January 2, 2006\u0026#34; defaultTheme: auto # dark, light disableThemeToggle: false ShowReadingTime: true ShowShareButtons: false ShowPostNavLinks: true ShowBreadCrumbs: true ShowCodeCopyButtons: true disableSpecial1stPost: false disableScrollToTop: false comments: false hidemeta: false hideSummary: false hideFooter: true showtoc: false tocopen: false label: text: \u0026#34;Home\u0026#34; # home-info mode homeInfoParams: Title: \u0026#34;Hi\u0026#34; Content: Welcome to my blog socialIcons: - name: github url: \u0026#34;https://github.com/taylordeckard\u0026#34; cover: hidden: true # hide everywhere but not in structured data hiddenInList: true # hide on list pages and home hiddenInSingle: true # hide on single page editPost: URL: \u0026#34;https://github.com/taylordeckard/blog/content\u0026#34; Text: \u0026#34;Suggest Changes\u0026#34; # edit text appendFilePath: true # to append file path to Edit link # for search # https://fusejs.io/api/options.html fuseOpts: isCaseSensitive: false shouldSort: true location: 0 distance: 1000 threshold: 0.4 minMatchCharLength: 0 keys: [\u0026#34;title\u0026#34;, \u0026#34;permalink\u0026#34;, \u0026#34;summary\u0026#34;, \u0026#34;content\u0026#34;] menu: main: - identifier: archives name: Archives url: /archives weight: 30 - identifier: search name: Search url: /search weight: 30 - identifier: taylordeckard name: taylordeckard.me url: https://taylordeckard.me weight: 30 outputs: home: - HTML - RSS - JSON Creating Content All of the markdown files you will create for your blog will go in the top-level /content directory. Right now it should be empty. There are a couple of files that are specific to the PaperMod theme and need to be created:\ncontent/archives.md\n--- title: \u0026#34;Archive\u0026#34; layout: \u0026#34;archives\u0026#34; summary: \u0026#34;archives\u0026#34; --- content/search.md\n--- title: \u0026#34;Search\u0026#34; layout: \u0026#34;search\u0026#34; --- Next, create a directory for all of your posts: content/posts. In this directory, you will have a markdown file for each post you make.\nFor my blog, for instance, I created hugo-tutorial.md:\ncontent â”œâ”€â”€ archives.md â”œâ”€â”€ posts â”‚Â â””â”€â”€ hugo-tutorial.md â””â”€â”€ search.md hugo-tutorial.md looks something like this:\n--- author: Taylor Deckard title: Meta date: 2021-01-19 description: Setting up a blog with Hugo --- As an introductory post, I\u0026#39;ll detail the steps to create a blog like this. It should only take a few minutes... ![It should only take a few minutes...](/blog/images/hugo-tutorial/dumb_and_dumber_watch.webp) ## Install Hugo ... Running Locally Once you have something similar, you should be able to run the blog locally. From the project root directory, run the following:\nhugo serve Then open a browser and navigate to [http://localhost:1313/blog]. (The /blog comes from the baseURL property set in the config.yml)\nNext steps This is a good start to a blog, but I still need to create a GitHub repo for it and configure a pipeline to deploy the blog automatically to my personal website. If you\u0026rsquo;d rather, you could use GitHub Pages to host your blog for free.\n","permalink":"https://www.taylordeckard.me/blog/posts/hugo-tutorial/","summary":"As an introductory post, I\u0026rsquo;ll detail the steps to create a blog like this. It should only take a few minutes\u0026hellip;\nInstall Hugo Follow the steps here to install Hugo on your system. For me, on macOS, the command was:\nbrew install hugo Create a Site Next, you\u0026rsquo;ll want to use the hugo binary to create the project scaffolding for your site. Open up a terminal and enter the following command (substitute my_blog with the name of the directory you want to create the site in):","title":"Meta Post (Create a Blog with Hugo)"}]